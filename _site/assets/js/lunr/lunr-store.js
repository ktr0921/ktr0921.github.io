var store = [{
        "title": "Value-based Reinforcement Learning",
        "excerpt":"     Prerequisites      Markov Chain   Markov Decision Process          Machine intelligence is the last invention that humanity will ever need to make.           Nick Bostrom   If the universe is constructed with patterns and humans can interpret this, then so may machines. If machines become intelligent over humans, humans need not apply.   Machine learning is the study of algorithms that involve training a computer to learn patterns and relationships in data without being explicitly programmed. It can be divided into three types:     Supervised Learning: The algorithm, or model, is trained on labeled data, where the goal is to map each input to the correct output.   Unsupervised Learning: The algorithm, or model, is trained on unlabeled data, where the goal is to identify patterns or structure in the data without being told what to look for.   Reinforcement Learning: The algorithm, or agent, is trained on the interactive worlds, where the goal is to learn a policy that maximizes rewards and minimize penalties.   The problems in ML can be largely divided by four types:     Computer Vision: The problems require machines to interpret and understand visual data, i.e. images and videos. They includes image classification, object localization, video analysis, image captioning and visual question answering.   Natural Language Processing: The problems require machines to interpret and understand human language, i.e. words and documents. They includes text classification, translation, summarization, speech analysis, question answering and dialogue system.   Graph Analysis: The problems require machines to interpret and understand graph data, i.e. nodes and edges in graphs. They includes graph classification, link prediction, knowledge graph, social network analysis, recommendation system, combinatorial optimization and path finding.   Decision Making: The problems require machines to interact with the world and choose an appropriate action based on data and objectives. They include game playing, autonomous vehicles and robotics.   Reinforcement learning is a subfield of machine learning, popular in decision making problems. The algorithms are generally grouped by:                            Value-based           Policy-based                               Model-based           Model-free                                 Classic                               Policy Iteration,                           Value Iteration                               Monte Carlo,             SARSA,                           Q Learning                                                Policy Gradient             ,                              Deterministic Policy Gradient                                                      Deep                               Deep Q Network, Double Deep Q Network,                           Duel Deep Q Network, Rainbow                               Vanilla Policy Gradient, Natural Policy Gradient,                           Trust Region Policy Optimization,                           Proximal Policy Optimization                                         Classic                               Actor Critic                                         Deep                               Deep Deterministic Policy Gradient, Twin Delayed Deep Deterministic Policy Gradient,                           Soft Q Learning, Soft Actor Critic                 Value-based reinforcement learning (RL) is one of two fundamental classes of RL algorithms. It implicitly constructs the policy based on estimated value functions, so its objective is to correctly estimate value functions. The procedure is divided into two steps.     Policy evaluation: A process of evaluating the policy by obtaining the Bellman equation.   Policy improvement: A process of obtaining the improved policy based on the Bellman equation.   This post aims to provide a tutorial on value-based RL algorithms.   Policy Evaluation   The policy evaluation is a process of evaluating the policy by obtaining the Bellman equation. There are mainly three methods in the policy evaluation.                  Dynamic Programming           Temporal Difference           Monte Carlo                                                                                                                           Methods for policy evaluation from                      David Silver's lecture                 Where the update for the value functions is:   \\[\\begin{align}     \\text{DP: } &amp; v_{\\pi}^{(i+1)} ( s ) \\leftarrow \\sum_{a \\in A} \\pi (a \\vert s)     \\left(     \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r' + \\gamma \\cdot v_{\\pi}^{(i)} (s') \\right)     \\right)     \\\\     \\text{TD: } &amp; v_{\\pi}^{(i+1)} ( s_{t} ) \\leftarrow v_{\\pi}^{(i)} ( s_{t} )     + \\alpha \\left( \\left( r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} ) \\right) - v_{\\pi}^{(i)} ( s_{t} ) \\right)     \\\\     \\text{MC: } &amp; v_{\\pi}^{(i+1)} ( s_{t} ) \\leftarrow v_{\\pi}^{(i)} ( s_{t} )     + \\alpha \\left( \\left( \\sum_{k = 0}^{\\infty} \\gamma^{k} r_{t + k + 1} \\right) - v_{\\pi}^{(i)} ( s_{t} ) \\right) \\end{align}\\]  Where \\(s \\equiv s_{t}\\) and \\(s' \\equiv s_{t+1}\\). The notation \\(s\\) and \\(s'\\) signify the state and the neighbouring state while \\(s_{t}\\) and \\(s_{t+1}\\) signify the current state and the next state for a given trajectory, \\(\\tau = ( S_{0} = s_{0} , A_{0} = a_{0} , \\cdots  A_{T-1} = a_{T-1} , S_{T} = s_{T} )\\).   Dynamic programming (DP) is both a mathematical optimization method and a computer programming method. It is a general idea of expressing a complex problem into simpler sub-problems, but in this post, I will focus on reinforcement learning (RL), in particular solving a Markov decision process (MDP). Note that this method is developed by Richard Bellman, which the Bellman equation is named after. DP simply iterates through all the states and actions to recursively update the value functions with full access to the environment dynamics of the MDP.   In practice, DP is not preferred as most of the environments are either 1) unknown MDPs, i.e. how the opponent will play the game Go is unknown, or 2) known but intractably large MDPs, i.e. the game Go has \\(\\vert \\mathcal{S} \\vert \\approx 10^{170}\\) and \\(\\vert \\mathcal{A} \\vert \\approx 10^{360}\\). To handle this, we often use sampling methods for the policy evaluation, which are divided into temporal difference (TD) or Monte-Carlo (MC). They do not require to access the environment dynamics, but instead, for a given \\(\\tau\\), the states and actions are sampled for value function updates.   Comparison   Policy evaluation methods can be compared with respect to 1) sampling and 2) bootstrapping.                      Comparison between methods from                      David Silver's lecture                    Full backups account for every possible action to compute the Bellman equations while sample backups account for only sampled actions.   Deep backups account for the entire trajectory to compute the Bellman equations while shallow backups account for only a single time step ahead.   While the term full backups and sample backups are used to describe the update method of the value functions, algorithms that utilize these techniques are referred differently:      Model-based: The optimal policy is obtained using the environment dynamics, such as \\(p ( s' \\vert s , a )\\).            DP is known as model-based while the heuristic search can be model-based or model-free.       Model-based algorithms include those models that do not access the environment dynamics, but learn and predict the environment dynamics. Thus, generally, any model-free algorithm with additional world predicting algorithms that help the update is considered model-based.           Model-free: The optimal policy is obtained using sampled trajectories \\(\\tau\\).            Both TD and MC are considered model-free since they do not access nor predict environment dynamics.       Majority of algorithms used in RL are model-free, but as mentioned, if additional world predicting algorithms are employed to help the update, they are classified as model-based.           The standard RL usually refers to model-free RL. TD and MC can further be generalized as:   \\[\\begin{equation}     v_{\\pi}^{(i+1)} ( s_{t} ) \\leftarrow v_{\\pi}^{(i)} ( s_{t} ) + \\alpha \\delta     \\quad     \\delta = \\hat{v}_{\\pi}^{(i)} ( s_{t} ) - v_{\\pi}^{(i)} ( s_{t} )     \\\\     \\hat{v}_{\\pi}^{(i)} ( s_{t} ) =     \\begin{cases}         r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} ) &amp; \\text{for TD} \\\\         R_{t} = \\sum_{k = 0}^{\\infty} \\gamma^{k} r_{t+k+1} &amp; \\text{for MC} \\\\     \\end{cases} \\end{equation}\\]  Where:      \\(\\hat{v}_{\\pi}^{(i)}\\) is the estimated target value and \\(v_{\\pi}^{(i)}\\) is the value.   \\(\\delta\\) is the difference between \\(\\hat{v}_{\\pi}^{(i)}\\) and \\(v_{\\pi}^{(i)}\\), where \\(\\delta \\rightarrow 0\\) as \\(i \\rightarrow \\infty\\).   Higher \\(\\alpha\\) results in faster convergence, but may induce oscillations during learning.   Lower \\(\\alpha\\) results in slow convergence that may potentially result in falling into the local optima.   The difference between TD and MC is how \\(\\hat{v}_{\\pi}^{(i)}\\) is constructed.   The idea of TD and MC is sampling techniques in the policy evaluation, where the update is done using trajectories that the agent travelled with the initial state distribution, \\(p_{0}\\), instead of accessing the state transition probabilities. While the policy evaluation differs, the policy improvement in TD and MC follows the greedy policy, the same as DP.   Bias and Variance   Model-free algorithms suffer from bias and variance problems since they use a sampling technique to update the value functions.           MC has no bias: Since \\(v_{\\pi} ( s_{t} ) = \\mathbb{E}_{ \\tau \\sim \\pi } \\left[ R_{t} \\vert s_{t} \\right]\\):   \\[\\begin{equation}      \\mathbb{E}_{ \\tau \\sim \\pi } \\left[ \\hat{v}_{\\pi}^{(i)} ( s_{t} ) \\right]      = \\mathbb{E}_{ \\tau \\sim \\pi } \\left[ R_{t} \\vert s_{t} \\right] \\equiv v_{\\pi} ( s_{t} )  \\end{equation}\\]           TD has bias: Since \\(\\hat{v}_{\\pi}^{(i)} ( s_{t} ) = r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} )\\), \\(\\hat{v}_{\\pi}^{(i)} ( s_{t} )\\) follows \\(v_{\\pi}^{(i)} ( s_{t+1} )\\), where \\(v_{\\pi}^{(i)} ( s_{t+1} )\\) is an incorrect estimate. Thus, the initialization of the value functions causes bias in TD.   \\[\\require{color}  \\begin{align}      v_{\\pi}^{(i+1)} ( s_{t} )      &amp; \\leftarrow v_{\\pi}^{(i)} ( s_{t} ) + \\alpha \\left( \\hat{v}_{\\pi}^{(i)} ( s_{t} ) - v_{\\pi}^{(i)} ( s_{t} ) \\right)      \\quad      \\hat{v}_{\\pi}^{(i)} ( s_{t} ) = r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} )      \\\\      &amp; = \\textcolor{red}{ v_{\\pi}^{(i)} ( s_{t} ) + \\alpha \\left( \\left( r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} ) \\right) - v_{\\pi}^{(i)} ( s_{t} ) \\right) }      \\\\      &amp; = \\textcolor{blue}{ ( 1 - \\alpha ) \\cdot v_{\\pi}^{(i)} ( s_{t} )      + \\alpha \\left( r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} ) \\right) }      \\\\      &amp; = ( 1 - \\alpha ) \\cdot \\left(      \\textcolor{red}{ v_{\\pi}^{(i-1)} ( s_{t} ) + \\alpha \\left( \\left( r_{t+1} + \\gamma v_{\\pi}^{(i-1)} ( s_{t+1} ) \\right) - v_{\\pi}^{(i-1)} ( s_{t} ) \\right) }      \\right)      + \\alpha \\left( r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} ) \\right)      \\\\      &amp; = ( 1 - \\alpha ) \\cdot \\left(      \\textcolor{blue}{ ( 1 - \\alpha ) \\cdot v_{\\pi}^{(i-1)} ( s_{t} )      + \\alpha \\left( r_{t+1} + \\gamma v_{\\pi}^{(i-1)} ( s_{t+1} ) \\right) }      \\right)      + \\alpha \\left( r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} ) \\right)      \\\\      &amp; = ( 1 - \\alpha )^{2} \\cdot v_{\\pi}^{(i-1)} ( s_{t} )      + \\alpha ( 1 - \\alpha )^{1} \\left( r_{t+1} + \\gamma v_{\\pi}^{(i-1)} ( s_{t+1} ) \\right)      + \\alpha ( 1 - \\alpha )^{0} \\left( r_{t+1} + \\gamma v_{\\pi}^{(i-0)} ( s_{t+1} ) \\right)      \\\\      &amp; \\; \\vdots      \\\\      &amp; = ( 1 - \\alpha )^{i} \\cdot v_{\\pi}^{(0)} ( s_{t} )      + \\sum_{j = 0}^{i} \\alpha ( 1 - \\alpha )^{j} \\left( r_{t+1} + \\gamma v_{\\pi}^{(i-j)} ( s_{t+1} ) \\right)  \\end{align}\\]             If \\(i \\rightarrow \\infty\\), then \\(( 1 - \\alpha )^{i} \\cdot v_{\\pi}^{(0)} ( s_{t} ) \\rightarrow 0\\). However, the second term still includes the value functions of the early time steps, i.e. \\(\\alpha ( 1 - \\alpha )^{i-1} \\left( r_{t+1} + \\gamma v_{\\pi}^{(1)} ( s_{t+1} ) \\right)\\). Thus, the bias decreases over time, but will not be removed. This is based on  Why is temporal difference learning biased in reinforcement learning?.                MC has high variance: \\(R_{t}\\) at small \\(t\\) change drastically if \\(a_{t}\\) at small \\(t\\) change. Intuitively, in complex environments, there are infinitely many trajectories, but most of them are useless, so \\(G_{0}\\) for most trajectories will be zero or very small while there are some trajectories that produce relatively high \\(G_{0}\\).   \\[\\require{color}  \\begin{align}      \\text{Var} \\left[ R_{t} \\right]      &amp; = \\text{Var} \\left[ \\sum_{k = 0}^{\\infty} \\gamma^{k} r_{t+k+1} \\right]      \\\\      &amp; = \\sum_{k = 0}^{\\infty} (\\gamma^{k})^{2} \\text{Var} \\left[ r_{t+k+1} \\right]      + \\sum_{i \\neq j} \\gamma^{i} \\cdot \\gamma^{j} \\cdot \\text{Cov} \\left[ r_{t+i+1}, r_{t+j+1} \\right]      \\quad      \\text{Cov} \\left[ r_{t+i+1}, r_{t+j+1} \\right] = 0      \\\\      &amp; = \\sum_{k = 0}^{\\infty} (\\gamma^{k})^{2} \\text{Var} \\left[ r_{t+k+1} \\right]      + \\sum_{i \\neq j} \\gamma^{i} \\cdot \\gamma^{j} \\cdot 0      \\\\      &amp; = \\sum_{k = 0}^{\\infty} (\\gamma^{k})^{2} \\text{Var} \\left[ r_{t+k+1} \\right]      \\\\      &amp; &lt; \\sum_{k = 0}^{\\infty} \\gamma^{k} \\text{Var} \\left[ r_{t+k+1} \\right]      \\quad      \\text{where, } \\gamma \\in (0, 1)  \\end{align}\\]             Note that \\(\\text{Cov} \\left[ r_{t+i+1}, r_{t+j+1} \\right] = 0\\) is not really true since rewards in different time steps are correlated, but this is to provide a simple intuitive derivation. This is based on  How does Monte Carlo have high variance?.                TD has low variance: This term ‘low’ is relative to MC. When computing \\(\\hat{v}_{\\pi}^{(i)} ( s_{t} ) = r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} )\\), we do not account variance for rewards in the later time steps, and thus, the variance is \\(\\text{Var} \\left[ r_{t+1} \\right]\\), which is smaller than \\(\\sum_{k = 0}^{\\infty} \\gamma^{k} \\text{Var} \\left[ r_{t+k+1} \\right]\\).       Thus, TD is known to have high bias and low variance, while MC is known to have low bias and high variance.   \\(n\\)-Step Bootstrapping   In RL, the bootstrapping is a process of computing \\(\\hat{v}_{\\pi}^{(i)} ( s_{t} )\\) based on \\(v_{\\pi}^{(i)} ( s_{t+1} )\\). Instead of thoroughly relying on either TD or MC, we can use the \\(n\\)-step bootstrapping to balance bias and variance.                      Comparison between temporal difference and Monte Carlo methods from                      David Silver's lecture                 Where the update for the value functions is done according to:   \\[\\require{color} \\begin{equation}     \\hat{v}_{\\pi}^{(i)} ( s_{t} ) = R_{t}^{( \\textcolor{red}{n} )} = \\sum_{\\textcolor{blue}{k} = 0}^{\\textcolor{red}{n} - 1} \\gamma^{\\textcolor{blue}{k}} r_{t + \\textcolor{blue}{k} + 1} + \\gamma^{ \\textcolor{red}{n} } v_{\\pi}^{(i)} ( s_{t + \\textcolor{red}{n}} ) \\end{equation}\\]  The \\(n\\)-step bootstrapping with \\(n = 1\\) is the standard TD approach and \\(n = \\infty\\) is equivalent to MC. Due to how it can balance bias and variance, the \\(n\\)-step bootstrapping with reasonable \\(n\\) is more preferred than MC.   Dynamic Programming   Since DP can provide nice intuition, I will go through some common algorithms in DP. There are two classes of value-based RL algorithms.     Policy iteration: The policy evaluation with the Bellman expectation equation and the policy improvement.   Value iteration: The policy evaluation with the Bellman optimality equation, which encapsulates the policy improvement with the greedy policy.   Policy Iteration   The policy iteration is to find the optimal policy with the Bellman expectation equation. There exist two policy iteration algorithms based on the type of value functions used in the policy evaluation.     Generalized policy iteration, or V-policy iteration or iterative policy evaluation: The state value is used as a value function.   \\[\\begin{equation}     v_{\\pi}^{(i+1)} ( s )     \\leftarrow \\sum_{a \\in A} \\pi (a \\vert s)     \\left( \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r' + \\gamma v_{\\pi}^{(i)} (s') \\right) \\right) \\end{equation}\\]     Q-policy iteration: The Q value is used as a value function.   \\[\\begin{equation}     q_{\\pi}^{(i+1)} ( s , a )     \\leftarrow \\sum_{s' \\in S} p (s' \\vert s , a)     \\left( r' + \\gamma \\sum_{a' \\in A} \\pi (a' \\vert s') q_{\\pi}^{(i)} (s', a') \\right) \\end{equation}\\]  While the policy evaluation differs, the policy improvement usually uses the greedy policy.   \\[\\begin{equation}     \\pi_{\\text{greedy}} ( a \\vert s )     = \\begin{cases}       1 &amp; \\arg \\max_{a} q_{\\pi} ( s , a ) \\\\       0 &amp; \\text{otherwise}     \\end{cases} \\end{equation}\\]  Where \\(\\pi_{\\text{greedy}}\\) is the most common example of the deterministic policy. Note that there are other ways to construct the policy in the policy improvement, i.e. the softmax policy, \\(\\pi_{\\text{softmax}} ( a \\vert s ) = \\frac{ e^{ q_{\\pi} ( s , a ) } }{ \\sum_{a' \\in \\mathcal{A}} e^{ q_{\\pi} ( s , a' ) } }\\), for the stochastic policy.   The procedure of the generalized policy iteration follows:      Policy evaluation: Obtain \\(v_{\\pi}^{(k)} ( s )\\) for a given \\(\\pi^{(i)} ( a \\vert s )\\), where \\(k\\) is any reasonable number. Then, obtain \\(q_{\\pi}^{(i)} (s, a) = \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r ( s , a , s' ) + \\gamma v_{\\pi}^{(k)} (s') \\right)\\) for the policy improvement.   Policy improvement: Set \\(\\pi^{(i+1)} ( a \\vert s )\\) as the greedy policy for a given \\(q_{\\pi}^{(i)} (s, a)\\).   Policy iteration: Repeat the policy evaluation and the policy improvement to obtain \\(\\pi^{(\\infty)} ( a \\vert s ) \\equiv \\pi^{\\ast} ( a \\vert s )\\).                      Generalized policy iteration diagram from                      David Silver's lecture                 Thus, this process is:   \\[\\begin{equation}     \\pi^{(0)} ( a \\vert s )     \\rightarrow v_{\\pi}^{(0 \\rightarrow k)} ( s )     \\rightarrow q_{\\pi}^{(1)} (s, a)     \\rightarrow \\pi^{(1)} ( a \\vert s ) \\rightarrow \\cdots     \\rightarrow \\pi^{(\\infty)} ( a \\vert s ) \\end{equation}\\]  The generalized policy iteration requires computing the Q value additionally to obtain the policy.   Example   Consider a 4-by-4 deterministic gridworld problem with action set, \\(\\mathcal{A} = \\{ N, S, E, W \\}\\).                      Gridworld from                      David Silver's lecture                 The grey regions are the terminal states and a reward is held in every state transition, \\(r_{t+1} = r ( s_{t} , a_{t} , s_{t+1} ) = -1\\). Let the initial policy be a uniform distribution over action, \\(\\pi^{(0)} ( a \\vert s ) = \\mathcal{U} ( \\mathcal{A} )\\), then, the value functions are computed by applying the Bellman expectation equation repetitively.                  $$ \\pi^{(0)} ( a \\vert s ) $$           $$ v_{\\pi}^{(0)} ( s ) $$           $$ v_{\\pi}^{(1)} ( s ) $$           $$ v_{\\pi}^{(2)} ( s ) $$           $$ v_{\\pi}^{(\\infty)} ( s ) $$           $$ \\pi^{(1)} ( a \\vert s ) $$                                                                                                                                                                                                                        Value and policy in gridworld with dynamic programming from                      David Silver's lecture                    Policy evaluation: For a given \\(\\pi^{(0)} ( a \\vert s )\\), iterate value functions until it converges, \\(v_{\\pi}^{(0)} ( s ) \\rightarrow v_{\\pi}^{(\\infty)} ( s )\\),            The grey regions have \\(v_{\\pi}^{(\\infty)} ( s ) = 0\\) since it is the terminal state, no action is available.           Policy improvement: Update the policy with actions from low to high value functions, \\(\\pi^{(0)} ( a \\vert s ) \\rightarrow \\pi^{(1)} ( a \\vert s )\\). For instance, define \\(\\pi ( a \\vert s ) = \\begin{bmatrix} p_{N} &amp; p_{S} &amp; p_{E} &amp; p_{W} \\end{bmatrix}^{\\intercal}\\):  \\(\\begin{equation}      \\pi^{(0)} ( a \\vert s = 1 )      = \\begin{bmatrix} \\frac{1}{4} &amp; \\frac{1}{4} &amp; \\frac{1}{4} &amp; \\frac{1}{4} \\end{bmatrix}^{\\intercal}      \\quad \\rightarrow \\quad      \\pi^{(1)} ( a \\vert s = 1 )      = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}^{\\intercal}      \\\\      \\pi^{(0)} ( a \\vert s = 2 )      = \\begin{bmatrix} \\frac{1}{4} &amp; \\frac{1}{4} &amp; \\frac{1}{4} &amp; \\frac{1}{4} \\end{bmatrix}^{\\intercal}      \\quad \\rightarrow \\quad      \\pi^{(1)} ( a \\vert s = 2 )      = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}^{\\intercal}      \\\\      \\pi^{(0)} ( a \\vert s = 3 )      = \\begin{bmatrix} \\frac{1}{4} &amp; \\frac{1}{4} &amp; \\frac{1}{4} &amp; \\frac{1}{4} \\end{bmatrix}^{\\intercal}      \\quad \\rightarrow \\quad      \\pi^{(1)} ( a \\vert s = 3 )      = \\begin{bmatrix} 0 &amp; \\frac{1}{2} &amp; 0 &amp; \\frac{1}{2} \\end{bmatrix}^{\\intercal}  \\end{equation}\\)   In this example, \\(k \\rightarrow \\infty\\). In theory, under \\(\\pi^{(0)} ( a \\vert s ) = \\mathcal{U} ( \\mathcal{A} )\\), a single policy improvement with \\(v_{\\pi}^{(\\infty)} ( s )\\) is required for the optimal policy, \\(\\pi^{(1)} ( a \\vert s ) \\equiv \\pi^{(\\ast)} ( a \\vert s )\\). However, in practice, iterating through the policy evaluation and the policy improvement is more efficient. Thus, the generalized policy iteration usually switches between the policy evaluation and the policy improvement with a reasonable \\(k\\).   Value Iteration   The policy iteration algorithms iterate both value function and policy. The value iteration algorithms iterate only the value functions under the idea that the greedy policy, a.k.a. the Bellman optimality equation. Similar to the policy iteration, there exist two value iteration algorithms.      V-value iteration: The state value is used as a value function.   \\[\\begin{equation}     v_{\\pi}^{(i+1)} ( s )     \\leftarrow \\max_{a}     \\left( \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r' + \\gamma v_{\\pi}^{(i)} (s') \\right) \\right) \\end{equation}\\]     Q-value iteration: The Q value is used as a value function.   \\[\\begin{equation}     q_{\\pi}^{(i+1)} ( s , a )     \\leftarrow \\sum_{s' \\in S} p (s' \\vert s , a)     \\left( r' + \\gamma \\max_{a'} q_{\\pi}^{(i)} (s', a') \\right) \\end{equation}\\]  The only difference between the policy iteration is that the value functions are computed following the greedy policy.   The value iteration can be seen as performing the policy improvement for every single step of the policy evaluation, so both policy iteration and value iteration yield the same optimal policy. Due to this, the value iteration is usually considered more efficient.   The process of the value iteration becomes much simpler than the policy iteration.   \\[\\begin{equation}     \\pi^{(0)} ( a \\vert s )     \\rightarrow v_{\\pi}^{(0 \\rightarrow k)} ( s )     \\rightarrow q_{\\pi}^{(1)} (s, a)     \\rightarrow \\pi^{(\\infty)} ( a \\vert s ) \\end{equation}\\]  Temporal Difference Algorithm   TD algorithms are the most popular method to solve MDP in the real world scenario.      Since they sample trajectories, they are robust to complex unknown environments.   Due to their low variance and simplicity, they converge quickly.   The bias and variance can be balanced with \\(n\\)-step bootstrapping.   The temporal difference (TD) learning is a TD version of the generalized policy iteration that follows the basic form:   \\[\\begin{equation}     v_{\\pi}^{(i+1)} ( s_{t} ) \\leftarrow v_{\\pi}^{(i)} ( s_{t} ) + \\alpha \\left( \\hat{v}_{\\pi}^{(i)} ( s_{t} ) - v_{\\pi}^{(i)} ( s_{t} ) \\right)     \\\\     \\hat{v}_{\\pi}^{(i)} ( s_{t} ) = r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} ) \\end{equation}\\]  It often requires more iterations than the generalized policy iteration as they sample the trajectories for value function update.   SARSA   Similar to the Q-policy iteration, instead of using state value in the TD learning, we can directly evaluate the policy using the Q value and apply the policy improvement. This TD version of Q-policy iteration is referred to as the state-action-reward-state-action (SARSA), originated from how it samples a data point \\(( s, a, r, s', a' )\\).   \\[\\begin{equation}     q_{\\pi}^{(i+1)} ( s_{t} , a_{t} ) \\leftarrow q_{\\pi}^{(i)} ( s_{t} , a_{t} ) + \\alpha \\left( \\hat{q}_{\\pi}^{(i)} ( s_{t} , a_{t} ) - q_{\\pi}^{(i)} ( s_{t} , a_{t} ) \\right)     \\\\     \\hat{q}_{\\pi}^{(i)} ( s_{t} , a_{t} ) = r_{t+1} + \\gamma q_{\\pi}^{(i)} ( s_{t+1} , a_{t+1} ) \\end{equation}\\]  Since the policy improvement follows the greedy policy, the agent only selects actions with the highest Q value. Due to this, both TD learning and SARSA suffer from poor exploration. To remedy this, we introduce:      On-policy: How the agent explores is the same as what it learns, or \\(a \\sim \\pi ( a \\vert s )\\) where \\(\\pi ( a \\vert s ) = \\arg \\max_{a} q_{\\pi} ( s , a )\\).   Off-policy: How the agent explores is different from what it learns, or \\(a \\sim \\beta ( a \\vert s )\\) and \\(\\pi ( a \\vert s ) = \\arg \\max_{a} q_{\\pi} ( s , a )\\), where \\(\\beta ( a \\vert s )\\) is referred to as the behaviour policy and \\(\\pi ( a \\vert s )\\) is referred to as the target policy such that \\(\\beta ( a \\vert s ) \\neq \\pi ( a \\vert s )\\).   \\(\\beta\\) is used to act on the environment and collect trajectories while \\(\\pi\\) is the policy updated with the collected trajectories. In general, \\(\\beta\\) uses \\(\\pi\\) partially, either with probability or smoothing. The most common off-policy algorithm is \\(\\epsilon\\)-greedy exploration:   \\[\\begin{equation}     \\pi ( a \\vert s )     = \\begin{cases}       1 &amp; \\arg \\max_{a} q_{\\pi} ( s , a ) \\\\       0 &amp; \\text{otherwise}     \\end{cases}     \\quad     \\beta ( a \\vert s )     =     \\begin{cases}         1 - \\epsilon + \\frac{ \\epsilon }{ \\vert \\mathcal{A} \\vert } &amp; \\arg \\max_{a} q_{\\pi} ( s , a ) \\\\         \\frac{ \\epsilon }{ \\vert \\mathcal{A} \\vert } &amp; \\text{otherwise}     \\end{cases} \\end{equation}\\]    \\(\\beta\\) selects any action with \\(\\frac{ \\epsilon }{ \\vert \\mathcal{A} \\vert }\\), resulting in diverse action selection, and thus, better exploration.   However, the off-policy method introduces the distribution shift, where the distribution we are following, \\(\\beta\\), is not the same as the distribution we are updating, \\(\\pi\\). To diminish this effect, we use the importance sampling (IS), or importance sampling correction.   \\[\\require{color} \\begin{align}     \\hat{q}_{\\pi}^{(i)} ( s_{t} , a_{t} )     &amp; = \\textcolor{blue}{\\prod_{l=0}^{\\infty}}     \\frac{ \\pi ( a_{t+\\textcolor{blue}{l}} \\vert s_{t+\\textcolor{blue}{l}} ) }     { \\beta ( a_{t+\\textcolor{blue}{l}} \\vert s_{t+\\textcolor{blue}{l}} ) }     \\left( \\textcolor{green}{\\sum_{k=0}^{\\infty}} \\gamma^{\\textcolor{green}{k}} r_{t+\\textcolor{green}{k}+1}^{[\\beta]} \\right)     = \\textcolor{blue}{\\prod_{l=0}^{\\infty}}     \\frac{ \\pi ( a_{t+\\textcolor{blue}{l}} \\vert s_{t+\\textcolor{blue}{l}} ) }     { \\beta ( a_{t+\\textcolor{blue}{l}} \\vert s_{t+\\textcolor{blue}{l}} ) }     R_{t}^{[\\beta]}     = R_{t}^{[\\pi]}     \\\\     &amp; \\equiv \\textcolor{blue}{\\prod_{l=0}^{\\textcolor{red}{n}-1}}     \\frac{ \\pi ( a_{t+\\textcolor{blue}{l}} \\vert s_{t+\\textcolor{blue}{l}} ) }     { \\beta ( a_{t+\\textcolor{blue}{l}} \\vert s_{t+\\textcolor{blue}{l}} ) }     \\left( \\textcolor{green}{\\sum_{k=0}^{\\textcolor{red}{n}-1}} \\gamma^{\\textcolor{green}{k}} r_{t+\\textcolor{green}{k}+1}^{[\\beta]}     + \\gamma^{\\textcolor{red}{n}} q_{\\pi}^{(i)} ( s_{t+\\textcolor{red}{n}} , a_{t+\\textcolor{red}{n}} ) \\right)     \\\\     &amp; \\; \\vdots     \\\\     &amp; \\equiv     \\frac{ \\pi ( a_{t} \\vert s_{t} ) }{ \\beta ( a_{t} \\vert s_{t} ) }     \\frac{ \\pi ( a_{t+1} \\vert s_{t+1} ) }{ \\beta ( a_{t+1} \\vert s_{t+1} ) }     \\left( r_{t+1}^{[\\beta]} + \\gamma r_{t+2}^{[\\beta]} + \\gamma^{2} q_{\\pi}^{(i)} ( s_{t+2} , a_{t+2} ) \\right)     \\\\     &amp; \\equiv     \\frac{ \\pi ( a_{t} \\vert s_{t} ) }{ \\beta ( a_{t} \\vert s_{t} ) }     \\left( r_{t+1}^{[\\beta]} + \\gamma q_{\\pi}^{(i)} ( s_{t+1} , a_{t+1} ) \\right) \\end{align}\\]  Where \\(\\prod_{l=0}^{\\infty} \\frac{ \\pi ( a_{t+l} \\vert s_{t+l} ) }{ \\beta ( a_{t+l} \\vert s_{t+l} ) }\\) is the IS and \\(r^{[\\beta]}\\) signifies the reward acquired by \\(a \\sim \\beta ( a \\vert s )\\). Applying the IS transforms \\(r^{[\\beta]} \\rightarrow r^{[\\pi]}\\), so that they can be used to update \\(\\pi ( a \\vert s )\\). In other words, \\(\\tau\\) sampled from \\(\\beta\\) can be transformed into \\(\\tau\\) sampled from \\(\\pi\\) with the IS, and thus, updating \\(q_{\\pi}^{(i)}\\) becomes valid. Therefore:     The SARSA follows \\(\\hat{q}_{\\pi}^{(i)} ( s_{t} , a_{t} ) \\equiv \\frac{ \\pi ( a_{t} \\vert s_{t} ) }{ \\beta ( a_{t} \\vert s_{t} ) } \\left( r_{t+1}^{[\\beta]} + \\gamma q_{\\beta}^{(i)} ( s_{t+1} , a_{t+1} ) \\right)\\).   The \\(n\\)-step TD method follows \\(\\hat{q}_{\\pi}^{(i)} ( s_{t} , a_{t} ) \\equiv \\prod_{l=0}^{n-1} \\frac{ \\pi ( a_{t+l} \\vert s_{t+l} ) }{ \\beta ( a_{t+l} \\vert s_{t+l} ) } \\left( \\sum_{k=0}^{n-1} \\gamma^{k} r_{t+k+1}^{[\\beta]} + \\gamma^{n} q_{\\pi}^{(i)} ( s_{t+n} , a_{t+n} ) \\right)\\).   The MC method follows \\(\\hat{q}_{\\pi}^{(i)} ( s_{t} , a_{t} ) \\equiv \\prod_{l=0}^{\\infty} \\frac{ \\pi ( a_{t+l} \\vert s_{t+l} ) }{ \\beta ( a_{t+l} \\vert s_{t+l} ) } R_{t}^{[\\beta]}\\).   The idea of IS can be used for re-using previously acquired experiences from old policies (experience replay) or learning from experiences collected by different agents (offline learning).   Q Learning   The Q learning (QL) is a TD version of the Q-value iteration. It is the most popular value-based RL algorithm as it naturally uses the off-policy mechanism to update the policy, but without the IS. The QL updates the Q value with respect to the maximum Q value at the next time step. In other words, the greedy policy is used as the target policy.   \\[\\begin{equation}     q_{\\pi}^{(i+1)} ( s_{t} , a_{t} ) \\leftarrow q_{\\pi}^{(i)} ( s_{t} , a_{t} ) + \\alpha \\left( \\hat{q}_{\\pi}^{(i)} ( s_{t} , a_{t} ) - q_{\\pi}^{(i)} ( s_{t} , a_{t} ) \\right)     \\\\     \\hat{q}_{\\pi}^{(i)} ( s_{t} , a_{t} )     = r_{t+1}^{(\\beta)} + \\gamma \\max_{a'} q^{(i)} ( s_{t+1} , a' )     \\equiv r_{t+1}^{(\\beta)} + \\gamma q_{\\pi}^{(i)} ( s_{t+1} , a_{t+1} ) \\end{equation}\\]     \\(\\max_{a'} q^{(i)} ( s_{t+1} , a' )\\) means that \\(a'\\) has been sampled in a way that obtains the maximum Q value at \\(s_{t+1}\\), which is equivalent to using the greedy target policy, so \\(\\max_{a'} q^{(i)} ( s_{t+1} , a' ) \\equiv q_{\\pi}^{(i)} ( s_{t+1} , a_{t+1} )\\).   This also means that the QL requires to compute \\(q^{(i)} ( s_{t+1} , a' )\\) for every \\(a' \\in \\mathcal{A}\\) during the update while the TD learning and SARSA do not.   The IS can be ignored in the QL because:            \\(\\max_{a'} q^{(i)} ( s_{t+1} , a' )\\) is equivalent to \\(\\gamma q_{\\pi}^{(i)} ( s_{t+1} , a_{t+1} )\\) with the greedy policy.       \\(r_{t+1}^{(\\beta)}\\) is used to update the Q value of the action that acquired the reward.       Some discussions on stack exchange are Why don’t we use importance sampling for one step Q-learning? and Why we don’t use importance sampling in tabular Q-Learning?.           The QL follows \\(1\\)-step bootstrapping, so in theory, if the \\(n\\)-step is used, the QL also needs the IS. Sutton and Barto introduced the \\(n\\)-step tree backup algorithm for the \\(n\\)-step QL, but it seems like small \\(n\\) without the IS still works in practice, i.e. 3-step in RAINBOW.   The QL pseudocode is:                      QL pseudocode from                      Reinforcement Learning: An Introduction                 The QL obtains the optimal value directly, so it is more efficient than the SARSA. However, this introduces additional maximization bias. Since \\(\\max\\) is convex, under the Jensen’s inequality:   \\[\\begin{align}     \\mathbb{E}_{ \\tau \\sim \\pi } \\left[ \\max_{a'} q^{(i)} ( s_{t+1} , a' ) \\right]     \\geq     \\max_{a'} \\mathbb{E}_{ \\tau \\sim \\pi } \\left[ q^{(i)} ( s_{t+1} , a' ) \\right] \\end{align}\\]  Thus, using the bias definition:   \\[\\begin{align}     b ( \\max_{a'} q^{(i)} ( s_{t+1} , a' ) )     &amp; = \\mathbb{E}_{ \\tau \\sim \\pi } \\left[ \\max_{a'} q^{(i)} ( s_{t+1} , a' ) \\right] - \\max_{a'} q^{(i)} ( s_{t+1} , a' )     \\\\     &amp; = \\mathbb{E}_{ \\tau \\sim \\pi } \\left[ \\max_{a'} q^{(i)} ( s_{t+1} , a' ) \\right] - \\max_{a'} \\mathbb{E}_{ \\tau \\sim \\pi } \\left[ q^{(i)} ( s_{t+1} , a' ) \\right]     \\\\     &amp; \\geq 0 \\end{align}\\]  This can be easily viewed in the cliff walking problem:                      Cliff walking problem with SARSA and QL from                      David Silver's lecture                 Where The Cliff is the terminal state with the reward of \\(-100\\), otherwise the reward of \\(-1\\). The QL always takes the shortest but relatively risky path while the SARSA takes the safe path.   Summary  In this post, the value-based RL algorithms are covered. Their methods are divided into:     Policy evaluation: A process of evaluating the policy by obtaining the Bellman equation.            Dynamic Programming: Full &amp; shallow backups, known as model-based. It is not preferred in the real world scenario since it requires to access the environment dynamics.       \\[\\begin{equation}       v_{\\pi}^{(i+1)} ( s ) \\leftarrow \\sum_{a \\in A} \\pi (a \\vert s)       \\left(       \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r' + \\gamma \\cdot v_{\\pi}^{(i)} (s') \\right)       \\right)   \\end{equation}\\]             Temporal Difference: Sample &amp; shallow backups, known as model-free. It is generally preferred due to its flexibility and simplicity. It is known to have high bias and low variance, but bias and variance can be balanced using n-step bootstrapping.       \\[\\begin{equation}       v_{\\pi}^{(i+1)} ( s_{t} ) \\leftarrow v_{\\pi}^{(i)} ( s_{t} )       + \\alpha \\left( \\left( r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} ) \\right) - v_{\\pi}^{(i)} ( s_{t} ) \\right)   \\end{equation}\\]             Monte Carlo: Sample &amp; deep backups, known as model-free. It is generally less preferred than the temporal difference due to its inflexibility and complexity. It is known to have low bias and high variance.       \\[\\begin{equation}       v_{\\pi}^{(i+1)} ( s_{t} ) \\leftarrow v_{\\pi}^{(i)} ( s_{t} )       + \\alpha \\left( \\left( \\sum_{k = 0}^{\\infty} \\gamma^{k} r_{t + k + 1} \\right) - v_{\\pi}^{(i)} ( s_{t} ) \\right)   \\end{equation}\\]      Policy improvement: A process of obtaining the improved policy from the Bellman equation.            Greedy policy: A type of deterministic policy.       \\[\\begin{equation}       \\pi_{\\text{greedy}} ( a \\vert s )       = \\begin{cases}         1 &amp; \\arg \\max_{a} q_{\\pi} ( s , a ) \\\\         0 &amp; \\text{otherwise}       \\end{cases}   \\end{equation}\\]             Softmax policy: A type of stochastic policy.       \\[\\begin{equation}       \\pi_{\\text{softmax}} ( a \\vert s )       = \\frac{ e^{ q_{\\pi} ( s , a ) } }       { \\sum_{a' \\in \\mathcal{A}} e^{ q_{\\pi} ( s , a' ) } }   \\end{equation}\\]      Based on the policy evaluation, the algorithms are divided into:     Model-based: The optimal policy is obtained using the environment dynamics, such as \\(p ( s' \\vert s , a )\\). DP is known as model-based.        Model-free: The optimal policy is obtained using sampled trajectories \\(\\tau\\). TD and MC are known as model-free.              The update of model-free algorithms can further be generalized as:       \\[\\begin{equation}       v_{\\pi}^{(i+1)} ( s_{t} ) \\leftarrow v_{\\pi}^{(i)} ( s_{t} ) + \\alpha \\delta       \\quad       \\delta = \\hat{v}_{\\pi}^{(i)} ( s_{t} ) - v_{\\pi}^{(i)} ( s_{t} )       \\\\       \\hat{v}_{\\pi}^{(i)} ( s_{t} ) =       \\begin{cases}           r_{t+1} + \\gamma v_{\\pi}^{(i)} ( s_{t+1} ) &amp; \\text{for TD} \\\\           R_{t} = \\sum_{k = 0}^{\\infty} \\gamma^{k} r_{t+k+1} &amp; \\text{for MC} \\\\       \\end{cases}   \\end{equation}\\]             Model-free algorithms are known to suffer from bias and variance problems.                    TD is known to have high bias and low variance           MC is known to have low bias and high variance.                       To balance bias and variance, the \\(n\\)-step bootstrapping is often employed.       \\[\\require{color}   \\begin{equation}       \\hat{v}_{\\pi}^{(i)} ( s_{t} ) = R_{t}^{( n )} = \\sum_{k = 0}^{n - 1} \\gamma^{k} r_{t + k + 1} + \\gamma^{ n } v_{\\pi}^{(i)} ( s_{t + n} )   \\end{equation}\\]      The value-based RL algorithms are generally divided into:     Policy Iteration: The policy evaluation with the Bellman expectation equation and the policy improvement.   Value Iteration: The policy evaluation with the Bellman optimality equation, which encapsulates the policy improvement with the greedy policy.   DP and TD algorithms are:                      A comparison between DP and TD from                      David Silver's lecture                 Where the first and second algorithms are of the policy iteration and the last algorithm is of the value iteration.   In TD, there are two ways to sample \\(\\tau\\).     On-policy: How the agent explores is the same as what it learns, or \\(a \\sim \\pi ( a \\vert s )\\) where \\(\\pi ( a \\vert s ) = \\arg \\max_{a} q_{\\pi} ( s , a )\\).   Off-policy: How the agent explores is different from what it learns, or \\(a \\sim \\beta ( a \\vert s )\\) and \\(\\pi ( a \\vert s ) = \\arg \\max_{a} q_{\\pi} ( s , a )\\), where \\(\\beta ( a \\vert s )\\) is referred to as the behaviour policy and \\(\\pi ( a \\vert s )\\) is referred to as the target policy such that \\(\\beta ( a \\vert s ) \\neq \\pi ( a \\vert s )\\).   \\(\\beta\\) is used to act on the environment and collect trajectories while \\(\\pi\\) is the policy updated with the collected trajectories. In general, \\(\\beta\\) uses \\(\\pi\\) partially, either with probability or smoothing.   The off-policy method introduces the distribution shift, where the distribution we are following, \\(\\beta\\), is not the same as the distribution we are updating, \\(\\pi\\). To diminish this effect, we use the IS.   \\[\\require{color} \\begin{align}     \\hat{q}_{\\pi}^{(i)} ( s_{t} , a_{t} )     &amp; = \\prod_{l=0}^{\\infty}     \\frac{ \\pi ( a_{t+l} \\vert s_{t+l} ) }     { \\beta ( a_{t+l} \\vert s_{t+l} ) }     \\left( \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1}^{[\\beta]} \\right)     = \\prod_{l=0}^{\\infty}     \\frac{ \\pi ( a_{t+l} \\vert s_{t+l} ) }     { \\beta ( a_{t+l} \\vert s_{t+l} ) }     R_{t}^{[\\beta]}     = R_{t}^{[\\pi]}     \\\\     &amp; \\equiv \\prod_{l=0}^{n-1}     \\frac{ \\pi ( a_{t+l} \\vert s_{t+l} ) }     { \\beta ( a_{t+l} \\vert s_{t+l} ) }     \\left( \\sum_{k=0}^{n-1} \\gamma^{k} r_{t+k+1}^{[\\beta]}     + \\gamma^{n} q_{\\pi}^{(i)} ( s_{t+n} , a_{t+n} ) \\right) \\end{align}\\]  The QL is the most popular value-based RL algorithm. In summary, the QL:     uses sampling, so it is robust to complex unknown environments. (Pros over DP)   has low variance due to bootstrapping, so it converges fast. (Pros over MC)   is off-policy, so it has better exploration than on-policy. (Pros over on-policy)   avoids importance sampling correction since it uses the Bellman optimality equation. (Pros over TD learning and SARSA)   has initialization bias, so the initialization influences the convergence. (Cons over MC)   has overestimation bias, so it takes a risky path. (Cons over SARSA)   Reference      Lecture 3 to 5 of David Silver’s lecture   Chapter 4 to 7 in Reinforcement Learning: An Introduction  ","categories": ["Machine Learning&#x003a; Decision"],
        "tags": [],
        "url": "/posts/machine_learning/decision_1_value_based_reinforcement_learning/",
        "teaser": null
      },{
        "title": "Policy-based Reinforcement Learning",
        "excerpt":"     Prerequisites      Markov Chain   Markov Decision Process   Value-based Reinforcement Learning          Machine intelligence is the last invention that humanity will ever need to make.           Nick Bostrom   If the universe is constructed with patterns and humans can interpret this, then so may machines. If machines become intelligent over humans, humans need not apply.   Machine learning is the study of algorithms that involve training a computer to learn patterns and relationships in data without being explicitly programmed. It can be divided into three types:     Supervised Learning: The algorithm, or model, is trained on labeled data, where the goal is to map each input to the correct output.   Unsupervised Learning: The algorithm, or model, is trained on unlabeled data, where the goal is to identify patterns or structure in the data without being told what to look for.   Reinforcement Learning: The algorithm, or agent, is trained on the interactive worlds, where the goal is to learn a policy that maximizes rewards and minimize penalties.   The problems in ML can be largely divided by four types:     Computer Vision: The problems require machines to interpret and understand visual data, i.e. images and videos. They includes image classification, object localization, video analysis, image captioning and visual question answering.   Natural Language Processing: The problems require machines to interpret and understand human language, i.e. words and documents. They includes text classification, translation, summarization, speech analysis, question answering and dialogue system.   Graph Analysis: The problems require machines to interpret and understand graph data, i.e. nodes and edges in graphs. They includes graph classification, link prediction, knowledge graph, social network analysis, recommendation system, combinatorial optimization and path finding.   Decision Making: The problems require machines to interact with the world and choose an appropriate action based on data and objectives. They include game playing, autonomous vehicles and robotics.   Reinforcement learning is a subfield of machine learning, popular in decision making problems. The algorithms are generally grouped by:                            Value-based           Policy-based                               Model-based           Model-free                                 Classic                               Policy Iteration,                           Value Iteration                               Monte Carlo,             SARSA,                           Q Learning                                                Policy Gradient             ,                              Deterministic Policy Gradient                                                      Deep                               Deep Q Network, Double Deep Q Network,                           Duel Deep Q Network, Rainbow                               Vanilla Policy Gradient, Natural Policy Gradient,                           Trust Region Policy Optimization,                           Proximal Policy Optimization                                         Classic                               Actor Critic                                         Deep                               Deep Deterministic Policy Gradient, Twin Delayed Deep Deterministic Policy Gradient,                           Soft Q Learning, Soft Actor Critic                 Policy-based reinforcement learning is one of two fundamental classes of reinforcement learning algorithms. It explicitly constructs the policy, directly mapping from the state to the action, such that maximizes the return. Unlike value-based reinforcement learning, policy-based reinforcement learning attempts directly find the optimal policy by optimizing:   \\[\\begin{equation}     \\mathcal{J} ( \\theta )     = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} } [ R ( s , a ) ] \\end{equation}\\]  Where maximizing the objective is to obtain \\(\\pi_{\\theta}\\) such that maximizes \\(R\\) for every \\(s\\) and \\(a\\). This post aims to provide a tutorial on policy-based reinforcement learning algorithms and introduce a combination of value-based and policy-based reinforcement learning algorithm.   Policy Gradient   The policy gradient (PG) is an on-policy reinforcement learning (RL) algorithm that finds the optimal policy using the Monte-Carlo (MC) method. Unlike value-based methods, the PG parameterizes the policy with a function approximator such that maximizes the return. This has two advantages over value-based RL algorithms.     The policy is naturally stochastic.   The action space can be either discrete or continuous.   Consider the standard RL framework with an agent with the parameterized policy, \\(\\pi_{\\theta}\\), where \\(\\theta\\) is the parameter. The optimal policy, \\(\\pi^{\\ast}\\), is:   \\[\\begin{equation}     \\pi^{\\ast} ( a \\vert s ) = \\arg \\max_{\\pi} \\mathbb{E}_{ s \\sim d^{\\pi}, a \\sim \\pi } [ R ( s , a ) ] \\end{equation}\\]  Then, the objective function can be expressed as:   \\[\\begin{equation}     \\mathcal{J} ( \\theta )     = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} } [ R ( s , a ) ] \\end{equation}\\]  Maximizing the objective through gradient ascent obtains \\(\\theta\\) that results in the policy which drives the agent to the high expected return at any state-action pair.     The PG objective is different from \\(Q^{\\pi_{\\theta}} ( s , a ) = \\mathbb{E}_{ \\tau \\sim \\pi_{\\theta} } [ R_{t} \\vert s_{t} = s , a_{t} = a ]\\).            The Q value is the expectation of the return for a particular state-action pair.       The PG objective is the expectation of the return for any state-action pair.           If the MDP is average-reward continuous and ergodic, maximizing \\(Q^{\\pi_{\\theta}}\\) is actually the same as the PG objective.   The PG objective function has many equivalent expressions.   \\[\\begin{align}     \\mathcal{J} ( \\theta )     &amp; = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} } [ R ( s , a ) ]     = \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) R ( s , a )     \\\\     &amp; \\equiv \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} } \\left[ Q^{\\pi_{\\theta}} (s, a) \\right]     = \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) Q^{\\pi_{\\theta}} (s, a)     \\\\     &amp; \\equiv \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} } \\left[ V^{\\pi_{\\theta}} (s) \\right]     = \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) V^{\\pi_{\\theta}} (s)     \\\\     &amp; \\equiv \\mathbb{E}_{ s_{0} \\sim p_{0} } \\left[ V^{\\pi_{\\theta}} (s_{0}) \\right]     = \\sum_{s_{0} \\in \\mathcal{S}} p_{0} ( s_{0} ) V^{\\pi_{\\theta}} (s_{0}) \\end{align}\\]  Maximizing any of the expressions leads to the optimal policy.     Since \\(d^{\\pi_{\\theta}} ( s ) \\in [0, 1]\\) and \\(\\pi_{\\theta} (a \\vert s) \\in [0, 1]\\), \\(\\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) R ( s , a )\\) is the expectation of the \\(R\\).   Even if \\(Q^{\\pi_{\\theta}}\\) is not the same as the PG objective, acquiring \\(\\theta\\) that maximizes \\(Q^{\\pi_{\\theta}}\\) is equivalent to maximizing \\(R\\).   Policy Gradient Theorem   Computing the gradient of the PG objective, \\(\\nabla_{\\theta} \\mathcal{J}\\), is very difficult since:     \\(\\nabla_{\\theta} \\left( \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) R ( s , a ) \\right)\\) is intractable because \\(d^{\\pi_{\\theta}} ( s )\\), \\(\\pi_{\\theta} (a \\vert s)\\), \\(Q^{\\pi_{\\theta}} (s, a)\\), \\(V^{\\pi_{\\theta}} (s)\\) and \\(R ( s , a )\\) are dependent on \\(\\theta\\).   \\(\\nabla_{\\theta} R ( s , a )\\) where \\(s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta}\\) is doable because \\(Q^{\\pi_{\\theta}} (s, a)\\), \\(V^{\\pi_{\\theta}} (s)\\) and \\(R ( s , a )\\) are not explicit functions of \\(\\theta\\).   The Theorem 1 (Policy Gradient) allows us to approximate \\(\\nabla_{\\theta} \\mathcal{J}\\) into tractable form.   \\[\\begin{align}     \\nabla_{\\theta} \\mathcal{J} ( \\theta )     &amp; = \\nabla_{\\theta}     \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s )     \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) Q^{\\pi_{\\theta}} (s, a)     = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} }     \\left[ \\nabla_{\\theta} Q^{\\pi_{\\theta}} (s, a) \\right]     \\\\     &amp; \\propto     \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s )     \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) Q^{\\pi_{\\theta}} (s, a) \\nabla_{\\theta} \\pi_{\\theta} (a \\vert s)     = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} }     \\left[ Q^{\\pi_{\\theta}} (s, a) \\nabla_{\\theta} \\ln \\pi_{\\theta} (a \\vert s) \\right] \\end{align}\\]  In the original paper, the theorem is proved in two different environments, start-state episodic and average-reward continuous, but in this post, I will stick with the episodic environment without a discount factor since it is more intuitive and blends well with previous posts.   The gradient of the PG objective can be defined as:   \\[\\begin{equation}     \\nabla_{\\theta} \\mathcal{J} ( \\theta )     \\equiv \\nabla_{\\theta} \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} } \\left[ V^{\\pi_{\\theta}} (s) \\right]     \\equiv \\nabla_{\\theta} V^{\\pi_{\\theta}} (s)     \\quad     \\forall s \\in \\mathcal{S} \\end{equation}\\]  We can decompose \\(\\nabla_{\\theta} V^{\\pi_{\\theta}} ( s )\\) into:   \\[\\require{color} \\begin{align}     \\textcolor{red}{\\nabla_{ \\theta }} V^{ \\pi_{\\theta} } (s)     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} \\left( \\sum_{a \\in \\mathcal{A} } \\pi_\\theta (a \\vert s) Q^{\\pi_{\\theta}} (s, a) \\right)     \\\\     = &amp; \\sum_{a \\in \\mathcal{A}} \\left(     \\bigl( \\textcolor{red}{\\nabla_{ \\theta }} \\pi_{\\theta} (a \\vert s) \\bigr)     Q^{\\pi_{\\theta}} (s, a)     + \\pi_{\\theta} (a \\vert s)     \\bigl( \\textcolor{red}{\\nabla_{ \\theta }} Q^{\\pi_{\\theta}} (s, a) \\bigr)     \\right)     \\\\     = &amp; \\sum_{a \\in \\mathcal{A}} \\left(     Q^{\\pi_{\\theta}} (s, a) \\textcolor{red}{\\nabla_{ \\theta }} \\pi_{\\theta} (a \\vert s)     + \\pi_{\\theta} (a \\vert s)     \\left( \\textcolor{red}{\\nabla_{ \\theta }} \\sum_{s' \\in \\mathcal{S}} p(s' \\vert s, a) \\textcolor{blue}{(r (s, a) + V^{\\pi_{\\theta}} (s'))} \\right)     \\right)     \\\\     = &amp; \\sum_{a \\in \\mathcal{A}} \\left(     Q^{\\pi_{\\theta}} (s, a) \\textcolor{red}{\\nabla_{ \\theta }} \\pi_{\\theta} (a \\vert s)     + \\pi_{\\theta} (a \\vert s)     \\sum_{s' \\in \\mathcal{S}} p(s' \\vert s,a) \\textcolor{blue}{} \\textcolor{red}{\\nabla_{ \\theta }} \\textcolor{blue}{V^{\\pi_{\\theta}}(s')}     \\right)     \\\\     = &amp; \\sum_{a \\in \\mathcal{A}}     Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\pi_{\\theta} (a \\vert s)     + \\sum_{a \\in \\mathcal{A}}     \\left( \\pi_{\\theta} (a \\vert s) \\sum_{s' \\in \\mathcal{S}} p(s' \\vert s,a) \\nabla_{ \\theta} V^{\\pi_{\\theta}}(s') \\right)     \\\\     = &amp; \\sum_{a \\in \\mathcal{A}}     Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\pi_{\\theta} (a \\vert s)     + \\sum_{s' \\in \\mathcal{S}}     \\left( \\textcolor{red}{\\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) p(s' \\vert s,a)} \\right)     \\nabla_{ \\theta} V^{\\pi_{\\theta}}(s')     \\\\     = &amp; \\sum_{a \\in \\mathcal{A}}     Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\pi_{\\theta} (a \\vert s)     + \\sum_{s' \\in \\mathcal{S}}     \\textcolor{blue}{\\Pr (s \\rightarrow s' \\vert 1, \\pi_{\\theta})}     \\nabla_{ \\theta} V^{\\pi_{\\theta}}(s') \\end{align}\\]  Where the state visitation probability is \\(\\textcolor{blue}{\\Pr (s \\rightarrow s' \\vert 1, \\pi_{\\theta})} = \\textcolor{red}{\\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) p(s' \\vert s,a)}\\).   Then, recursively:   \\[\\require{color} \\begin{align}     \\nabla_{ \\theta } V^{ \\pi_{\\theta} } (\\textcolor{red}{s})     = &amp; \\sum_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{red}{s}, a) \\nabla_{ \\theta } \\pi_{\\theta} (a \\vert \\textcolor{red}{s})     \\\\     &amp; + \\sum_{\\textcolor{blue}{s'} \\in \\mathcal{S}} \\Pr (\\textcolor{red}{s} \\rightarrow \\textcolor{blue}{s'} \\vert 1, \\pi_{\\theta})     \\nabla_{ \\theta} V^{\\pi_{\\theta}}(\\textcolor{blue}{s'})     \\\\     = &amp; \\sum_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{red}{s}, a) \\nabla_{ \\theta } \\pi_{\\theta} (a \\vert \\textcolor{red}{s})     \\\\     &amp; + \\sum_{\\textcolor{blue}{s'} \\in \\mathcal{S}} \\Pr (\\textcolor{red}{s} \\rightarrow \\textcolor{blue}{s'} \\vert 1, \\pi_{\\theta})     \\Biggl( \\sum_{a’ \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{blue}{s'}, a’) \\nabla_{ \\theta } \\pi_{\\theta} (a’ \\vert \\textcolor{blue}{s'})     + \\sum_{\\textcolor{green}{s''} \\in \\mathcal{S}} \\Pr (\\textcolor{blue}{s'} \\rightarrow \\textcolor{green}{s''} \\vert 1, \\pi_{\\theta})     \\nabla_{ \\theta} V^{\\pi_{\\theta}}(\\textcolor{green}{s''}) \\Biggr)     \\\\     = &amp; \\sum_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{red}{s}, a) \\nabla_{ \\theta } \\pi_{\\theta} (a \\vert \\textcolor{red}{s})     \\\\     &amp; + \\sum_{\\textcolor{blue}{s'} \\in \\mathcal{S}} \\Pr (\\textcolor{red}{s} \\rightarrow \\textcolor{blue}{s'} \\vert 1, \\pi_{\\theta})     \\sum_{a’ \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{blue}{s'}, a’) \\nabla_{ \\theta } \\pi_{\\theta} (a’ \\vert \\textcolor{blue}{s'})     \\\\     &amp; + \\sum_{\\textcolor{blue}{s'} \\in \\mathcal{S}} \\Pr (\\textcolor{red}{s} \\rightarrow \\textcolor{blue}{s'} \\vert 1, \\pi_{\\theta})     \\sum_{\\textcolor{green}{s''} \\in \\mathcal{S}} \\Pr (\\textcolor{blue}{s'} \\rightarrow \\textcolor{green}{s''} \\vert 1, \\pi_{\\theta})     \\nabla_{ \\theta} V^{\\pi_{\\theta}}(\\textcolor{green}{s''})     \\\\     = &amp; \\sum_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{red}{s}, a) \\nabla_{ \\theta } \\pi_{\\theta} (a \\vert \\textcolor{red}{s})     \\\\     &amp; + \\sum_{\\textcolor{blue}{s'} \\in \\mathcal{S}} \\Pr (\\textcolor{red}{s} \\rightarrow \\textcolor{blue}{s'} \\vert 1, \\pi_{\\theta})     \\sum_{a’ \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{blue}{s'}, a’) \\nabla_{ \\theta } \\pi_{\\theta} (a’ \\vert \\textcolor{blue}{s'})     \\\\     &amp; + \\sum_{\\textcolor{green}{s''} \\in \\mathcal{S}} \\Pr (\\textcolor{red}{s} \\rightarrow \\textcolor{green}{s''} \\vert 2, \\pi_{\\theta})     \\nabla_{ \\theta} V^{\\pi_{\\theta}}(\\textcolor{green}{s''})     \\\\     = &amp; \\sum_{\\textcolor{red}{s} \\in \\mathcal{S}} \\Pr (\\textcolor{red}{s} \\rightarrow \\textcolor{red}{s} \\vert 0, \\pi_{\\theta})     \\sum_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{red}{s}, a) \\nabla_{ \\theta } \\pi_{\\theta} (a \\vert \\textcolor{red}{s})     \\\\     &amp; + \\sum_{\\textcolor{blue}{s'} \\in \\mathcal{S}} \\Pr (\\textcolor{red}{s} \\rightarrow \\textcolor{blue}{s'} \\vert 1, \\pi_{\\theta})     \\sum_{a’ \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{blue}{s'}, a’) \\nabla_{ \\theta } \\pi_{\\theta} (a’ \\vert \\textcolor{blue}{s'})     \\\\     &amp; + \\sum_{\\textcolor{green}{s''} \\in \\mathcal{S}} \\Pr (\\textcolor{red}{s} \\rightarrow \\textcolor{green}{s''} \\vert 2, \\pi_{\\theta})     \\sum_{a’’ \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{green}{s''}, a'') \\nabla_{ \\theta } \\pi_{\\theta} (a'' \\vert \\textcolor{green}{s''})     + \\cdots     \\\\     = &amp; \\sum_{\\textcolor{violet}{x} \\in \\mathcal{S}} \\sum_{\\textcolor{orange}{k} = 0}^{\\infty}     \\Pr (\\textcolor{red}{s} \\rightarrow \\textcolor{violet}{x} \\vert \\textcolor{orange}{k}, \\pi_{\\theta})     \\sum_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{violet}{x}, a) \\nabla_{ \\theta } \\pi_{\\theta} (a \\vert \\textcolor{violet}{x}) \\end{align}\\]  Finally, the update is:   \\[\\begin{align}     \\nabla_{ \\theta } \\mathcal{J} (\\theta)     \\equiv &amp; \\sum_{\\textcolor{red}{s_{0}} \\in \\mathcal{S}} p ( \\textcolor{red}{s_{0}} ) \\nabla_{ \\theta } V^{ \\pi_{\\theta} } (\\textcolor{red}{s_{0}})     \\\\     = &amp; \\sum_{\\textcolor{red}{s_{0}} \\in \\mathcal{S}} p ( \\textcolor{red}{s_{0}} )     \\sum_{\\textcolor{blue}{s} \\in \\mathcal{S}} \\sum_{\\textcolor{orange}{k} = 0}^{\\infty}     \\Pr (\\textcolor{red}{s_{0}} \\rightarrow \\textcolor{blue}{s} \\vert \\textcolor{orange}{k}, \\pi_{\\theta})     \\sum_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{blue}{s}, a) \\nabla_{ \\theta } \\pi_{\\theta} (a \\vert \\textcolor{blue}{s})     \\\\     = &amp; \\sum_{\\textcolor{blue}{s} \\in \\mathcal{S}}     \\left(     \\sum_{\\textcolor{red}{s_{0}} \\in \\mathcal{S}}     \\sum_{\\textcolor{orange}{k} = 0}^{\\infty}     p ( \\textcolor{red}{s_{0}} )     \\Pr (\\textcolor{red}{s_{0}} \\rightarrow \\textcolor{blue}{s} \\vert \\textcolor{orange}{k}, \\pi_{\\theta}) \\right)     \\sum_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{blue}{s}, a) \\nabla_{ \\theta } \\pi_{\\theta} (a \\vert \\textcolor{blue}{s})     \\\\     = &amp; \\sum_{\\textcolor{blue}{s} \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( \\textcolor{blue}{s} )     \\sum_{a \\in \\mathcal{A}} Q^{\\pi_{\\theta}} (\\textcolor{blue}{s}, a) \\nabla_{ \\theta} \\pi_{\\theta} (a \\vert \\textcolor{blue}{s})     \\\\     = &amp; \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\frac{ \\pi_{\\theta} (a \\vert s) }{\\pi_{\\theta} (a \\vert s)} Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\pi_{\\theta} (a \\vert s)     \\\\     = &amp; \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) Q^{\\pi_{\\theta}} (s, a) \\frac{ \\nabla_{ \\theta} \\pi_{\\theta} (a \\vert s) }{\\pi_{\\theta} (a \\vert s)}     \\\\     = &amp; \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} } \\left[ Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s) \\right] \\end{align}\\]  Another way to view the PG objective is from the log-likelihood. The PG objective without the value function is actually the same as the objective used in the behavioral cloning in imitation learning and more generally the log-likelihood in supervised learning. In the behavioral cloning, there are two factors that determine the policy:     The representation of the demonstrations in the dataset   The sampling distribution of the demonstrations from the dataset   If the sampling distribution is skewed for the update, the agent will learn the representations of the demonstration data that are more frequently sampled. On the other hand, the PG contains the value function in its objective. This enforces the agent to learn from:     The representation of the experiences   The sampling distribution of the experiences which is determined by the exploration   The value function as the magnitude of the acquired experiences   Thus, what the value function does is that it acts as the magnitude on how important the experience is. This will be discussed in later posts.   Policy Gradient with Function Approximation   The use of the real Q value results in high variance since it uses MC to construct the Q value. To reduce this variance, we can use another function approximator for Q value with parameter \\(\\phi\\), where \\(Q_{\\phi} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}\\), under the Theorem 2 (Policy Gradient with Function Approximation), which deals with two conditions:   \\[\\begin{equation}     \\nabla_{ \\phi } Q_{ \\phi } (s, a) = \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     \\qquad     \\nabla_{ \\phi } \\mathcal{J} ( \\phi )     = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} , a \\sim \\pi_{\\theta} } \\left[     \\left( Q_{\\phi} (s, a) - Q^{\\pi_{\\theta}} (s, a) \\right)^{2}     \\right]     \\leq     \\varepsilon \\end{equation}\\]  Where \\(\\varepsilon\\) is a small real number.     The first condition is that the gradient of the estimated Q value, \\(\\nabla_{ \\phi } Q_{ \\phi } (s, a)\\), must be equal to \\(\\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)\\).   The second is that \\(Q_{\\phi} (s, a)\\) must be optimized with the mean squared error with respect to \\(Q^{\\pi_{\\theta}} (s, a)\\).   This is actually outside of the scope of the PG since this introduces the value-based method into the policy-based method. However, since the original paper proves this, here it goes. The objective of the Q value can be expressed as:   \\[\\begin{align}     \\nabla_{ \\phi } \\mathcal{J} ( \\phi )     = &amp; \\nabla_{ \\phi } \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} , a \\sim \\pi_{\\theta} } \\left[     \\left( Q_{ \\phi } (s, a) - Q^{\\pi_{\\theta}} (s, a) \\right)^{2}     \\right]     = \\nabla_{ \\phi } \\varepsilon= 0     \\\\     = &amp; \\nabla_{ \\phi } \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s)     \\left( Q_{ \\phi } (s, a) - Q^{\\pi_{\\theta}} (s, a) \\right)^{2}     \\\\     = &amp; \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s)     \\cdot 2 \\left( Q_{ \\phi } (s, a) - Q^{\\pi_{\\theta}} (s, a) \\right) \\nabla_{ \\phi } Q_{ \\phi } (s, a)     \\\\     \\propto &amp; \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s)     \\left( Q_{ \\phi } (s, a) - Q^{\\pi_{\\theta}} (s, a) \\right) \\nabla_{ \\phi } Q_{ \\phi } (s, a)     \\quad     \\nabla_{ \\phi } Q_{ \\phi } (s, a) = \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     \\\\     = &amp; \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s)     \\left( Q_{ \\phi } (s, a) - Q^{\\pi_{\\theta}} (s, a) \\right) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     \\\\     = &amp; \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s)     Q_{ \\phi } (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     - \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s)     Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s) = 0 \\end{align}\\]  This means:   \\[\\begin{align}     \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s)     Q_{ \\phi } (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     &amp; = \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s)     Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     \\\\     \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} , a \\sim \\pi_{\\theta} } \\left[     Q_{ \\phi } (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     \\right]     &amp; = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} , a \\sim \\pi_{\\theta} } \\left[     Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     \\right] \\end{align}\\]  Therefore, 1) if \\(\\nabla_{ \\phi } Q_{ \\phi } (s, a) = \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)\\) holds and 2) if the function approximation of \\(Q_{ \\phi }\\) is optimized with the mean squared error with \\(Q^{\\pi_{\\theta}}\\), we can set the objective function as:   \\[\\begin{equation}     \\nabla_{ \\theta } \\mathcal{J} (\\theta)     \\propto \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} , a \\sim \\pi_{\\theta} } \\left[     Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     \\right]     \\equiv \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} , a \\sim \\pi_{\\theta} } \\left[     Q_{ \\phi } (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     \\right] \\end{equation}\\]  In traditional PG, the policy distribution is constructed with parameter, \\(\\theta\\), and a feature vector of state-action pair, \\(\\mathbf{x} ( s , a )\\), where \\(\\theta, \\mathbf{x} ( s , a ) \\in \\mathbb{R}^{n}\\).   Consider a cartpole problem with two state features, \\(( x_{p} , x_{v} )\\) as (Position, Velocity), and two actions, \\(( a_{l} , a_{r} )\\) as (Left, Right). The simplest form of \\(\\mathbf{x} ( s , a )\\) can be expressed as:   \\[\\begin{equation}     \\mathbf{x} ( s , a = a_{l} ) = \\begin{bmatrix} x_{p} &amp; x_{v} &amp; 0 &amp; 0 \\end{bmatrix}     \\qquad     \\mathbf{x} ( s , a = a_{r} ) = \\begin{bmatrix} 0 &amp; 0 &amp; x_{p} &amp; x_{v} \\end{bmatrix}     \\qquad     \\theta = \\begin{bmatrix} \\theta_{pl} &amp; \\theta_{vl} &amp; \\theta_{pr} &amp; \\theta_{vr} \\end{bmatrix}^{\\intercal} \\end{equation}\\]  For a given \\(\\theta\\), parameterized policy will be:   \\[\\begin{equation}     \\pi_{\\theta} (a_{l} \\vert s) = \\mathbf{x} ( s , a = a_{l} ) \\cdot \\theta     \\qquad     \\pi_{\\theta} (a_{r} \\vert s) = \\mathbf{x} ( s , a = a_{r} ) \\cdot \\theta \\end{equation}\\]  Thus, the first two values of \\(\\theta\\) learn to map features to \\(a_{l}\\) and the last two map features to \\(a_{r}\\). However, \\(\\theta\\) and \\(\\mathbf{x}\\) are the design choice, so based on the problems and algorithms, they vary.   There are two types of policy distribution that meet the two conditions in the Theorem 2 (Policy Gradient with Function Approximation).          Softmax policy distribution: A Gibbs distribution in a linear combination of features, a.k.a. softmax policy distribution, to produce action probability for discrete action space.   \\[\\begin{equation}       \\pi_{\\theta} (a \\vert s)       = \\text{Softmax} ( e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } )       = \\frac{ e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } }{ \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } }   \\end{equation}\\]             Where softmax signifies the normalization of \\(e^{ \\mathbf{x} ( s , a ) \\cdot \\theta }\\) across all the actions. To meet the condition in the Theorem 2 (Policy Gradient with Function Approximation):       \\[\\require{color}   \\begin{align}       \\nabla_{ \\phi } Q_{ \\phi } (s, a)       &amp; = \\nabla_{ \\theta } \\ln \\pi_{\\theta} (a \\vert s)       = \\frac{ \\nabla_{ \\theta } \\pi_{\\theta} (a \\vert s) }{\\pi_{\\theta} (a \\vert s)}       \\\\       &amp; = \\frac{ 1 }{ \\pi_{\\theta} (a \\vert s) }       \\nabla_{ \\theta }       \\textcolor{green}{ \\frac{ e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } }       { \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } } }       \\\\       &amp; = \\frac{ 1 }{\\pi_{\\theta} (a \\vert s)}       \\frac{ \\left( \\nabla_{ \\theta } \\textcolor{blue}{ e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } } \\right)       \\textcolor{red}{ \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } }       - \\textcolor{blue}{ e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } }       \\left( \\nabla_{ \\theta } \\textcolor{red}{ \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } } \\right) }       { \\left( \\textcolor{red}{ \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } } \\right)^{2} }       \\\\       &amp; = \\frac{ 1 }{\\pi_{\\theta} (a \\vert s)}       \\left(       \\frac{ \\mathbf{x} ( s , a ) \\textcolor{blue}{ e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } }       \\textcolor{red}{ \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } } }       { \\left( \\textcolor{red}{ \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } } \\right)^{2} }       - \\frac{ \\textcolor{blue}{ e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } }       \\sum_{ a' \\in \\mathcal{A} } \\mathbf{x} ( s , a' ) e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } }       { \\left( \\textcolor{red}{ \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } } \\right)^{2} }       \\right)       \\\\       &amp; = \\frac{ 1 }{\\pi_{\\theta} (a \\vert s)}       \\left(       \\mathbf{x} ( s , a )       \\textcolor{green}{ \\frac{ e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } }       { \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } } }       - \\textcolor{green}{ \\frac{ e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } }       { \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } } }       \\cdot \\frac{ \\sum_{ a' \\in \\mathcal{A} } \\mathbf{x} ( s , a' ) e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } }       { \\textcolor{red}{ \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } } }       \\right)       \\\\       &amp; = \\frac{ 1 }{ \\pi_{\\theta} (a \\vert s) }       \\left( \\mathbf{x} ( s , a ) \\cdot \\pi_{\\theta} (a \\vert s)       - \\pi_{\\theta} (a \\vert s) \\sum_{ a'' \\in \\mathcal{A} } \\mathbf{x} ( s , a'' )       \\textcolor{green}{ \\frac{ e^{ \\theta \\cdot \\mathbf{x} ( s , a'' ) } }       { \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } } }       \\right)       \\\\       &amp; = \\mathbf{x} ( s , a ) - \\sum_{ a' \\in \\mathcal{A} } \\mathbf{x} ( s , a' ) \\pi_{\\theta} (a' \\vert s)   \\end{align}\\]             Where \\(\\theta, \\mathbf{x} \\in \\mathbb{R}^{n}\\). Thus, Q value is parameterized with \\(\\phi \\in \\mathbb{R}^{n}\\).       \\[\\begin{equation}       Q_{ \\phi } (s, a)       = \\left( \\mathbf{x} ( s , a ) - \\sum_{ a' \\in \\mathcal{A} } \\mathbf{x} ( s , a' ) \\pi_{\\theta} (a' \\vert s) \\right) \\cdot \\phi   \\end{equation}\\]           Gaussian policy distribution: A Gaussian distribution with a parameterized mean and a fixed standard deviation for continuous action space.   \\[\\begin{equation}       \\pi_{\\theta} (a \\vert s) = \\mathcal{N} ( \\mu ( s , a ) , \\sigma^{2} )       \\quad       \\mu ( s , a ) = \\mathbf{x} ( s , a ) \\cdot \\theta   \\end{equation}\\]             Where \\(\\mu\\) is the parameterized mean of Gaussian distribution and \\(\\sigma\\) is the fixed standard deviation. To meet the condition in the Theorem 2 (Policy Gradient with Function Approximation):       \\[\\require{color}   \\begin{align}       \\nabla_{ \\phi } Q_{ \\phi } (s, a)       &amp; = \\nabla_{ \\theta } \\ln \\pi_{\\theta} (a \\vert s)       = \\frac{ \\nabla_{ \\theta } \\pi_{\\theta} (a \\vert s) }{\\pi_{\\theta} (a \\vert s)}       \\\\       &amp; = \\frac{ 1 }{\\pi_{\\theta} (a \\vert s)} \\nabla_{ \\theta } \\mathcal{N} ( \\mu ( s , a ) , \\sigma^{2} )       = \\frac{ 1 }{\\pi_{\\theta} (a \\vert s)}       \\left( \\nabla_{ \\theta } \\frac{1}{ \\sigma \\sqrt{ 2 \\pi } }       e^{ - \\frac{ ( a - \\mu ( s , a ) )^{2} }{ 2 \\sigma^{2} } } \\right)       \\\\       &amp; = \\frac{ 1 }{\\pi_{\\theta} (a \\vert s)}       \\left( \\frac{1}{ \\sigma \\sqrt{ 2 \\pi } }       e^{ - \\frac{ ( a - \\mu ( s , a ) )^{2} }{ 2 \\sigma^{2} } } \\right)       - \\frac{ \\nabla_{ \\theta } ( a - \\mu ( s , a ) )^{2} }{ 2 \\sigma^{2} }       \\\\       &amp; = \\frac{ 1 }{\\pi_{\\theta} (a \\vert s)} \\mathcal{N} ( \\mu ( s , a ) , \\sigma^{2} )       - \\frac{ -2 ( a - \\mu ( s , a ) ) \\nabla_{ \\theta } \\mu ( s , a ) }{ 2 \\sigma^{2} }       \\\\       &amp; = \\frac{ 1 }{\\pi_{\\theta} (a \\vert s)} \\pi_{\\theta} (a \\vert s)       + \\frac{ ( a - \\mu ( s , a ) ) \\mathbf{x} ( s , a ) }{ \\sigma^{2} }       \\\\       &amp; = \\mathbf{x} ( s , a ) \\frac{ a - \\mu ( s , a ) }{ \\sigma^{2} }   \\end{align}\\]             Where \\(\\theta, \\mathbf{x} \\in \\mathbb{R}^{n}\\). Thus, Q value is parameterized with \\(\\phi \\in \\mathbb{R}^{n}\\).       \\[\\begin{equation}       Q_{ \\phi } (s, a)       = \\left( \\mathbf{x} ( s , a ) \\frac{ a - \\mu ( s , a ) }{ \\sigma^{2} } \\right) \\cdot \\phi   \\end{equation}\\]      In both cases, the agent samples action from the policy distribution \\(a \\sim \\pi_{\\theta} (a \\vert s)\\).   Deterministic Policy Gradient   The standard stochastic PG with the continuous action space requires integrating over both state and action spaces.   \\[\\begin{equation}     \\nabla_{ \\theta } \\mathcal{J} (\\theta)     \\propto \\int_{\\mathcal{S}} d^{\\pi_{\\theta}} ( s )     \\int_{\\mathcal{A}} \\pi_{\\theta} (a \\vert s) Q^{\\pi_{\\theta}} (s, a)     \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s) \\text{ d}a \\text{ d}s     = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} , a \\sim \\pi_{\\theta} } \\left[     Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)     \\right] \\end{equation}\\]  To increase the computational efficiency, the deterministic policy gradient (DPG) is proposed, where the policy is no longer distribution, but just value in the continuous action space. Thus, instead of forming the Gaussian policy to sample actions, \\(a \\sim \\mathcal{N} ( \\mu ( s ) , \\sigma^{2} )\\), we treat the policy as the action, \\(a = \\mu ( s )\\), or the greedy policy in the continuous action space. This allows the objective to be formalized to only integrate over the state space:   \\[\\begin{align}     \\mathcal{J} ( \\theta )     &amp; = \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } [ R ( s , \\mu_{\\theta} ( s ) ) ]     = \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s ) R ( s , \\mu_{\\theta} ( s ) ) \\text{ d}s     \\\\     &amp; \\equiv \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[ Q^{\\mu_{\\theta}} (s, \\mu_{\\theta} ( s )) \\right]     = \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s ) Q^{\\mu_{\\theta}} (s, \\mu_{\\theta} ( s )) \\text{ d}s     \\\\     &amp; \\equiv \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[ V^{\\mu_{\\theta}} (s) \\right]     = \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s ) V^{\\mu_{\\theta}} (s) \\text{ d}s     \\\\     &amp; \\equiv \\mathbb{E}_{ s_{0} \\sim p_{0} } \\left[ V^{\\mu_{\\theta}} (s_{0}) \\right]     = \\int_{\\mathcal{S}} p_{0} ( s_{0} ) V^{\\mu_{\\theta}} (s_{0}) \\text{ d}s_{0} \\end{align}\\]  Few things to note are:     The reason why the DPG is more computationally efficient is because the computation is independent of the action space.   To avoid deficient exploration, the DPG is often implemented as off-policy with the behaviour policy, \\(a \\sim \\beta ( a \\vert s ) = \\mathcal{N} ( \\mu ( s ) , \\sigma^{2} )\\).            In other words, the DPG updates the deterministic policy with the trajectories collected by the stochastic policy.           The original paper used \\(\\mu ( s )\\) notation instead of \\(\\mu ( s , a )\\) to generalize its applications.   Deterministic Policy Gradient Theorem   The Theorem 1 (Deterministic Policy Gradient Theorem) allows us to approximate \\(\\nabla_{\\theta} \\mathcal{J}\\) into tractable form.   \\[\\begin{align}     \\nabla_{\\theta} \\mathcal{J} ( \\theta )     &amp; = \\nabla_{\\theta}     \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s ) Q^{\\mu_{\\theta}} (s, \\mu_{\\theta} ( s )) \\text{ d}s     = \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} }     \\left[ \\nabla_{\\theta} Q^{\\mu_{\\theta}} (s, \\mu_{\\theta} ( s )) \\right]     \\\\     &amp; \\propto \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )     \\nabla_{ \\theta} \\mu_{\\theta} ( s )     \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\text{ d}s     = \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[     \\nabla_{ \\theta} \\mu_{\\theta} ( s )     \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     \\right] \\end{align}\\]  Of course, you can simply apply the chain rule, \\(\\nabla_{\\theta} Q^{\\mu_{\\theta}} (s, \\mu_{\\theta} ( s )) = \\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}\\), but the original paper proves this in a similar manner to the PG paper.   The gradient of the DPG objective can be defined as:   \\[\\begin{equation}     \\nabla_{\\theta} \\mathcal{J} ( \\theta )     \\equiv \\nabla_{\\theta} \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[ V^{\\mu_{\\theta}} (s) \\right]     \\equiv \\nabla_{\\theta} V^{\\mu_{\\theta}} (s)     \\quad     \\forall s \\in \\mathcal{S} \\end{equation}\\]  We can decompose \\(\\nabla_{\\theta} V^{\\mu_{\\theta}} ( s )\\) into:   \\[\\begin{align}     \\textcolor{red}{\\nabla_{ \\theta }} V^{ \\mu_{\\theta} } (s)     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} Q^{\\mu_{\\theta}} (s, \\mu_{\\theta} (s))     \\\\     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} \\left(     \\int_{\\mathcal{S}} p(s' \\vert s, \\mu_{\\theta} ( s )) \\left(     r ( s , \\mu_{\\theta} ( s ) ) + V^{\\mu_{\\theta}} (s')     \\right) \\text{ d}s'     \\right)     \\\\     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} r ( s , \\mu_{\\theta} ( s ) )     + \\textcolor{red}{\\nabla_{ \\theta }}     \\int_{\\mathcal{S}} p(s' \\vert s, \\mu_{\\theta} ( s )) V^{\\mu_{\\theta}} (s') \\text{ d}s'     \\\\     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s ) \\textcolor{blue}{\\nabla_{ a }} r ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )}     \\\\     &amp; + \\int_{\\mathcal{S}}     \\left( \\textcolor{red}{\\nabla_{ \\theta }} p(s' \\vert s, \\mu_{\\theta} ( s )) \\right) V^{\\mu_{\\theta}} (s')     + p(s' \\vert s, \\mu_{\\theta} ( s )) \\left( \\textcolor{red}{\\nabla_{ \\theta }} V^{\\mu_{\\theta}} (s') \\right)     \\text{ d}s'     \\\\     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s ) \\textcolor{blue}{\\nabla_{ a }} r ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )}     \\\\     &amp; + \\int_{\\mathcal{S}}     V^{\\mu_{\\theta}} (s') \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s ) \\textcolor{blue}{\\nabla_{ a }} p(s' \\vert s, a) \\vert_{a = \\mu_{\\theta} ( s )}     + p(s' \\vert s, \\mu_{\\theta} ( s )) \\textcolor{red}{\\nabla_{ \\theta }} V^{\\mu_{\\theta}} (s')     \\text{ d}s'     \\\\     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s ) \\textcolor{blue}{\\nabla_{ a }}     \\left. \\left(     r ( s , a )     + \\int_{\\mathcal{S}}     V^{\\mu_{\\theta}} (s') p(s' \\vert s, a)     \\text{ d}s'     \\right) \\right\\vert_{a = \\mu_{\\theta} ( s )}     + \\int_{\\mathcal{S}}     p(s' \\vert s, \\mu_{\\theta} ( s )) \\textcolor{red}{\\nabla_{ \\theta }} V^{\\mu_{\\theta}} (s')     \\text{ d}s'     \\\\     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s ) \\textcolor{blue}{\\nabla_{ a }}     Q^{ \\mu_{\\theta} } ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )}     + \\int_{\\mathcal{S}}     \\Pr (s \\rightarrow s' \\vert 1 , \\mu_{\\theta} ( s )) \\textcolor{red}{\\nabla_{ \\theta }} V^{\\mu_{\\theta}} (s')     \\text{ d}s' \\end{align}\\]  Then, recursively:   \\[\\begin{align}     \\textcolor{red}{\\nabla_{ \\theta }} V^{ \\mu_{\\theta} } (s)     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s )     \\textcolor{blue}{\\nabla_{ a }} Q^{ \\mu_{\\theta} } ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )}     \\\\     &amp; + \\int_{\\mathcal{S}}     \\Pr (s \\rightarrow s' \\vert 1 , \\mu_{\\theta})     \\textcolor{red}{\\nabla_{ \\theta }} V^{\\mu_{\\theta}} (s')     \\text{ d}s'     \\\\     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s )     \\textcolor{blue}{\\nabla_{ a }} Q^{ \\mu_{\\theta} } ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )}     \\\\     &amp; + \\int_{\\mathcal{S}}     \\Pr (s \\rightarrow s' \\vert 1 , \\mu_{\\theta})     \\left(     \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s' )     \\textcolor{blue}{\\nabla_{ a' }} Q^{ \\mu_{\\theta} } ( s' , a' ) \\vert_{a' = \\mu_{\\theta} ( s' )}     + \\int_{\\mathcal{S}} \\Pr (s' \\rightarrow s'' \\vert 1 , \\mu_{\\theta})     \\textcolor{red}{\\nabla_{ \\theta }} V^{\\mu_{\\theta}} (s'')     \\text{ d}s''     \\right) \\text{ d}s'     \\\\     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s )     \\textcolor{blue}{\\nabla_{ a }} Q^{ \\mu_{\\theta} } ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )}     \\\\     &amp; + \\int_{\\mathcal{S}}     \\Pr (s \\rightarrow s' \\vert 1 , \\mu_{\\theta})     \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s' )     \\textcolor{blue}{\\nabla_{ a' }} Q^{ \\mu_{\\theta} } ( s' , a' ) \\vert_{a' = \\mu_{\\theta} ( s' )} \\text{ d}s'     \\\\     &amp; + \\int_{\\mathcal{S}}     \\Pr (s \\rightarrow s' \\vert 1 , \\mu_{\\theta}) \\int_{\\mathcal{S}}     \\Pr (s' \\rightarrow s'' \\vert 1 , \\mu_{\\theta})     \\textcolor{red}{\\nabla_{ \\theta }} V^{\\mu_{\\theta}} (s'')     \\text{ d}s'' \\text{ d}s'     \\\\     = &amp; \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s )     \\textcolor{blue}{\\nabla_{ a }} Q^{ \\mu_{\\theta} } ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )}     \\\\     &amp; + \\int_{\\mathcal{S}}     \\Pr (s \\rightarrow s' \\vert 1 , \\mu_{\\theta})     \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s' )     \\textcolor{blue}{\\nabla_{ a' }} Q^{ \\mu_{\\theta} } ( s' , a' ) \\vert_{a' = \\mu_{\\theta} ( s' )} \\text{ d}s'     \\\\     &amp; + \\int_{\\mathcal{S}}     \\Pr (s \\rightarrow s'' \\vert 2 , \\mu_{\\theta})     \\textcolor{red}{\\nabla_{ \\theta }} V^{\\mu_{\\theta}} (s'')     \\text{ d}s''     \\\\     = &amp; \\int_{\\mathcal{S}}     \\Pr (s \\rightarrow s \\vert 0 , \\mu_{\\theta})     \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s )     \\textcolor{blue}{\\nabla_{ a }} Q^{ \\mu_{\\theta} } ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )} \\text{ d}s     \\\\     &amp; + \\int_{\\mathcal{S}}     \\Pr (s \\rightarrow s' \\vert 1 , \\mu_{\\theta})     \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s' )     \\textcolor{blue}{\\nabla_{ a' }} Q^{ \\mu_{\\theta} } ( s' , a' ) \\vert_{a' = \\mu_{\\theta} ( s' )} \\text{ d}s'     \\\\     &amp; + \\int_{\\mathcal{S}}     \\Pr (s \\rightarrow s'' \\vert 2 , \\mu_{\\theta})     \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( s'' )     \\textcolor{blue}{\\nabla_{ a'' }} Q^{ \\mu_{\\theta} } ( s'' , a'' ) \\vert_{a'' = \\mu_{\\theta} ( s'' )} \\text{ d}s'' + \\cdots     \\\\     = &amp; \\int_{\\mathcal{S}}     \\sum_{k=0}^{\\infty} \\Pr (s \\rightarrow x \\vert k , \\mu_{\\theta})     \\textcolor{red}{\\nabla_{ \\theta }} \\mu_{\\theta} ( x ) \\textcolor{blue}{\\nabla_{ a }} Q^{ \\mu_{\\theta} } ( x , a ) \\vert_{a = \\mu_{\\theta} ( x )} \\text{ d}x \\end{align}\\]  Finally, the update is:   \\[\\begin{align}     \\nabla_{ \\theta } \\mathcal{J} ( \\theta )     = &amp; \\int_{\\mathcal{S}} p ( s_{0} ) \\nabla_{ \\theta } V^{ \\mu_{\\theta} } ( s_{0} ) \\text{ d}s_{0}     \\\\     = &amp; \\int_{\\mathcal{S}} p ( s_{0} )     \\int_{\\mathcal{S}}     \\sum_{k=0}^{\\infty} \\Pr (s_{0} \\rightarrow s \\vert k , \\mu_{\\theta})     \\nabla_{ \\theta } \\mu_{\\theta} ( s ) \\nabla_{ a } Q^{ \\mu_{\\theta} } ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )} \\text{ d}s     \\text{ d}s_{0}     \\\\     = &amp; \\int_{\\mathcal{S}}     \\left( \\int_{\\mathcal{S}}     \\sum_{k=0}^{\\infty} p ( s_{0} ) \\Pr (s_{0} \\rightarrow s \\vert k , \\mu_{\\theta}) \\text{ d}s_{0} \\right)     \\nabla_{ \\theta } \\mu_{\\theta} ( s ) \\nabla_{ a } Q^{ \\mu_{\\theta} } ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )} \\text{ d}s     \\\\     = &amp; \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )     \\nabla_{ \\theta } \\mu_{\\theta} ( s ) \\nabla_{ a } Q^{ \\mu_{\\theta} } ( s , a ) \\vert_{a = \\mu_{\\theta} ( s )} \\text{ d}s     = \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} ( s ) } \\left[     \\nabla_{ \\theta} \\mu_{\\theta} ( s )     \\nabla_{ a } Q^{ \\mu_{\\theta} } (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     \\right] \\end{align}\\]  The DPG can be seen as the PG with zero standard deviation.   \\[\\begin{equation}     \\lim_{\\sigma \\rightarrow 0}     \\nabla_{\\theta} \\mathcal{J} ( \\pi_{ \\mu_{\\theta}, \\sigma } ) = \\nabla_{\\theta} \\mathcal{J} ( \\mu_{\\theta} ) \\end{equation}\\]     This is the Theorem 2 (Limit of the Stochastic Policy Gradient), which is not proven in this post.   testing   Compatible Function Approximation   Similar to the PG, the DPG also provides a proof of the compatibility of a function approximator for the Q value under the Theorem 3 (Compatible Function Approximation). In the DPG, we use function approximator for \\(\\nabla_{ a } Q_{ \\phi } (s, a) \\vert_{a = \\mu_{\\theta} ( s )}\\) instead of \\(Q_{\\phi} (s, a)\\). The two conditions are:   \\[\\begin{equation}     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} = \\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\cdot \\phi     \\qquad     \\mathcal{J} ( \\phi )     = \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}}} \\left[ \\left(     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     - \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     \\right)^{2} \\right]     \\leq     \\varepsilon \\end{equation}\\]  Where \\(\\varepsilon\\) is a small real number.     The first condition is that the gradient of the estimated Q value for the greedy policy, \\(\\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}\\), must be equal to \\(\\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\cdot \\phi\\).   The second is that \\(\\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}\\) must be optimized with the mean squared error with respect to \\(\\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}\\).   Since a function approximator does not estimate the Q value, this is not really the value-based method, but rather a unique method that the DPG has. The objective of the gradient of the Q value can be expressed as:   \\[\\begin{align}     \\nabla_{ \\phi } \\mathcal{J} ( \\phi )     = &amp; \\nabla_{ \\phi } \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}}} \\left[ \\left(     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     - \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     \\right)^{2} \\right]     = \\nabla_{ \\phi } \\varepsilon = 0     \\\\     = &amp; \\nabla_{ \\phi } \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )     \\left(     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     - \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     \\right)^{2} \\text{ d}s     \\\\     = &amp; \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )     \\cdot 2 \\left(     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     - \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     \\right) \\nabla_{ \\phi } \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\text{ d}s     \\\\     \\propto &amp; \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )     \\left(     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     - \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     \\right) \\nabla_{ \\phi } \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\text{ d}s     \\\\     &amp; \\begin{aligned}         \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad \\qquad         \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} = \\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\cdot \\phi     \\end{aligned}     \\\\     = &amp; \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )     \\left(     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     - \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}     \\right) \\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\text{ d}s     \\\\     = &amp; \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\text{ d}s     - \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )     \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\text{ d}s \\end{align}\\]  This means,   \\[\\begin{align}     \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\text{ d}s     &amp; = \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )     \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\text{ d}s     \\\\     \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\nabla_{ \\theta} \\mu_{\\theta} ( s )     \\right]     &amp; = \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[     \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\nabla_{ \\theta} \\mu_{\\theta} ( s )     \\right] \\end{align}\\]  Therefore, 1) if \\(\\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} = \\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\cdot \\phi\\) holds and 2) if the function approximation of \\(\\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}\\) is optimized with the mean squared error with \\(\\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}\\), we can set the objective function as:   \\[\\begin{equation}     \\nabla_{ \\theta } \\mathcal{J} (\\theta)     \\propto \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[     \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\nabla_{ \\theta} \\mu_{\\theta} ( s )     \\right]     \\equiv \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[     \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\nabla_{ \\theta} \\mu_{\\theta} ( s )     \\right] \\end{equation}\\]  Practical Algorithm   Although the PG can be practically implemented, it is often that we divide it from practical algorithms.   REINFORCE   The REINFORCE is the earliest practical PG algorithm proposed from Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning. It is an on-policy stochastic PG algorithm that uses a raw return for the update per every episode.                      REINFORCE pseudocode from                      Reinforcement Learning: An Introduction                 Where \\(G_{t}\\) is the return at \\(t\\), so for every \\(t\\), compute \\(G_{t}\\) for the update.   Actor Critic   Since the PG uses MC, it suffers from high variance. To remedy this, the actot critic (AC) is introduced, which employs a bootstrapping strategy in the PG. The AC parameterizes both the value function and policy, and 1) update the value functions with a bootstrapping to estimate the true value functions and 2) update the policy in respect to the estimated Q value to acquire higher value functions. The name is originated from the fact that the value function critizes (critic) the action selection from the policy (actor).                      Actor critic pseudocode from                      Reinforcement Learning: An Introduction                 Where the temporal difference (TD) residual is used to compute \\(\\delta\\). Unlike REINFORCE, one-step AC updates the parameters every time step.   Summary  The PG is an on-policy RL algorithm that finds the optimal policy using the MC method. Unlike value-based methods, the PG parameterizes the policy with a function approximator such that maximizes the return. The PG objective function has many equivalent expressions.   \\[\\begin{align}     \\mathcal{J} ( \\theta )     &amp; = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} } [ R ( s , a ) ]     = \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) R ( s , a )     \\\\     &amp; \\equiv \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} } \\left[ Q^{\\pi_{\\theta}} (s, a) \\right]     = \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) Q^{\\pi_{\\theta}} (s, a)     \\\\     &amp; \\equiv \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} } \\left[ V^{\\pi_{\\theta}} (s) \\right]     = \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s ) V^{\\pi_{\\theta}} (s)     \\\\     &amp; \\equiv \\mathbb{E}_{ s_{0} \\sim p_{0} } \\left[ V^{\\pi_{\\theta}} (s_{0}) \\right]     = \\sum_{s_{0} \\in \\mathcal{S}} p_{0} ( s_{0} ) V^{\\pi_{\\theta}} (s_{0}) \\end{align}\\]          Theorem 1 (Policy Gradient):   \\[\\begin{equation}       \\nabla_{\\theta} \\mathcal{J} ( \\theta )       \\propto       \\sum_{s \\in \\mathcal{S}} d^{\\pi_{\\theta}} ( s )       \\sum_{a \\in \\mathcal{A}} \\pi_{\\theta} (a \\vert s) Q^{\\pi_{\\theta}} (s, a) \\nabla_{\\theta} \\pi_{\\theta} (a \\vert s)       = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} }       \\left[ Q^{\\pi_{\\theta}} (s, a) \\nabla_{\\theta} \\ln \\pi_{\\theta} (a \\vert s) \\right]   \\end{equation}\\]           Theorem 2 (Policy Gradient with Function Approximation):   \\[\\begin{equation}       \\nabla_{ \\theta } \\mathcal{J} (\\theta)       \\propto \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} , a \\sim \\pi_{\\theta} } \\left[       Q^{\\pi_{\\theta}} (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)       \\right]       \\equiv \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}} , a \\sim \\pi_{\\theta} } \\left[       Q_{ \\phi } (s, a) \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)       \\right]   \\end{equation}\\]             Where two conditions must meet.       \\[\\begin{equation}       \\nabla_{ \\phi } Q_{ \\phi } (s, a) = \\nabla_{ \\theta} \\ln \\pi_{\\theta} (a \\vert s)       \\qquad       \\mathcal{J} ( \\phi )       = \\mathbb{E}_{ s \\sim d^{\\pi_{\\theta}}, a \\sim \\pi_{\\theta} } \\left[       \\left( Q_{\\phi} (s, a) - Q^{\\pi_{\\theta}} (s, a) \\right)^{2}       \\right]       \\leq       \\varepsilon   \\end{equation}\\]             There are two types of policy distribution that meet the two conditions.                    Softmax policy distribution for discrete action space.           \\[\\begin{equation}       \\pi_{\\theta} (a \\vert s)       = \\text{Softmax} ( e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } )       = \\frac{ e^{ \\mathbf{x} ( s , a ) \\cdot \\theta } }{ \\sum_{ a' \\in \\mathcal{A} } e^{ \\mathbf{x} ( s , a' ) \\cdot \\theta } }       \\qquad       Q_{ \\phi } (s, a)       = \\left( \\mathbf{x} ( s , a ) - \\sum_{ a' \\in \\mathcal{A} } \\mathbf{x} ( s , a' ) \\pi_{\\theta} (a' \\vert s) \\right) \\cdot \\phi   \\end{equation}\\]                     Gaussian policy distribution for continuous action space.           \\[\\begin{equation}       \\pi_{\\theta} (a \\vert s) = \\mathcal{N} ( \\mu ( s , a ) , \\sigma^{2} )       \\quad       \\mu ( s , a ) = \\mathbf{x} ( s , a ) \\cdot \\theta       \\qquad       Q_{ \\phi } (s, a)       = \\left( \\mathbf{x} ( s , a ) \\frac{ a - \\mu ( s , a ) }{ \\sigma^{2} } \\right) \\cdot \\phi   \\end{equation}\\]                  The DPG is just the PG with the greedy policy, developed to improve computational efficiency in continuous state and action space.   \\[\\begin{align}     \\mathcal{J} ( \\theta )     &amp; = \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } [ R ( s , \\mu_{\\theta} ( s ) ) ]     = \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s ) R ( s , \\mu_{\\theta} ( s ) ) \\text{ d}s     \\\\     &amp; \\equiv \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[ Q^{\\mu_{\\theta}} (s, \\mu_{\\theta} ( s )) \\right]     = \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s ) Q^{\\mu_{\\theta}} (s, \\mu_{\\theta} ( s )) \\text{ d}s     \\\\     &amp; \\equiv \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[ V^{\\mu_{\\theta}} (s) \\right]     = \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s ) V^{\\mu_{\\theta}} (s) \\text{ d}s     \\\\     &amp; \\equiv \\mathbb{E}_{ s_{0} \\sim p_{0} } \\left[ V^{\\mu_{\\theta}} (s_{0}) \\right]     = \\int_{\\mathcal{S}} p_{0} ( s_{0} ) V^{\\mu_{\\theta}} (s_{0}) \\text{ d}s_{0} \\end{align}\\]          Theorem 1 (Deterministic Policy Gradient Theorem):   \\[\\begin{align}       \\nabla_{\\theta} \\mathcal{J} ( \\theta )       &amp; = \\nabla_{\\theta}       \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s ) Q^{\\mu_{\\theta}} (s, \\mu_{\\theta} ( s )) \\text{ d}s       = \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} }       \\left[ \\nabla_{\\theta} Q^{\\mu_{\\theta}} (s, \\mu_{\\theta} ( s )) \\right]       \\\\       &amp; \\propto \\int_{\\mathcal{S}} d^{\\mu_{\\theta}} ( s )       \\nabla_{ \\theta} \\mu_{\\theta} ( s )       \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\text{ d}s       = \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[       \\nabla_{ \\theta} \\mu_{\\theta} ( s )       \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}       \\right]   \\end{align}\\]           Theorem 2 (Limit of the Stochastic Policy Gradient):   \\[\\begin{equation}       \\lim_{\\sigma \\rightarrow 0}       \\nabla_{\\theta} \\mathcal{J} ( \\pi_{ \\mu_{\\theta}, \\sigma } ) = \\nabla_{\\theta} \\mathcal{J} ( \\mu_{\\theta} )   \\end{equation}\\]           Theorem 3 (Compatible Function Approximation):   \\[\\begin{equation}       \\nabla_{ \\theta } \\mathcal{J} (\\theta)       \\propto \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[       \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\nabla_{ \\theta} \\mu_{\\theta} ( s )       \\right]       \\equiv \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}} } \\left[       \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} \\nabla_{ \\theta} \\mu_{\\theta} ( s )       \\right]   \\end{equation}\\]             Where two conditions must meet.       \\[\\begin{equation}       \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )} = \\nabla_{ \\theta} \\mu_{\\theta} ( s ) \\cdot \\phi       \\qquad       \\mathcal{J} ( \\phi )       = \\mathbb{E}_{ s \\sim d^{\\mu_{\\theta}}} \\left[ \\left(       \\nabla_{ a } Q_{\\phi} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}       - \\nabla_{ a } Q^{\\mu_{\\theta}} (s, a) \\vert_{a = \\mu_{\\theta} ( s )}       \\right)^{2} \\right]       \\leq       \\varepsilon   \\end{equation}\\]      Three popular model-based algorithms are:     The QL:            parameterizes the value function.       naturally encapsulates the deterministic policy.       handles the MDP with only the discrete state/action space.       updates per step using a bootstrapping.       has relatively low variance, so it converges fast.       has relatively high bias, so it may be more likely to fall into local optima.                    has initialization bias, so the initialization influences the final performance.           has overestimation bias, so it takes a risky path.                       is naturally off-policy, so it has better exploration.           The PG:            parameterizes the policy.       naturally encapsulates the stochastic policy.       handles the MDP with both the discrete and continuous state/action space.       updates per episode.       has relatively high variance, so it converges slow.       has relatively low bias, so it may be less likely to fall into local optima.       is naturally on-policy, but off-policy can be employed with the importance sampling.           The AC:            parameterizes the value function and the policy.       naturally encapsulates the stochastic policy.       handles the MDP with both the discrete and continuous state/action space.       updates per step using a bootstrapping.       has relatively low variance, so it converges fast.       has relatively low bias, so it may be less likely to fall into local optima.       is naturally on-policy, but off-policy can be employed with the importance sampling.           Reference      Policy Gradient Methods for Reinforcement Learning with Function Approximation   Deterministic Policy Gradient Algorithms   Chapter 13 in Reinforcement Learning: An Introduction   Other helpful resources are:     Policy Gradient Algorithms   Going Deeper Into Reinforcement Learning: Fundamentals of Policy Gradients  ","categories": ["Machine Learning&#x003a; Decision"],
        "tags": [],
        "url": "/posts/machine_learning/decision_2_policy_based_reinforcement_learning/",
        "teaser": null
      },{
        "title": "Markov Chain",
        "excerpt":"     Prerequisites      Basic probability          All is number.           Pythagoras   Humans have invented and developed languages to symbolize entities, constructing Matrix that captures the essence of reality. On the other hand, any form of entities can be modelled as number, from natural to complex numbers and from points to objects, which existed long before human civilization. Thereby, the universe itself is inherently number, or in short, all is number.   Mathematics is the study of numbers, so if an entity exists, there exists mathematics to describe it, even if we cannot perceive. This is why we (arguably) refer mathematics as discovery instead of invention and often considered the most fundamental and precise way of describing reality. Mathematics is a vast, diverse and abstract field such that it requires books of worth. Here, a few branches of mathematics are focused on.   A Markov model is a stochastic process that describes pseudo-randomly changing systems under the Markov property, or the Markov assumption. The Markov property stipulates the future state depends only on the current state, not the history of states. There are four common Markov models:                                        Fully Observable                               Partially Observable                                         Autonomous System                               Markov Chain                               Hidden Markov Model                             Controllable System                       Markov Decision Process                               Partially Observable Markov Decision Process                 A Markov chain is the simplest form of a Markov model, which describes the world with an autonomous system. It is used to model simulations, speech recognition, time series forecasting, bioinformatics, game theories and many more. This post aims to provide a tutorial on a Markov chain and introduce a commonly used variant, a hidden Markov model.   Markov Chain   A Markov chain (MC), or Markov process, is the simplest form of a Markov model. In an MC, there exists a system that starts at a particular state, and every time step, the system travels to neighbouring states within the MC.   The Markov property is formally defined as following. There exists a state, \\(s \\in \\mathcal{S}\\), and a transition probability, \\(p \\in \\mathcal{P}\\), such that:   \\[\\begin{equation}     \\Pr ( S_{t+1} = s_{t+1} \\vert S_{0:t} = s_{0:t} )     \\equiv p ( S_{t+1} = s_{t+1} \\vert S_{t} = s_{t} )     = p ( s_{t+1} \\vert s_{t} ) \\end{equation}\\]  Where \\(S_{0:t} = s_{0:t}\\) signifies a sequence of states that the system visits from \\(0\\) to \\(t\\) time, \\(S_{0} = s_{0} , \\cdots , S_{t} = s_{t}\\). A collection of states that the system visited for a given maximum time, \\(T\\), is referred to as a path, \\(\\{ S_{0} = s_{0} , \\cdots S_{T} = s_{T} \\}\\). Unlike computer science, people from mathematics often refer to a state as an event or a process with \\(\\{ X_{t} \\}_{t=0}^{T}\\) notation.   The Markov property assumes that the probability of transitioning between two states is only dependent on the one-step previously visited states, not on the history of all the previously visited states. This allows us to formalize the transition probabilities as a fixed transition matrix, \\(\\mathbf{P}\\):   \\[\\begin{equation}     \\mathbf{P}     = \\begin{bmatrix}         p ( s_{1} \\vert s_{1} ) &amp; p ( s_{2} \\vert s_{1} ) &amp; \\cdots &amp; p ( s_{n} \\vert s_{1} ) \\\\         p ( s_{1} \\vert s_{2} ) &amp; p ( s_{2} \\vert s_{2} ) &amp; \\cdots &amp; p ( s_{n} \\vert s_{2} ) \\\\         \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\         p ( s_{1} \\vert s_{n} ) &amp; p ( s_{2} \\vert s_{n} ) &amp; \\cdots &amp; p ( s_{n} \\vert s_{n} ) \\\\     \\end{bmatrix} \\end{equation}\\]  Consider the following MC.                              Example Markov Chain from                              Wikimedia Commons                                                                                     $$             \\begin{align}                 \\mathbf{P}                 &amp; = \\begin{bmatrix}                     p ( E \\vert E ) &amp; p ( A \\vert E ) \\\\                     p ( E \\vert A ) &amp; p ( A \\vert A ) \\\\                 \\end{bmatrix}                 \\\\                 &amp; = \\begin{bmatrix}                     0.3 &amp; 0.7 \\\\                     0.4 &amp; 0.6 \\\\                 \\end{bmatrix}             \\end{align}             $$                 \\(\\mathbf{P}\\) is what determines the property of a state in an MC, and moreover, the property of an MC. However, \\(\\mathbf{P}\\) is not really intuitive, so instead, we usually analyze the accessibility of states from each other, referred to as communication.      If \\(s_{i}\\) communicates with itself, \\(s_{i} \\leftrightarrow s_{i}\\).   If \\(s_{i}\\) communicates with \\(s_{j}\\), \\(s_{i} \\leftrightarrow s_{j}\\).   If \\(s_{i}\\) communicates with \\(s_{k}\\) through \\(s_{j}\\), \\(s_{i} \\leftrightarrow s_{j}\\) and \\(s_{j} \\leftrightarrow s_{k}\\), then \\(s_{i} \\leftrightarrow s_{k}\\).   With these, we can determine the property of an MC. Few additional important concepts in an MC are:     A closed set is a set of states that the system cannot entre or leave.            A set includes a single element as well, so a state alone can be a closed set.           An initial distribution is a probability distribution over states that the system starts. It is a row vector, \\(\\pi^{(0)} \\in \\mathbb{R}^{n}\\).            The proability of the system visiting states at \\(k\\) step for a given \\(\\pi^{(0)}\\) is \\(\\pi^{(0)} \\mathbf{P}^{k}\\).           A stationary distribution is an equilibrium probability distribution over states that do not change over time. It is a row vector, \\(\\pi \\in \\mathbb{R}^{n}\\).            \\(\\pi\\) is the left eigenvector for \\(\\mathbf{P}\\), or \\(\\pi = \\pi \\mathbf{P}\\).       There may exist more than one \\(\\pi\\) depending on the perperty of an MC.           A limiting distribution is a unique non-zero equilibrium probability distribution over states that the system will end up if it travels infinitely. It is a row vector, \\(\\pi^{(\\infty)} \\in \\mathbb{R}^{n}\\).            \\(\\pi^{(\\infty)}\\) is the proability of the system visiting states at an infinite step for a given \\(\\pi^{(0)}\\), or \\(\\pi^{(\\infty)} = \\pi^{(0)} \\mathbf{P}^{\\infty}\\).       Similar to \\(\\pi\\), \\(\\pi^{(\\infty)}\\) is the left eigenvector for \\(\\mathbf{P}\\), or \\(\\pi^{(\\infty)} = \\pi^{(\\infty)} \\mathbf{P}\\).       Unlike \\(\\pi^{(0)}\\) and \\(\\pi\\), \\(\\pi^{(\\infty)}\\) may not exist depending on the property of an MC. If \\(\\pi^{(\\infty)}\\) exists, it is unique and equivalent to \\(\\pi\\), or \\(\\pi^{(\\infty)} = \\pi\\).                    In other words, if \\(\\pi^{(\\infty)}\\) exists, there only exists a single \\(\\pi\\) such that \\(\\pi = \\pi^{(\\infty)}\\).                           Usually, \\(\\mathbf{P}\\) is used to determine:     Whether a particular state communicates with other states.   The characteristics of the stationary distribution.   The existence of the limiting distribution.   Here, I will go through some typical properties of states and MC, particularly about countably finite discrete-time MC.   Absorption   A state is referred to as an absorbing state and a chain of states is referred to as an absorbing chain if the system cannot leave once entered, or absorption occurs. An MC with an absorbing state and/or an absorbing chain is referred to as an absorbing MC. In an absorbing MC, a non-absorbing state is referred to as a transient state. Thus, two types of states exist, absorbing and transient in an absorbing MC.   In an absorbing MC, \\(\\mathbf{P} \\in \\mathbb{R}^{n \\times n}\\) is usually expressed in canonical form. Let \\(t\\) be transient states and \\(r\\) be absorbing states, where \\(n = t + r\\).   \\[\\begin{equation}     \\mathbf{P}     = \\begin{bmatrix}         \\mathbf{Q} &amp; \\mathbf{R} \\\\         \\mathbf{0} &amp; \\mathbf{I} \\\\     \\end{bmatrix} \\end{equation}\\]     \\(\\mathbf{Q} \\in \\mathbb{R}^{t \\times t}\\) is \\(p\\) between transient states.   \\(\\mathbf{R} \\in \\mathbb{R}^{t \\times r}\\) is \\(p\\) from transient states to absorbing states.   \\(\\mathbf{I} \\in \\mathbb{R}^{r \\times r}\\) is \\(p\\) between absorbing states.   \\(\\mathbf{0} \\in \\mathbb{R}^{r \\times t}\\) is \\(p\\) from absorbing states to transient states, which is \\(0\\)-matrix.   A fundamental matrix, \\(\\mathbf{N} \\in \\mathbb{R}^{t \\times t}\\), is the expected number of the visitations between every transient state before absorption, \\(\\mathbf{N} = \\sum_{k=0}^{\\infty} \\mathbf{Q}^{k} = ( \\mathbf{I} - \\mathbf{Q} )^{-1}\\).     Each \\(( i , j )\\) element represents the expected number of the visitations from \\(s_{i}\\) to \\(s_{j}\\) before absorption.   \\(\\mathbf{Q}^{k}\\) is the proability of the system travelling between transient states at \\(k\\) step.   \\(\\mathbf{Q}^{\\infty}\\) approaches zero over time, \\(\\lim_{ k \\rightarrow \\infty } \\mathbf{Q}^{k} = \\mathbf{0} \\in \\mathbb{R}^{t \\times t}\\).   Under the Taylor series, \\(\\frac{1}{1-x} = \\sum_{k=0}^{\\infty} x^{k}\\), and thus, \\(\\frac{1}{\\mathbf{I} - \\mathbf{Q}} = ( \\mathbf{I} - \\mathbf{Q} )^{-1} = \\sum_{k=0}^{\\infty} \\mathbf{Q}^{k} = \\mathbf{N}\\).   An irreduciblility is another commonly seen term in an absorbing MC. It is often confused with absorption since it can be a synonym or antonym based on the context. A set of states that the system cannot leave once entered is referred to as a closed irreducible set. An MC with a closed irreducible set is referred to as a reducible MC.   Thus, a closed irreducible set is a synonym for an absorbing state and absorbing chain, but an absorbing MC is antonym for an irreducible MC. More formally:     An MC is an absorbing MC if there exist one or more closed irreducible sets, in which the system can reach from any state in a finite number of steps.   An MC is an irreducible MC if the system can reach any state from any other state in a finite number of time step.   If an absorbing MC is reduced into multiple disjoint subsets based on reduciblility, that are an absorbing state, an absorbing chain and a chain of transient states, states in each subset has the property that all states can communicate with each other, i.e. absorbing or transient.   Recurrence   A state is referred to as a recurrent state if the system is guaranteed to return to the starting state within a finite step. Note that the term recurrent is not often used to refer to an MC. More formally, a state is a recurrent state if:   \\[\\begin{equation}     \\sum_{t = 1}^{\\infty} \\Pr ( S_{t} = s_{i} \\vert S_{0} = s_{i} ) = 1 \\end{equation}\\]     \\(\\Pr ( S_{t} = s_{i} \\vert S_{0} = s_{i} )\\): For a given starting state, \\(s_{i}\\), the probability of the system returning to \\(s_{i}\\) at \\(t\\) step.   \\(\\sum_{t = 1}^{\\infty} \\Pr ( S_{t} = s_{i} \\vert S_{0} = s_{i} ) = 1\\): The sum of the probabilities that the system will return to \\(s_{i}\\) is \\(1\\) in finite step. This means that the system guarantees that it will re-visit the starting state.   The notation omits \\(S_{1} \\neq s_{i} , \\cdots , S_{t-1} \\neq s_{i}\\), so what it really signifies is \\(\\Pr ( S_{t} = s_{i} \\vert S_{0} = s_{i} , S_{1} \\neq s_{i} , \\cdots , S_{t-1} \\neq s_{i} )\\). This means that, for a given \\(S_{0} = s_{i}\\), the probability of the system returning to \\(s_{i}\\) at \\(t\\) step without visiting \\(s_{i}\\) in between.   This definition is equivalent to all the following statements:     \\(\\sum_{t=1}^{\\infty} \\mathbb{1} ( S_{t} = s_{i} \\vert S_{0} = s_{i} ) = \\infty\\): For a given \\(S_{0} = s_{i}\\), the summation of the booleans that the system will return to \\(s_{i}\\) is infinite.   \\(\\mathbb{1} ( S_{t} = s_{i} \\text{ for infinitely many } t \\vert S_{0} = s ) = 1\\): For a given \\(S_{0} = s_{i}\\), the boolean that the system will return to \\(s_{i}\\) for infinitely many \\(t\\) is 1.   If the system is not guaranteed to return, the state is referred to as a transient state, which is equivalent to the transient in an absorbing MC. More formally:   \\[\\begin{equation}     \\sum_{t=1}^{\\infty} \\Pr ( S_{t} = s_{i} \\vert S_{0} = s_{i} ) &lt; 1     \\\\     \\sum_{t=1}^{\\infty} \\mathbb{1} ( S_{t} = s_{i} \\vert S_{0} = s_{i} ) &lt; \\infty     \\\\     \\mathbb{1} ( S_{t} = s_{i} \\text{ for infinitely many } t \\vert S_{0} = s_{i} ) = 0 \\end{equation}\\]  An absorbing MC is the common MC that contains both recurrent and transient states.     An absorbing state is the recurrent state. If \\(s_{i}\\) is absorbing, \\(\\Pr ( S_{1} = s_{i} \\vert S_{0} = s_{i} ) = 1\\) and \\(\\Pr ( S_{n} = s_{i} \\vert S_{0} = s_{i} ) = 0\\) for \\(n &gt; 1\\).   All the states in an absorbing chain is the recurrent states. If \\(s_{i}\\) is within the absorbing chain, \\(\\sum_{t = 1}^{\\infty} \\Pr ( S_{t} = s_{i} \\vert S_{0} = s_{i} ) = 1\\) within the absorbing chain.   A non-absorbing state is the transient state. If \\(s_{i}\\) is not absorbing, the system will fall into an absorbing state or chain within a finite step, \\(\\sum_{t=1}^{\\infty} \\Pr ( S_{t} = s_{i} \\vert S_{0} = s_{i} ) &lt; 1\\).   A recurrence is to claim that the system will return to the starting state. However, there is a further division, positive or null. Both of them guarantee that the system will return to the starting state, but what matters is when. More formally, let a hitting time to be \\(\\tau_{s} := \\inf \\{ t \\geq 0 : s_{i} \\in \\mathcal{S} \\}\\), which is the infimum of the number of steps that the system returns to the starting state, \\(s_{i}\\).      Positive Recurrent: \\(\\tau_{s}\\) is finite and finite expectation, or \\(\\tau_{s} &lt; \\infty\\) and \\(\\mathbb{E} \\left[ \\tau_{s} \\right] &lt; \\infty\\), and thus, guaranteed to return in finite step.   Null Recurrent: \\(\\tau_{s}\\) is finite, but infinite expectation, or \\(\\tau_{s} &lt; \\infty\\) and \\(\\mathbb{E} \\left[ \\tau_{s} \\right] = \\infty\\), and thus, guaranteed to return in infinite step.   Transient: \\(\\tau_{s}\\) is infinite and infinite expectation, or \\(\\tau_{s} = \\infty\\) and \\(\\mathbb{E} \\left[ \\tau_{s} \\right] = \\infty\\), and thus, not guaranteed to return.   Due to this, positive recurrent is often described as it will return in a finite step while null recurrent as it will almost certainly return in a finite step.   A null recurrent state can only be present in a countably infinite MC, so in a countably finite MC, if an MC is said to be recurrent, it is always positive recurrent.     If an MC is countably finite:            There must be at least one recurrent state.                    If reducible, an absorbing state or chain is recurrent and others are transient.           If irreducible, every state is positive recurrent.                       There can be no transient state, but not every state can be transient.           If an MC is countably infinite:            Every state can be either positive recurrent, null recurrent or transient.           Periodicity  A state is referred to as a periodic state if the system returns to a particular state \\(s_{i}\\) at some rates \\(\\{ n , 2n , 3n , \\cdots \\}\\), where \\(n &gt; 1\\). For instance, for a given \\(S_{0} = s_{i}\\), if the system returns to \\(s_{i}\\) at \\(t = \\{ 4 , 16 , 24 , 28 , \\cdots \\}\\), then the state is periodic with \\(n = 4\\). If one state is periodic, then all the other states are periodic with the same period \\(n\\). An MC with periodic states is referred to as a periodic MC.   If at least one state has \\(p\\) to itself, \\(p ( s_{i} \\vert s_{i} ) &gt; 0\\), the state is aperiodic, and thus, the MC is also aperiodic. In general, it is said that an aperiodic MC has \\(n = 1\\) or \\(n\\) does not exist.   To compare with other properties:     If an MC is absorbing, it cannot be periodic since the system will get stuck at an absorbing state or chain in a finite step.   If an MC is transient or null recurrent, it cannot be periodic since the system may never return or return in an infinite step.   Thus, a periodic MC is always irreducible and positive recurrent while aperiodic can hold any property. However, irreducible and/or positive recurrent does not tell us any information about periodicity.   Ergodicity   The ergodicity is one of the typical assumptions in an MC for machine learning applications. It is derived from ergodic theory, a branch of mathematics that studies statistical properties of deterministic dynamical systems, but the use of ergodicity is fairly simple in machine learning. The regular MC we usually refer to in machine learning actually refers to an ergodic MC.   An MC is referred to as an ergodic MC if the system can travel from any state to any other state in any period, a.k.a. aperiodic, irreducible and positive recurrent. If an MC is ergodic, there exists a limiting distribution.   \\[\\begin{equation}     \\pi_{i}^{(\\infty)}     = \\lim_{n \\rightarrow \\infty} \\Pr ( S_{n} = s_{i} \\vert S_{0} = s ) &gt; 0     \\quad     s \\in \\mathcal{S} \\end{equation}\\]  This means, given any starting state, there exists a unique non-zero equilibrium probability that the system lands on a particular state, regardless of where it starts, if it travels infinitely.   A limiting distribution is often interchangeably used with a stationary distribution, but they are different.      A stationary distribution is an equilibrium probability distribution that does not change over time, which is the left eigenvector for \\(\\mathbf{P}\\), \\(\\pi = \\pi \\mathbf{P}\\), such that \\(\\pi_{i} \\geq 0 , \\sum_{i}^{n} \\pi_{i} = 1\\), where \\(\\mathcal{S} = \\{ s_{0} , \\cdots , s_{n} \\}\\).            For an MC to have \\(\\pi\\), it must have at least one positive recurrent state, in other words, if an MC has countably finite states, it has \\(\\pi\\).       There is no restriction on how many \\(\\pi\\) to exist, and thus, there may be more than one \\(\\pi\\).       For instance, a periodic MC has a single \\(\\pi\\) while an absorbing MC has infinitely many \\(\\pi\\).       One way to obtain \\(\\pi\\) is by finding eigenvalues and eigenvectors of the transposed \\(\\mathbf{P}\\).       \\[\\begin{equation}      \\pi = \\pi \\mathbf{P}      \\quad \\rightarrow \\quad      \\lambda \\pi^{\\intercal} = \\mathbf{P}^{\\intercal} \\pi^{\\intercal}      \\quad      \\text{where, } \\lambda = 1 , \\pi_{i} \\geq 0 , \\sum_{i}^{n} \\pi_{i} = 1  \\end{equation}\\]             However, this method does not obtain every \\(\\pi\\). For instance, in an absorbing MC, there are infinitely many \\(\\pi\\) and this method only acquires a few instances.                Another way to view \\(\\pi\\) is from \\(\\tau_{s}\\).   \\[\\begin{equation}       \\pi_{i} = \\frac{1}{ \\mathbb{E} \\left[ \\tau_{i} \\right] }   \\end{equation}\\]                     This also reflects why if \\(\\mathbb{E} \\left[ \\tau_{s} \\right] = \\infty\\), i.e. null recurrent or transient, \\(\\pi\\) approaches zero.           For an MC with null recurrent states only or transient states only, \\(\\tau_{s}\\) for all the states is infinite, so \\(\\sum_{i}^{\\infty} \\pi_{i} = 1\\) does not hold, and thus, there is no \\(\\pi\\). Often determining if a \\(\\pi\\) exists in the case of a recurrent MC is a method of proving if an MC is positive recurrent.                       So, if an MC is countably finite, at least one \\(\\pi\\) exists, but if countably infinite, there may be no \\(\\pi\\).           A limiting distribution is a unique non-zero equilibrium distribution that is not dependent on an initial distribution, \\(\\pi_{i}^{(\\infty)} = \\lim_{n \\rightarrow \\infty} \\Pr ( S_{n} = s_{i} \\vert S_{0} = s ) &gt; 0\\).            Since \\(\\pi^{(\\infty)}\\) is a unique distribution, the multiplication of \\(\\mathbf{P}\\) to any \\(\\pi^{(0)}\\) converges to \\(\\pi^{(\\infty)}\\).       \\[\\begin{align}      \\mathbf{P}^{\\ast}      = \\lim_{k \\rightarrow \\infty} \\mathbf{P}^{k}      &amp; = \\lim_{k \\rightarrow \\infty}      \\begin{bmatrix}          p ( s_{1} \\vert s_{1} ) &amp; p ( s_{2} \\vert s_{1} ) &amp; \\cdots &amp; p ( s_{n} \\vert s_{1} ) \\\\          p ( s_{1} \\vert s_{2} ) &amp; p ( s_{2} \\vert s_{2} ) &amp; \\cdots &amp; p ( s_{n} \\vert s_{2} ) \\\\          \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\          p ( s_{1} \\vert s_{n} ) &amp; p ( s_{2} \\vert s_{n} ) &amp; \\cdots &amp; p ( s_{n} \\vert s_{n} ) \\\\      \\end{bmatrix}^{k}      \\\\      &amp; = \\begin{bmatrix}          \\pi_{1}^{(\\infty)} &amp; \\pi_{2}^{(\\infty)} &amp; \\cdots &amp; \\pi_{n}^{(\\infty)} \\\\          \\pi_{1}^{(\\infty)} &amp; \\pi_{2}^{(\\infty)} &amp; \\cdots &amp; \\pi_{n}^{(\\infty)} \\\\          \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\          \\pi_{1}^{(\\infty)} &amp; \\pi_{2}^{(\\infty)} &amp; \\cdots &amp; \\pi_{n}^{(\\infty)} \\\\      \\end{bmatrix}      = \\begin{bmatrix}          \\pi^{(\\infty)} \\\\          \\pi^{(\\infty)} \\\\          \\vdots \\\\          \\pi^{(\\infty)} \\\\      \\end{bmatrix}      \\quad      \\text{where, }      \\pi^{(\\infty)}      = \\begin{bmatrix}          \\pi_{1}^{(\\infty)} \\\\ \\pi_{2}^{(\\infty)} \\\\ \\vdots \\\\ \\pi_{n}^{(\\infty)} \\\\      \\end{bmatrix}^{\\intercal}  \\end{align}\\]             A periodic MC has no \\(\\pi^{(\\infty)}\\) since it oscillates periodically.       An absorbing MC has no \\(\\pi^{(\\infty)}\\) since it converges to a distribution dependent to \\(\\pi^{(0)}\\) and the converged distribution contains zero value at transient states.       If \\(\\pi^{(\\infty)}\\) exists, \\(\\pi^{(\\infty)} = \\pi\\), and thus, \\(\\pi\\) is non-zero and unique.           Therefore:     The existence of \\(\\pi^{(\\infty)}\\) is necessary and sufficient for the ergodicity of an MC.   The ergodicity of an MC is necessary and sufficient for the existence of \\(\\pi^{(\\infty)}\\).   The existence of \\(\\pi^{(\\infty)}\\) is only sufficient for a non-zero unique \\(\\pi\\), not necessary.   A non-zero unique \\(\\pi\\) is neither necessary nor sufficient for the existence of \\(\\pi^{(\\infty)}\\).   A non-zero unique \\(\\pi\\) is neither necessary nor sufficient for the ergodicity of an MC.   Markov Chain Example   To give better insights, I will go through some examples.                  Absorbing Markov chain           Periodic Markov chain           Ergodic Markov chain                                                                                                                                        $ 1 $ and $ 2 $ are transient while $ 3 $ and $ 4 $ are absorbing.                               All are periodic with $ n = 2 $, $ \\{ 2, 4, 6, \\cdots \\} $.                               Other than $ p ( 3 \\vert 4 ) $ and $ p ( 2 \\vert 3 ) $, all the transition probabilities are non-zero.                                         $$             \\begin{equation}                 \\mathbf{P}                 = \\begin{bmatrix}                     0.2 &amp; 0.3 &amp; 0.1 &amp; 0.4 \\\\                     0.2 &amp; 0.3 &amp; 0.4 &amp; 0.1 \\\\                     0.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 \\\\                     0.0 &amp; 0.0 &amp; 0.0 &amp; 1.0 \\\\                 \\end{bmatrix}             \\end{equation}             $$                               $$             \\begin{equation}                 \\mathbf{P}                 = \\begin{bmatrix}                     0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\                     0.0 &amp; 0.0 &amp; 0.0 &amp; 1.0 \\\\                     1.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 \\\\                     0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\                 \\end{bmatrix}             \\end{equation}             $$                               $$             \\begin{equation}                 \\mathbf{P}                 = \\begin{bmatrix}                     0.2 &amp; 0.3 &amp; 0.1 &amp; 0.4 \\\\                     0.2 &amp; 0.3 &amp; 0.4 &amp; 0.1 \\\\                     0.5 &amp; 0.0 &amp; 0.2 &amp; 0.3 \\\\                     0.1 &amp; 0.7 &amp; 0.0 &amp; 0.2 \\\\                 \\end{bmatrix}             \\end{equation}             $$                            Markov chain examples. The original code for visualization and calculation can be found in                      here                 Absorbing Markov Chain   An absorbing MC has infinitely many stationary distributions, but no limiting distribution. The eigenvalues and eigenvectors for \\(\\mathbf{P}^{\\intercal}\\) are:   \\[\\begin{equation}     \\lambda \\pi^{\\intercal} = \\mathbf{P}^{\\intercal} \\pi^{\\intercal}     \\quad \\rightarrow \\quad     \\pi     = \\begin{bmatrix}         0.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad \\text{or} \\quad     \\begin{bmatrix}         0.0 &amp; 0.0 &amp; 0.0 &amp; 1.0 \\\\     \\end{bmatrix} \\end{equation}\\]  However, they are not all. Consider the transition matrix.   \\[\\begin{equation}     \\mathbf{P}     = \\begin{bmatrix}         0.2 &amp; 0.3 &amp; 0.1 &amp; 0.4 \\\\         0.2 &amp; 0.3 &amp; 0.4 &amp; 0.1 \\\\         0.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 \\\\         0.0 &amp; 0.0 &amp; 0.0 &amp; 1.0 \\\\     \\end{bmatrix}     \\quad \\rightarrow \\quad     \\mathbf{P}^{\\infty}     = \\begin{bmatrix}         0.0 &amp; 0.0 &amp; 0.38 &amp; 0.62 \\\\         0.0 &amp; 0.0 &amp; 0.68 &amp; 0.32 \\\\         0.0 &amp; 0.0 &amp; 1.0  &amp; 0.0  \\\\         0.0 &amp; 0.0 &amp; 1.0  &amp; 0.0  \\\\     \\end{bmatrix} \\end{equation}\\]  Under different initial distributions:   \\[\\begin{align}     \\pi^{(0)}     = \\begin{bmatrix}         1.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad \\rightarrow \\quad &amp;     \\pi^{(\\infty)}     = \\pi^{(0)} \\mathbf{P}^{\\infty}     = \\begin{bmatrix}         0.0 &amp; 0.0 &amp; 0.38 &amp; 0.62 \\\\     \\end{bmatrix}     \\\\     \\pi^{(0)}     = \\begin{bmatrix}         0.2 &amp; 0.8 &amp; 0.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad \\rightarrow \\quad &amp;     \\pi^{(\\infty)}     = \\pi^{(0)} \\mathbf{P}^{\\infty}     = \\begin{bmatrix}         0.0 &amp; 0.0 &amp; 0.62 &amp; 0.38 \\\\     \\end{bmatrix}     \\\\     \\pi^{(0)}     = \\begin{bmatrix}         0.0 &amp; 1.0 &amp; 0.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad \\rightarrow \\quad &amp;     \\pi^{(\\infty)}     = \\pi^{(0)} \\mathbf{P}^{\\infty}     = \\begin{bmatrix}         0.0 &amp; 0.0 &amp; 0.68 &amp; 0.32 \\\\     \\end{bmatrix}     \\\\     \\pi^{(0)}     = \\begin{bmatrix}         0.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad \\rightarrow \\quad &amp;     \\pi^{(\\infty)}     = \\pi^{(0)} \\mathbf{P}^{\\infty}     = \\begin{bmatrix}         0.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\\\     \\pi^{(0)}     = \\begin{bmatrix}         0.3 &amp; 0.3 &amp; 0.4 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad \\rightarrow \\quad &amp;     \\pi^{(\\infty)}     = \\pi^{(0)} \\mathbf{P}^{\\infty}     = \\begin{bmatrix}         0.0 &amp; 0.0 &amp; 0.718 &amp; 0.282 \\\\     \\end{bmatrix} \\end{align}\\]     A stationary distribution depends on the initial distribution, so there is no unique equilibrium, a.k.a. no limiting distribution.   Transient states always have zero stationary distribution, since the system falls into absorbing states.   A fundamental matrix and an absorption time are:   \\[\\begin{equation}     \\mathbf{P}     = \\begin{bmatrix}         0.2 &amp; 0.3 &amp; 0.1 &amp; 0.4 \\\\         0.2 &amp; 0.3 &amp; 0.4 &amp; 0.1 \\\\         0.0 &amp; 0.0 &amp; 1.0 &amp; 0.0 \\\\         0.0 &amp; 0.0 &amp; 0.0 &amp; 1.0 \\\\     \\end{bmatrix}     = \\begin{bmatrix}         \\mathbf{Q} &amp; \\mathbf{R} \\\\         \\mathbf{0} &amp; \\mathbf{I} \\\\     \\end{bmatrix}     \\\\     \\mathbf{Q}     = \\begin{bmatrix}         0.2 &amp; 0.3 \\\\         0.2 &amp; 0.3 \\\\     \\end{bmatrix}     \\quad     \\mathbf{Q}^{2}     = \\begin{bmatrix}         0.1 &amp; 0.15 \\\\         0.1 &amp; 0.15 \\\\     \\end{bmatrix}     \\quad     \\mathbf{Q}^{\\infty}     = \\begin{bmatrix}         0.0 &amp; 0.0 \\\\         0.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\\\     \\mathbf{N} = \\sum_{k=0}^{\\infty} \\mathbf{Q}^{k} = ( \\mathbf{I} - \\mathbf{Q} )^{-1}     = \\begin{bmatrix}         1.4 &amp; 0.6 \\\\         0.4 &amp; 1.6 \\\\     \\end{bmatrix} \\end{equation}\\]     \\(( i , j )\\) element in \\(\\mathbf{Q}^{2}\\) is the probability of visiting \\(s_{j}\\) from \\(s_{i}\\) at \\(t = 2\\), \\(p ( S_{2} = s_{j} \\vert S_{0} = s_{i}\\).   Every element in \\(\\mathbf{Q}^{\\infty}\\) reaches zero since the system falls into absorbing states.   \\(( i , j )\\) element in \\(\\mathbf{N}\\) is the number of times that the system will visit \\(s_{j}\\) from \\(s_{i}\\).   Periodic Markov Chain   A periodic MC has a single stationary distribution, but no limiting distribution since \\(\\lim_{k \\rightarrow \\infty} \\mathbf{P}^{k}\\) never converges.   \\[\\begin{equation}     \\lambda \\pi^{\\intercal} = \\mathbf{P}^{\\intercal} \\pi^{\\intercal}     \\quad \\rightarrow \\quad     \\pi     = \\begin{bmatrix}         0.25 &amp; 0.25 &amp; 0.25 &amp; 0.25 \\\\     \\end{bmatrix} \\end{equation}\\]  For \\(\\mathbf{P}^{k}\\), as \\(k \\rightarrow \\infty\\):   \\[\\begin{equation}     \\mathbf{P}     = \\begin{bmatrix}         0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\         0.0 &amp; 0.0 &amp; 0.0 &amp; 1.0 \\\\         1.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 \\\\         0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad     \\mathbf{P}^{2}     = \\begin{bmatrix}         0.5 &amp; 0.0 &amp; 0.0 &amp; 0.5 \\\\         0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\         0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\         0.5 &amp; 0.0 &amp; 0.0 &amp; 0.5 \\\\     \\end{bmatrix}     \\\\     \\mathbf{P}^{3}     = \\begin{bmatrix}         0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\         0.5 &amp; 0.0 &amp; 0.0 &amp; 0.5 \\\\         0.5 &amp; 0.0 &amp; 0.0 &amp; 0.5 \\\\         0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad     \\mathbf{P}^{4}     = \\begin{bmatrix}         0.5 &amp; 0.0 &amp; 0.0 &amp; 0.5 \\\\         0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\         0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\         0.5 &amp; 0.0 &amp; 0.0 &amp; 0.5 \\\\     \\end{bmatrix} \\end{equation}\\]  \\(\\mathbf{P}^{k}\\) oscillates between \\(\\mathbf{P}^{2}\\) and \\(\\mathbf{P}^{3}\\), and thus,   \\[\\begin{equation}     \\pi^{(0)}     = \\begin{bmatrix}         1.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad     \\pi^{(0)} \\mathbf{P}     = \\begin{bmatrix}         0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\     \\end{bmatrix}     \\\\     \\pi^{(0)} \\mathbf{P}^{2}     = \\begin{bmatrix}         0.5 &amp; 0.0 &amp; 0.0 &amp; 0.5 \\\\     \\end{bmatrix}     \\quad     \\pi^{(0)} \\mathbf{P}^{3}     = \\begin{bmatrix}         0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\\\     \\end{bmatrix} \\end{equation}\\]  So, it repeats between \\(\\begin{bmatrix} 0.5 &amp; 0.0 &amp; 0.0 &amp; 0.5 \\end{bmatrix}\\) and \\(\\begin{bmatrix} 0.0 &amp; 0.5 &amp; 0.5 &amp; 0.0 \\end{bmatrix}\\).   Ergodic Markov Chain   An ergodic MC has a limiting distribution, and so, a stationary distribution is unique.   \\[\\begin{align}     \\lambda \\pi^{\\intercal} = \\mathbf{P}^{\\intercal} \\pi^{\\intercal}     \\quad \\rightarrow \\quad &amp;     \\pi     = \\begin{bmatrix}         0.236 &amp; 0.334 &amp; 0.197 &amp; 0.233 \\\\     \\end{bmatrix}     \\\\     \\mathbf{P}     = \\begin{bmatrix}         0.2 &amp; 0.3 &amp; 0.1 &amp; 0.4 \\\\         0.2 &amp; 0.3 &amp; 0.4 &amp; 0.1 \\\\         0.5 &amp; 0.0 &amp; 0.2 &amp; 0.3 \\\\         0.1 &amp; 0.7 &amp; 0.0 &amp; 0.2 \\\\     \\end{bmatrix}     \\quad \\rightarrow \\quad &amp;     \\mathbf{P}^{\\infty}     = \\begin{bmatrix}         0.236 &amp; 0.334 &amp; 0.197 &amp; 0.233 \\\\         0.236 &amp; 0.334 &amp; 0.197 &amp; 0.233 \\\\         0.236 &amp; 0.334 &amp; 0.197 &amp; 0.233 \\\\         0.236 &amp; 0.334 &amp; 0.197 &amp; 0.233 \\\\     \\end{bmatrix}     = \\begin{bmatrix}         \\pi \\\\         \\pi \\\\         \\pi \\\\         \\pi \\\\     \\end{bmatrix} \\end{align}\\]  Under different initial distributions:   \\[\\begin{align}     \\pi^{(0)}     = \\begin{bmatrix}         1.0 &amp; 0.0 &amp; 0.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad \\rightarrow \\quad &amp;     \\pi^{(\\infty)}     = \\pi^{(0)} \\mathbf{P}^{\\infty}     = \\begin{bmatrix}         0.236 &amp; 0.334 &amp; 0.197 &amp; 0.233 \\\\     \\end{bmatrix}     = \\pi     \\\\     \\pi^{(0)}     = \\begin{bmatrix}         0.2 &amp; 0.8 &amp; 0.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad \\rightarrow \\quad &amp;     \\pi^{(\\infty)}     = \\pi^{(0)} \\mathbf{P}^{\\infty}     = \\begin{bmatrix}         0.236 &amp; 0.334 &amp; 0.197 &amp; 0.233 \\\\     \\end{bmatrix}     = \\pi     \\\\     \\pi^{(0)}     = \\begin{bmatrix}         0.0 &amp; 1.0 &amp; 0.0 &amp; 0.0 \\\\     \\end{bmatrix}     \\quad \\rightarrow \\quad &amp;     \\pi^{(\\infty)}     = \\pi^{(0)} \\mathbf{P}^{\\infty}     = \\begin{bmatrix}         0.236 &amp; 0.334 &amp; 0.197 &amp; 0.233 \\\\     \\end{bmatrix}     = \\pi \\end{align}\\]  So, the stationary distribution converges the same regardless of the initial distribution, and hence, it is the limiting distribution.   Hidden Markov Model   A hidden Markov model (HMM) is an MC with partial observability. The idea behind an HMM is that the state is hidden from the system, or a hidden state or unobserved event or latent variable. So, instead of observing a state, the system needs to infer the state from an additional variable, referred to as an observation, \\(\\mathcal{O}\\). There also exists a probability of emitting different observations for a given state, referred to as an emission probability, \\(\\mathcal{P}_{o}\\).   An emission probability also holds the Markov property. There exists an observation, \\(o \\in \\mathcal{O}\\), and an emission probability, \\(p_{o} \\in \\mathcal{P}_{o}\\), such that,   \\[\\begin{equation}     \\Pr ( O_{t} = o_{t} \\vert S_{1:t} = s_{1:t} , O_{1:t-1} = o_{1:t-1} )     \\equiv p_{o} ( O_{t} = o_{t} \\vert S_{t} = s_{t} )     = p_{o} ( o_{t} \\vert s_{t} ) \\end{equation}\\]  So, in an HMM, \\(p\\) and \\(p_{o}\\) are only dependent on the previous state and the current state, respectively.   Consider the following HMM.                              Hidden Markov model from                              Wikimedia Commons                                                                                     $$             \\begin{equation}                 \\mathbf{P}                 = \\begin{bmatrix}                     p ( X1 \\vert X1 ) &amp; p ( X2 \\vert X1 ) &amp; p ( X3 \\vert X1 ) \\\\                     p ( X1 \\vert X2 ) &amp; p ( X2 \\vert X2 ) &amp; p ( X3 \\vert X2 ) \\\\                     p ( X1 \\vert X3 ) &amp; p ( X2 \\vert X3 ) &amp; p ( X3 \\vert X3 ) \\\\                 \\end{bmatrix}                 \\\\                 \\mathbf{P}_{o}                 = \\begin{bmatrix}                     p ( y1 \\vert X1 ) &amp; p ( y2 \\vert X1 ) &amp; p ( y3 \\vert X1 ) &amp; p ( y4 \\vert X1 ) \\\\                     p ( y1 \\vert X2 ) &amp; p ( y2 \\vert X2 ) &amp; p ( y3 \\vert X2 ) &amp; p ( y4 \\vert X2 ) \\\\                     p ( y1 \\vert X3 ) &amp; p ( y2 \\vert X3 ) &amp; p ( y3 \\vert X3 ) &amp; p ( y4 \\vert X3 ) \\\\                 \\end{bmatrix}             \\end{equation}             $$                 The joint probability of all the states and observations can be expressed as:   \\[\\begin{align}     \\Pr ( S_{1:T} = s_{1:T} , O_{1:T} = o_{1:T} )     = &amp; \\Pr ( s_{1} , \\cdots , s_{T} , o_{1} , \\cdots , o_{T})     \\\\     = &amp; p_{0} ( s_{1} ) p_{o} ( o_{T} \\vert s_{T} ) \\Pi_{t=1}^{T-1} p ( s_{t+1} \\vert s_{t} ) p_{o} ( o_{t} \\vert s_{t} ) \\end{align}\\]  Where:     \\(p_{0} ( s_{1} )\\) is the initial distribution for \\(s_{1}\\).   \\(p ( s_{t+1} \\vert s_{t} )\\) is the transition probability from \\(s_{t}\\) to \\(s_{t+1}\\).   \\(p_{o} ( o_{t} \\vert s_{t} )\\) is the emission probability of \\(o_{t}\\) at \\(s_{t}\\).   In an HMM, the property is not really what we concern about, instead, the inference of hidden states is what we concern about.   Inference   There are two core inferences in an HMM.      Forward inference: The joint probability of \\(s_{t}\\) and \\(o_{1:t}\\).   \\[\\require{color} \\begin{align}     \\alpha_{1} ( s_{1} )     &amp; = \\Pr ( S_{1} = s_{1} , O_{1} = o_{1} )     \\\\     &amp; = \\textcolor{red}{ p_{o} ( o_{1} \\vert s_{1} ) p_{0} ( s_{1} ) }     \\\\     \\alpha_{2} ( s_{2} )     &amp; = \\Pr ( S_{2} = s_{2} , O_{1} = o_{1} , O_{2} = o_{2} )     \\\\     &amp; = \\textcolor{blue}{ \\sum_{s_{1} \\in \\mathcal{S}}     \\textcolor{red}{ p_{o} ( o_{1} \\vert s_{1} ) p_{0} ( s_{1} ) }     p_{o} ( o_{2} \\vert s_{2} ) p ( s_{2} \\vert s_{1} ) }     = \\sum_{s_{1} \\in \\mathcal{S}} \\alpha_{1} ( s_{1} ) p_{o} ( o_{2} \\vert s_{2} ) p ( s_{2} \\vert s_{1} )     \\\\     \\alpha_{3} ( s_{3} )     &amp; = \\Pr ( S_{3} = s_{3} , O_{1} = o_{1} , O_{2} = o_{2} , O_{3} = o_{3} )     \\\\     &amp; = \\sum_{s_{2} \\in \\mathcal{S}}     \\textcolor{blue}{ \\sum_{s_{1} \\in \\mathcal{S}}     \\textcolor{red}{ p_{o} ( o_{1} \\vert s_{1} ) p_{0} ( s_{1} ) }     p_{o} ( o_{2} \\vert s_{2} ) p ( s_{2} \\vert s_{1} ) }     p_{o} ( o_{3} \\vert s_{3} ) p ( s_{3} \\vert s_{2} )     = \\sum_{s_{2} \\in \\mathcal{S}} \\alpha_{2} ( s_{2} ) p_{o} ( o_{3} \\vert s_{3} ) p ( s_{3} \\vert s_{2} )     \\\\     &amp; \\; \\vdots     \\\\     \\alpha_{t} ( s_{t} )     &amp; = \\Pr ( S_{t} = s_{t} , O_{1:t} = o_{1:t} )     = \\sum_{s_{t-1} \\in \\mathcal{S}} \\alpha_{t-1} ( s_{t-1} )     p_{o} ( o_{t} \\vert s_{t} ) p ( s_{t} \\vert s_{t-1} ) \\end{align}\\]     Backward inference: The conditional probability of \\(o_{t+1:T}\\) for a given \\(s_{t}\\).   \\[\\require{color} \\begin{align}     \\beta_{T} ( s_{T} )     &amp; = \\Pr ( \\cdot \\vert S_{T} = s_{T} )     \\\\     &amp; = \\textcolor{red}{ p_{o} ( \\cdot \\vert \\cdot ) p ( \\cdot \\vert s_{T} ) } = 1     \\\\     \\beta_{T-1} ( s_{T-1} )     &amp; = \\Pr ( O_{T} = o_{T} \\vert S_{T-1} = s_{T-1} )     \\\\     &amp; = \\textcolor{blue}{ \\sum_{s_{T} \\in \\mathcal{S}}     \\textcolor{red}{ p_{o} ( \\cdot \\vert \\cdot ) p ( \\cdot \\vert s_{T} ) }     p_{o} ( o_{T} \\vert s_{T} ) p ( s_{T} \\vert s_{T-1} ) }     = \\sum_{s_{T} \\in \\mathcal{S}} \\beta_{T} ( s_{T} ) p_{o} ( o_{T} \\vert s_{T} ) p ( s_{T} \\vert s_{T-1} )     \\\\     \\beta_{T-2} ( s_{T-2} )     &amp; = \\Pr ( O_{T-1} = o_{T-1} , O_{T} = o_{T} \\vert S_{T-2} = s_{T-2} )     \\\\     &amp; = \\sum_{s_{T-1} \\in \\mathcal{S}}     \\textcolor{blue}{ \\sum_{s_{T} \\in \\mathcal{S}}     \\textcolor{red}{ p_{o} ( \\cdot \\vert \\cdot ) p ( \\cdot \\vert s_{T} ) }     p_{o} ( o_{T} \\vert s_{T} ) p ( s_{T} \\vert s_{T-1} ) }     p_{o} ( o_{T-1} \\vert s_{T-1} ) p ( s_{T-1} \\vert s_{T-2} )     \\\\     &amp; = \\sum_{s_{T-1} \\in \\mathcal{S}} \\beta_{T-1} ( s_{T-1} ) p_{o} ( o_{T-1} \\vert s_{T-1} ) p ( s_{T-1} \\vert s_{T-2} )     \\\\     &amp; \\; \\vdots     \\\\     \\beta_{t} ( s_{t} )     &amp; = \\Pr ( O_{t+1:T} = o_{t+1:T} \\vert S_{t} = s_{t} )     = \\sum_{s_{t+1} \\in \\mathcal{S}}     \\beta_{t+1} ( s_{t+1} ) p_{o} ( o_{t+1} \\vert s_{t+1} ) p ( s_{t+1} \\vert s_{t} ) \\end{align}\\]  Together, we can derive two important inferences.          The probability of the system visiting \\(s_{i}\\) at \\(t\\) for a given \\(o_{1:T}\\).   \\[\\begin{align}       \\gamma_{t} ( s_{i} )       &amp; = \\Pr ( S_{t} = s_{i} \\vert O_{1:T} = o_{1:T} )       \\\\       &amp; = \\frac{\\Pr ( S_{t} = s_{i} , O_{1:T} = o_{1:T} )}       {\\Pr ( O_{1:T} = o_{1:T} )}       \\\\       &amp; = \\frac{ \\Pr ( S_{t} = s_{i} , O_{1:t} = o_{1:t} ) \\Pr ( O_{t+1:T} = o_{t+1:T} \\vert S_{t} = s_{i} ) }       { \\sum_{i'} \\Pr ( S_{t} = s_{i'} , O_{1:t} = o_{1:t} ) \\Pr ( O_{t+1:T} = o_{t+1:T} \\vert S_{t} = s_{i'} ) }       \\\\       &amp; = \\frac{ \\alpha_{t} ( s_{i} ) \\beta_{t} ( s_{i} ) }       { \\sum_{s_{j} \\in \\mathcal{S}} \\alpha_{t} ( s_{j} ) \\beta_{t} ( s_{j} ) }   \\end{align}\\]             \\(\\sum_{t=1}^{T-1} \\gamma_{t} ( s_{i} )\\) is the expected number of the visitations to \\(s_{i}\\).                The probability of \\(s_{i}\\) at \\(t\\) and \\(s_{j}\\) at \\(t+1\\) for a given \\(o_{1:T}\\).   \\[\\begin{align}       \\xi_{t} ( s_{i} , s_{j} )       &amp; = \\Pr ( S_{t} = s_{i} , S_{t+1} = s_{j} \\vert O_{1:T} = o_{1:T} )       \\\\       &amp; = \\frac{\\Pr ( S_{t} = s_{i} , S_{t+1} = s_{j} , O_{1:T} = o_{1:T} )}       {\\Pr ( O_{1:T} = o_{1:T} )}       \\\\       &amp; = \\frac{ p_{o} ( o_{t+1} \\vert s_{j} ) p ( s_{j} \\vert s_{i} ) \\alpha_{t} ( s_{i} ) \\beta_{t+1} ( s_{j} ) }       { \\sum_{s_{l} \\in \\mathcal{S}} \\sum_{s_{k} \\in \\mathcal{S}} p_{o} ( o_{t+1} \\vert s_{l} ) p ( s_{l} \\vert s_{k} ) \\alpha_{t} ( s_{k} ) \\beta_{t+1} ( s_{l} ) }   \\end{align}\\]             \\(\\sum_{t=1}^{T-1} \\xi_{t} ( s_{i} , s_{j} )\\) is the expected number of the visitations to \\(s_{i}\\) at \\(t\\) and \\(s_{j}\\) at \\(t+1\\).       \\(\\sum_{s_{j} \\in \\mathcal{S}} \\xi_{t} ( s_{i} , s_{j} ) = \\gamma_{t} ( s_{i} )\\) since summing all the visitations to \\(s_{j}\\) at \\(t+1\\) in \\(\\xi_{t} ( s_{i} , s_{j} )\\) only leaves the expected number of the visitations to \\(s_{i}\\) at \\(t\\).           Using these four basic inferences, we can infer hidden states in an HMM. Note that literatures often use notations, \\(\\pi_{i} = p_{0} ( S_{1} = s_{i} )\\), \\(a_{ij} = p ( X_{t} = s_{j} \\vert X_{t-1} = s_{i} )\\), and \\(b_{j} ( o_{i} ) = p_{o} ( O_{t} = o_{i} \\vert S_{t} = s_{j} )\\).   Baum-Welch Algorithm   Most real-world problems do not provide environment dynamics, so \\(p_{0}\\), \\(p\\) and \\(p_{o}\\) are unknown. The Baum-Welch algorithm, a type of expectation maximization algorithm, allows us to obtain these environment dynamics.           Expectation step   \\[\\begin{align}      \\gamma_{t} ( s_{i} )      = \\frac{ \\alpha_{t} ( s_{i} ) \\beta_{t} ( s_{i} ) }      { \\sum_{s_{j} \\in \\mathcal{S}} \\alpha_{t} ( s_{j} ) \\beta_{t} ( s_{j} ) }      \\quad      \\xi_{t} ( s_{i} , s_{j} )      = \\frac{ p_{o} ( o_{t+1} \\vert s_{j} ) p ( s_{j} \\vert s_{i} ) \\alpha_{t} ( s_{i} ) \\beta_{t+1} ( s_{j} ) }      { \\sum_{s_{l} \\in \\mathcal{S}} \\sum_{s_{k} \\in \\mathcal{S}} p_{o} ( o_{t+1} \\vert s_{l} ) p ( s_{l} \\vert s_{k} ) \\alpha_{t} ( s_{k} ) \\beta_{t+1} ( s_{l} ) }  \\end{align}\\]             \\(\\gamma_{t} ( s_{i} )\\) is the probability of \\(s_{i}\\) at \\(t\\) for a given \\(o_{1:T}\\).       \\(\\xi_{t} ( s_{i} , s_{j} )\\) is the probability of \\(s_{i}\\) at \\(t\\) and \\(s_{j}\\) at \\(t+1\\) for a given \\(o_{1:T}\\).                Maximization step   \\[\\begin{align}      p_{0} ( s_{i} ) = \\gamma_{1} ( s_{i} )      \\quad      p ( s_{j} \\vert s_{i} ) = \\frac{ \\sum_{t=1}^{T-1} \\xi_{t} ( s_{i} , s_{j} ) }      { \\sum_{t=1}^{T-1} \\gamma_{t} ( s_{i} ) }      \\quad      p_{o} ( o_{i} \\vert s_{i} ) = \\frac{ \\sum_{t=1}^{T-1} \\gamma_{t} ( s_{i} ) \\mathbb{1} ( o_{t} = o_{i} ) }      { \\sum_{t=1}^{T-1} \\gamma_{t} ( s_{i} ) }  \\end{align}\\]             \\(\\gamma_{1} ( s_{i} )\\) is the probability of \\(s_{i}\\) at \\(t = 1\\) for a given \\(o_{1:T}\\).       \\(\\frac{ \\sum_{t=1}^{T-1} \\xi_{t} ( s_{i} , s_{j} ) }{ \\sum_{t=1}^{T-1} \\gamma_{t} ( s_{i} ) }\\) is the probability of \\(s_{j}\\) at \\(t+1\\) for a given \\(s_{i}\\) at \\(t\\) and \\(o_{1:T}\\).       \\(\\frac{ \\sum_{t=1}^{T-1} \\gamma_{t} ( s_{i} ) \\mathbb{1} ( o_{t} = o_{i} ) }{ \\sum_{t=1}^{T-1} \\gamma_{t} ( s_{i} ) }\\) is the probability of \\(o_{i}\\) at \\(t\\) for a given \\(s_{i}\\) at \\(t\\) and \\(o_{1:T}\\).       \\(p_{0} ( s_{i} )\\), \\(p ( s_{j} \\vert s_{i} )\\) and \\(p_{o} ( o_{i} \\vert s_{i} )\\) are normalized at each maximization step.           Since the probability is a fraction, multiplying them continuously will approach zero due to the precision. To avoid this underflow, often normalization is applied to \\(\\alpha\\) and \\(\\beta\\).   \\[\\begin{align}     \\hat{\\alpha} ( s_{t} )     = \\frac{ \\alpha ( s_{t} ) }{ \\sum_{s_{t’}} \\alpha ( s_{t’} ) }     \\quad     \\hat{\\beta} ( s_{t} )     = \\frac{ \\beta ( s_{t} ) }{ \\sum_{s_{t’}} \\beta ( s_{t’} ) } \\end{align}\\]  Summary   In this post, Markov models with an autonomous system in a discrete-time setting are covered.   A transition probability in an MC follows the Markov property.   \\[\\begin{align}     \\Pr ( S_{t+1} = s_{t+1} \\vert S_{0:t} = s_{0:t} )     \\equiv p ( S_{t+1} = s_{t+1} \\vert S_{t} = s_{t} )     = p ( s_{t+1} \\vert s_{t} ) \\end{align}\\]  An MC is all about the properties, which can be determined by:     Is an MC countably finite or infinite?            If countably infinite MC, it can be any of transient, null recurrent or positive recurrent, more theories behind.                    A limiting distribution and stationary distribution may or may not exist.                       If countably finite MC, it has one or more stationary distributions. Next question.           Is an MC absorbing or irreducible?            If absorbing MC, it is composed of positive recurrent absorbing states/chains and transient states.                    A limiting distribution does not exist, but more than one stationary distribution exist.           Fundamental matrix, \\(\\mathbf{N}\\):           \\[\\begin{equation}       \\mathbf{N}       = \\sum_{k=0}^{\\infty} \\mathbf{Q}^{k}       = ( \\mathbf{I} - \\mathbf{Q} )^{-1}       \\quad       \\text{where, }       \\mathbf{P}       = \\begin{bmatrix}           \\mathbf{Q} &amp; \\mathbf{R} \\\\           \\mathbf{0} &amp; \\mathbf{I} \\\\       \\end{bmatrix}       ,       \\lim_{ k \\rightarrow \\infty } \\mathbf{Q}^{k} = \\mathbf{0}   \\end{equation}\\]              If irreducible MC, it is composed of only positive recurrent states with no transient state. Next question.           Is an MC periodic or aperiodic?            If periodic MC, it returns to a particular state at some rates \\(\\{ n , 2n , 3n , \\cdots \\}\\), where \\(n &gt; 1\\).                    A limiting distribution does not exist, but a single stationary distribution exists.                       If aperiodic MC, an MC is said to be an ergodic MC.                    A limiting distribution exists and is equivalent to a stationary distribution.           \\[\\begin{equation}       \\pi_{i}^{(\\infty)} = \\lim_{n \\rightarrow \\infty} \\Pr ( S_{n} = s_{i} \\vert S_{0} = s ) &gt; 0       \\\\       \\pi = \\pi \\mathbf{P}       \\equiv       \\pi^{(\\infty)} = \\pi^{(\\infty)} \\mathbf{P}       \\equiv       \\pi^{(0)} \\mathbf{P}^{\\infty}   \\end{equation}\\]                  An emission probability in an HMM follows the Markov property.   \\[\\begin{align}     \\Pr ( O_{t} = o_{t} \\vert S_{1:t} = s_{1:t} , O_{1:t-1} = o_{1:t-1} )     \\equiv p_{o} ( O_{t} = o_{t} \\vert S_{t} = s_{t} )     = p_{o} ( o_{t} \\vert s_{t} ) \\end{align}\\]  An HMM is all about the inferences.   \\[\\begin{align}     \\alpha_{1} ( s_{1} )     &amp; = \\Pr ( S_{1} = s_{1} , O_{1} = o_{1} )     = p_{o} ( o_{1} \\vert s_{1} ) p_{0} ( s_{1} )     \\\\     \\alpha_{t} ( s_{t} )     &amp; = \\Pr ( S_{t} = s_{t} , O_{1:t} = o_{1:t} )     = \\sum_{s_{t-1} \\in \\mathcal{S}} \\alpha_{t-1} ( s_{t-1} )     p_{o} ( o_{t} \\vert s_{t} ) p ( s_{t} \\vert s_{t-1} )     \\\\     \\beta_{t} ( s_{t} )     &amp; = \\Pr ( O_{t+1:T} = o_{t+1:T} \\vert S_{t} = s_{t} )     = \\sum_{s_{t+1} \\in \\mathcal{S}}     \\beta_{t+1} ( s_{t+1} ) p_{o} ( o_{t+1} \\vert s_{t+1} ) p ( s_{t+1} \\vert s_{t} )     \\\\     \\beta_{T} ( s_{T} )     &amp; = \\Pr ( \\cdot \\vert S_{T} = s_{T} )     = p_{o} ( \\cdot \\vert \\cdot ) p ( \\cdot \\vert s_{T} ) = 1     \\\\     \\gamma_{t} ( s_{i} )     &amp; = \\Pr ( S_{t} = s_{i} \\vert O_{1:T} = o_{1:T} )     = \\frac{ \\alpha_{t} ( s_{i} ) \\beta_{t} ( s_{i} ) }     { \\sum_{s_{j} \\in \\mathcal{S}} \\alpha_{t} ( s_{j} ) \\beta_{t} ( s_{j} ) }     \\\\     \\xi_{t} ( s_{i} , s_{j} )     &amp; = \\Pr ( S_{t} = s_{i} , S_{t+1} = s_{j} \\vert O_{1:T} = o_{1:T} )     = \\frac{ p_{o} ( o_{t+1} \\vert s_{j} ) p ( s_{j} \\vert s_{i} ) \\alpha_{t} ( s_{i} ) \\beta_{t+1} ( s_{j} ) }     { \\sum_{s_{l} \\in \\mathcal{S}} \\sum_{s_{k} \\in \\mathcal{S}} p_{o} ( o_{t+1} \\vert s_{l} ) p ( s_{l} \\vert s_{k} ) \\alpha_{t} ( s_{k} ) \\beta_{t+1} ( s_{l} ) } \\end{align}\\]  Since the environment dynamics in an HMM, \\(p_{0}\\), \\(p\\) and \\(p_{o}\\), are usually unknown in practice, the Baum-Welch algorithm is used to estimate them.           Expectation step   \\[\\begin{align}      \\gamma_{t} ( s_{i} )      = \\frac{ \\alpha_{t} ( s_{i} ) \\beta_{t} ( s_{i} ) }      { \\sum_{s_{j} \\in \\mathcal{S}} \\alpha_{t} ( s_{j} ) \\beta_{t} ( s_{j} ) }      \\quad      \\xi_{t} ( s_{i} , s_{j} )      = \\frac{ p_{o} ( o_{t+1} \\vert s_{j} ) p ( s_{j} \\vert s_{i} ) \\alpha_{t} ( s_{i} ) \\beta_{t+1} ( s_{j} ) }      { \\sum_{s_{l} \\in \\mathcal{S}} \\sum_{s_{k} \\in \\mathcal{S}} p_{o} ( o_{t+1} \\vert s_{l} ) p ( s_{l} \\vert s_{k} ) \\alpha_{t} ( s_{k} ) \\beta_{t+1} ( s_{l} ) }  \\end{align}\\]           Maximization step   \\[\\begin{align}      p_{0} ( s_{i} ) = \\gamma_{1} ( s_{i} )      \\quad      p ( s_{j} \\vert s_{i} ) = \\frac{ \\sum_{t=1}^{T-1} \\xi_{t} ( s_{i} , s_{j} ) }      { \\sum_{t=1}^{T-1} \\gamma_{t} ( s_{i} ) }      \\quad      p_{o} ( o_{i} \\vert s_{i} ) = \\frac{ \\sum_{t=1}^{T-1} \\gamma_{t} ( s_{i} ) \\mathbb{1} ( o_{t} = o_{i} ) }      { \\sum_{t=1}^{T-1} \\gamma_{t} ( s_{i} ) }  \\end{align}\\]      Reference      Lecture Notes on Stochastic Modeling I   Basics of Applied Stochastic Processes  ","categories": ["Mathematics&#x003a; Markov Model"],
        "tags": [],
        "url": "/posts/mathematics/markov_model_1_markov_chain/",
        "teaser": null
      },{
        "title": "Markov Decision Process",
        "excerpt":"     Prerequisites      Markov Chain          All is number.           Pythagoras   Humans have invented and developed languages to symbolize entities, constructing Matrix that captures the essence of reality. On the other hand, any form of entities can be modelled as number, from natural to complex numbers and from points to objects, which existed long before human civilization. Thereby, the universe itself is inherently number, or in short, all is number.   Mathematics is the study of numbers, so if an entity exists, there exists mathematics to describe it, even if we cannot perceive. This is why we (arguably) refer mathematics as discovery instead of invention and often considered the most fundamental and precise way of describing reality. Mathematics is a vast, diverse and abstract field such that it requires books of worth. Here, a few branches of mathematics are focused on.   A Markov model is a stochastic process that describes pseudo-randomly changing systems under the Markov property, or the Markov assumption. The Markov property stipulates the future state depends only on the current state, not the history of states. There are four common Markov models:                                        Fully Observable                               Partially Observable                                         Autonomous System                               Markov Chain                               Hidden Markov Model                             Controllable System                       Markov Decision Process                               Partially Observable Markov Decision Process                 A Markov decision process is a type of Markov models used in decision making problems, which describes the world with a controllable system. It is used to model board games, robot locomotions, and more recently language models. This post aims to provide a tutorial on a Markov decision process and its variants, including a partially observable Markov decision process.   Decision Making   Let the agent navigate the world and execute an action. To model decision making problems, we need to have the following three concepts.     A state, \\(s \\in \\mathcal{S}\\), is a computational model of the world that the agent observes, i.e. hungry state, an enemy is in front of me state, or an instance of a board game.   An action, \\(a \\in \\mathcal{A}\\), is an incremental move that the agent takes to change the state, i.e. eat action for hungry state, or attack action for an enemy is in front of me state.   A reward, \\(r \\in \\mathcal{R}\\), is a quantity that drives the agent to produce an appropriate action for a given state. It can be positive to encourage or negative to suppress a particular action, i.e. positive rewards on reaching full state by executing eat action from hungry state while negative on reaching hungrier state by executing run action from hungry state.   However, the model that accounts for these three concepts is intractable. Here, I will go through a Markov decision process in a countably finite discrete-time setting. It employs the Markov property to simplify the world into a tractable model. Note that an agent is a system that executes an action, so the term agent is often used in decision making problems while the term system is for autonomous MCs.   Markov Decision Process  A Markov Decision Process (MDP) is a Markov chain (MC) with actions for controllability and rewards for motivation. It is a tuple of \\(( \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma )\\), where \\(\\mathcal{S}\\) is the state space, \\(\\mathcal{A}\\) is the action set, \\(\\mathcal{P}\\) is the state transition probability, \\(\\mathcal{R}\\) is the reward function, and \\(\\gamma\\) is the discount factor.   In an MDP, the agent ought to 1) execute actions, \\(a \\in \\mathcal{A}\\); 2) to navigate to the states, \\(s \\in \\mathcal{S}\\); 3) that hold positive rewards, \\(r \\in \\mathcal{R}\\). Due to the presence of \\(\\mathcal{A}\\), an MDP has two main differences compared to an MC.     There exists a probability distribution over actions for a given history of states and actions, referred to as a policy, \\(\\pi ( A_{t} = a_{t} \\vert S_{0:t} = s_{0:t} ) \\in \\Pi\\).   A transition probability is often referred to as a state transition probability and is a function of a history of states and actions, \\(p ( S_{t+1} = s_{t+1} \\vert S_{0:t} = s_{0:t} , A_{0:t} = a_{0:t} ) \\in \\mathcal{P}\\).   Thus, more precisely, in an MDP, the agent ought to 1) execute actions, \\(a \\in \\mathcal{A}\\); 2) following the policy, \\(\\pi \\in \\Pi\\); 3) to navigate to the states, \\(s \\in \\mathcal{S}\\); 4) through state transition probabilities, \\(p \\in \\mathcal{P}\\); 5) that hold positive rewards, \\(r \\in \\mathcal{R}\\).   Under the Markov property, \\(\\Pi\\), \\(\\mathcal{P}\\) and \\(\\mathcal{R}\\) can be expressed as:   \\[\\begin{equation}     \\Pr ( A_{t} = a_{t} \\vert S_{0:t} = s_{0:t} )     \\equiv \\pi ( A_{t} = a_{t} \\vert S_{t} = s_{t} )     = \\pi ( a_{t} \\vert s_{t} , a_{t-1} )     \\\\     \\Pr ( S_{t+1} = s_{t+1} \\vert S_{0:t} = s_{0:t} , A_{0:t} = a_{0:t} )     \\equiv p ( S_{t+1} = s_{t+1} \\vert S_{t} = s_{t} , A_{t} = a_{t} )     = p ( s_{t+1} \\vert s_{t} , a_{t} )     \\\\     r ( S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} )     \\equiv r ( S_{t} = s_{t} , A_{t-1} = a_{t-1} , S_{t-1} = s_{t-1} )     = r ( s_{t} , a_{t-1} , s_{t-1} ) \\end{equation}\\]  Note that a reward is expressed as \\(\\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}\\), but this is a design choice, where some define it as \\(\\mathcal{R}: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}\\) or \\(\\mathcal{R}: \\mathcal{S} \\rightarrow \\mathbb{R}\\).   A collection of states and actions that the agent travelled for a given maximum time, \\(T\\), is referred to as a trajectory, \\(\\tau = ( S_{0} = s_{0} , A_{0} = a_{0} , \\cdots  A_{T-1} = a_{T-1} , S_{T} = s_{T} )\\).     If \\(T \\rightarrow \\infty\\), an MDP is referred to as an average-reward continuous MDP, where the task may continue forever. The environment usually provides a reward at each time step.   If \\(T &lt; \\infty\\), an MDP is referred to as a start-state episodic MDP, where the task has a clear ending. The environment usually provides the sum of rewards at each episode.   Here, I will focus on an episodic MDP. An episode may finish if a player died in gameplay or a time step reaches the maximum time in robot locomotion.   The objective of an MDP is to design an agent that produces the optimal policy, \\(\\pi^{\\ast} \\in \\Pi\\), that receives as high total cumulative rewards as possible in the shortest travel distance. Consider the following MDP.                              Markov decision process from                              Wikimedia Commons                                                                                     $$             \\begin{equation}                 \\mathcal{S} = \\{ s_{0} , s_{1} , s_{2} \\}                 \\qquad                 \\mathcal{A} = \\{ a_{0} , a_{1} \\}                 \\\\                 r ( s_{0} , a_{1} , s_{2} ) = -1                 \\qquad                 r ( s_{0} , a_{0} , s_{1} ) = +5             \\end{equation}             $$                                         $$             \\begin{align}                 \\mathbf{P} ( a_{0} )                 &amp; = \\begin{bmatrix}                     p ( s_{0} \\vert s_{0} , a_{0} ) &amp; p ( s_{1} \\vert s_{0} , a_{0} ) &amp; p ( s_{2} \\vert s_{0} , a_{0} ) \\\\                     p ( s_{0} \\vert s_{1} , a_{0} ) &amp; p ( s_{1} \\vert s_{1} , a_{0} ) &amp; p ( s_{2} \\vert s_{1} , a_{0} ) \\\\                     p ( s_{0} \\vert s_{2} , a_{0} ) &amp; p ( s_{1} \\vert s_{2} , a_{0} ) &amp; p ( s_{2} \\vert s_{2} , a_{0} ) \\\\                 \\end{bmatrix}                 \\\\                 &amp; = \\begin{bmatrix}                     0.5 &amp; 0.0 &amp; 0.5 \\\\                     0.7 &amp; 0.1 &amp; 0.2 \\\\                     0.4 &amp; 0.0 &amp; 0.6 \\\\                 \\end{bmatrix}                 \\\\                 \\mathbf{P} ( a_{1} )                 &amp; = \\begin{bmatrix}                     p ( s_{0} \\vert s_{0} , a_{1} ) &amp; p ( s_{1} \\vert s_{0} , a_{1} ) &amp; p ( s_{2} \\vert s_{0} , a_{1} ) \\\\                     p ( s_{0} \\vert s_{1} , a_{1} ) &amp; p ( s_{1} \\vert s_{1} , a_{1} ) &amp; p ( s_{2} \\vert s_{1} , a_{1} ) \\\\                     p ( s_{0} \\vert s_{2} , a_{1} ) &amp; p ( s_{1} \\vert s_{2} , a_{1} ) &amp; p ( s_{2} \\vert s_{2} , a_{1} ) \\\\                 \\end{bmatrix}                 \\\\                 &amp; = \\begin{bmatrix}                     0.0 &amp; 0.0 &amp; 1.0 \\\\                     0.0 &amp; 0.95 &amp; 0.05 \\\\                     0.3 &amp; 0.3 &amp; 0.4 \\\\                 \\end{bmatrix}             \\end{align}             $$                 Since the objective is to acquire high rewards, the agent should move towards positive rewards and avoid negative rewards. Therefore, the optimal policy, \\(\\pi^{\\ast} \\in \\Pi\\) is:   \\[\\begin{equation}     \\pi^{\\ast} ( a \\vert s )     = \\begin{bmatrix}         \\pi ( a_{0} \\vert s_{0} ) &amp; \\pi ( a_{1} \\vert s_{0} ) \\\\         \\pi ( a_{0} \\vert s_{1} ) &amp; \\pi ( a_{1} \\vert s_{1} ) \\\\         \\pi ( a_{0} \\vert s_{2} ) &amp; \\pi ( a_{1} \\vert s_{2} ) \\\\     \\end{bmatrix}     = \\begin{bmatrix}         0.0 &amp; 1.0 \\\\         1.0 &amp; 0.0 \\\\         0.0 &amp; 1.0 \\\\     \\end{bmatrix} \\end{equation}\\]  A policy can be:     Deterministic: The choice of action is non-probabilistic, so the next action is produced for a given state, or \\(a = \\pi ( a \\vert s )\\).   Stochastic: The choice of action is probabilistic, so the next action is sampled from policy distribution, or \\(a \\sim \\pi ( a \\vert s ) \\in \\mathbb{R}^{m}\\), where \\(\\sum_{a \\in \\mathcal{A}} \\pi ( a \\vert s ) = 1\\).   Any type of policy can be used for any type of MDPs, i.e. a deterministic policy in a stochastic MDP or a stochastic MDP in a deterministic MDP.   Both have pros and cons. In the optimal world, where you can fully observe how the environment dynamics work, which is the above example, the optimal policy is deterministic, but in the real world, the stochastic policy is usually preferred because:     Deterministic policy lacks diversity (exploration-exploitation dilemma).   The agent does not fully observe states in many practical scenarios (partial observability).   A good example is an aliased gridworld from Stanford CS234 lecture.   Ergodicity   The ergodicity is a good property for an MDP to hold, but this is more complicated than an MC due to the presence of the actions and the policy. Consider that the agent is in an enemy is in front of me state with two available actions, attack and wait. When the agent executes attack, the agent may or may not defeat the enemy.     With \\(p (\\) an enemy is defeated \\(\\vert\\) an enemy is in front of me, attack \\()\\), the agent reaches an enemy is defeated state.   With \\(p (\\) an enemy dodged attack \\(\\vert\\) an enemy is in front of me, attack \\()\\), the agent reaches an enemy dodged attack state.   The agent might require to execute attack until it reaches an enemy is defeated state. However, if the agent executes wait, it will reach an enemy killed you state, or \\(p (\\) an enemy killed you \\(\\vert\\) an enemy is in front of me, wait \\() = 1\\). If the agent dies, it can never return to an enemy is in front of me state again.     If there is even a small chance of selecting wait, \\(\\pi (\\) wait \\(\\vert\\) an enemy is in front of me \\() &gt; 0\\), an MDP has absorbing states or chains.   If there is no chance of selecting wait, \\(\\pi (\\) wait \\(\\vert\\) an enemy is in front of me \\() = 0\\), an MDP has only positive recurrent states.   However, unlike an MC, having absorbing states or chains does not mean that the MDP is absorbing. More formally, define a state stationary distribution, stationary distribution or steady-state distribution, \\(d ( s )\\):   \\[\\begin{align}     d ( s )     = &amp; \\lim_{t \\rightarrow \\infty} \\Pr (S_{t} = s \\vert s_{0} , \\pi)     \\\\     \\equiv &amp; \\sum_{k=0}^{\\infty} \\Pr ( S_{0} = s_{0} \\rightarrow S_{k} = s \\vert k , \\pi)     = \\sum_{k=0}^{\\infty} \\Pr (s_{0} \\rightarrow s \\vert k , \\pi) \\end{align}\\]  Similar to an MC, it is the probability distribution over states that you will end up if you travel an MDP infinitely under \\(\\pi\\).     The first term signifies that for a given \\(S_{0} = s_{0}\\), the probability that the agent travels to \\(s\\) when \\(t \\rightarrow \\infty\\) following \\(\\pi\\).   The second term signifies the summation of the probability that the agent travels from \\(S_{0} = s_{0}\\) to \\(s\\) at \\(k\\) number of steps following \\(\\pi\\) across \\(k = 0\\) to \\(\\infty\\). This is often referred to as a state visitation probability.   \\[\\begin{align}     \\Pr (s \\rightarrow s \\vert 0 , \\pi )     = &amp; 1     \\\\     \\Pr (s \\rightarrow s' \\vert 0 , \\pi )     = &amp; 0     \\\\     \\Pr (s \\rightarrow s' \\vert 1 , \\pi )     = &amp; \\sum_{a \\in \\mathcal{A}} \\pi ( a \\vert s ) p ( s' \\vert s , a )     \\\\     \\Pr (s \\rightarrow s'' \\vert k , \\pi )     = &amp; \\sum_{s' \\in \\mathcal{S}} \\Pr (s \\rightarrow s' \\vert n, \\pi ) \\cdot \\Pr (s' \\rightarrow s'' \\vert k - n , \\pi ) \\end{align}\\]  Furthermore, \\(d\\) can be represented recursively.   \\[\\begin{equation}     d ( s' )     = \\sum_{s \\in \\mathcal{S}} d ( s )     \\sum_{a \\in \\mathcal{A}} \\pi ( a \\vert s ) p ( s' \\vert s , a ) \\end{equation}\\]  Before getting into the ergodicity, there is one additional class of an MDP. An unichain MDP is an MDP that guarantees a unique stationary state distribution for every policy, but its visitation is not guaranteed for every state. This means that the unichain MDP has a unique stationary state distribution that contains \\(d (s) = 0\\) for some \\(s\\), therefore, not ergodic.   There seems to be no formal definition on the ergodicity in an MDP, but three different definitions I found from What is ergodicity in a Markov Decision Process (MDP)? are:     There exists a policy \\(\\pi\\) for a unique stationary distribution, \\(d ( s )\\), such that \\(d ( s ) &gt; 0\\) from Moldovan.   For every policy \\(\\pi\\), a unique stationary distribution exists, \\(d ( s )\\), such that \\(d ( s ) &gt; 0\\) from Puterman.   For every policy \\(\\pi\\), a unique stationary distribution exists, \\(d ( s )\\), such that \\(d ( s ) \\geq 0\\) from Sutton.   Since the policy is usually learned, any of the definitions has a risk of falling into absorbing states. Most reinforcement learning takes the third definition, but it is not truly ergodic since a limiting distribution may reach zero for some \\(\\pi\\).     In many reinforcement learning problems, an ergodicity assumption breaks since closed irreducible sets are present in an MDP, i.e. the permanent damages to robots or the progress of the game stage. There is one interesting perspective on how the ergodicity deals with the safety of the agent, Safe Exploration in Markov Decision Processes by Moldovan.   The research towards ergodicity in an MDP appears to be more related to operations research, so if you are interested, take a look at operations research.   Value Function   The optimal policy is to guide the agent towards states with the highest total cumulative rewards.   \\[\\begin{equation}     \\pi^{\\ast} ( a \\vert s ) = \\arg \\max_{\\pi} \\mathbb{E} [ \\sum_{t = 0}^{\\infty} r ( s_{t} ) ] \\end{equation}\\]  However, the agent must pursue the highest total cumulative rewards in the shortest travel distance. \\(\\sum_{t = 0}^{\\infty} r ( s_{t} )\\) weights the significance of the rewards in short distance and long distance equal. Then, the agent may potentially:     Fall into an infinite loop, where it repetitively visits the same states with rewards.   Aim for only large but distant rewards, where it ignores many small rewards in a short distance.   To comprehend the length of the travel distance into the optimal policy, we include a discount factor inside the optimization, often referred to as a return that is the total discounted cumulative rewards that the agent received throughout the trajectory, \\(R_{t} = \\sum_{k = 0}^{\\infty} \\gamma^{k} r ( s_{t + k + 1} )\\).   \\[\\begin{equation}     \\pi^{\\ast} ( a \\vert s )     = \\arg \\max_{\\pi} \\mathbb{E} [ R_{t} ]     = \\arg \\max_{\\pi} \\mathbb{E} [ \\sum_{k = 0}^{\\infty} \\gamma^{k} r ( s_{t + k + 1} ) ] \\end{equation}\\]  Appropriately selecting \\(\\gamma \\in [0,1]\\) diminishes the effects of the long-distance rewards when computing the return.     If \\(\\gamma = 0\\), the agent only cares about rewards in a single step, or short-sighted, which may result in the agent not learning anything if the rewards are sparse.   If \\(\\gamma = 1\\), the agent cares about rewards of the infinite horizon, or long-sighted, which may fall into an infinite loop or care about high but far-to-reach rewards.   You can view short-sighted and long-sighted from the Stanford marshmallow experiment. A discount factor has a mathematical property that is bounded by \\(\\sum_{k = 0}^{\\infty} \\gamma^{k} = \\frac{ 1 }{ 1 - \\gamma }\\). There are other ways to balance short- and long-sighted and they have different mathematic properties.   Additionally, \\(d\\) also includes a discount factor in its expression.   \\[\\begin{align}     d ( s )     = &amp; \\lim_{t \\rightarrow \\infty} \\gamma^{t} \\Pr (S_{t} = s \\vert s_{0} , \\pi) \\end{align}\\]  To make the mathematical expression of the optimization more flexible for reinforcement learning algorithms, we use the expectation of a return, or a value function.   \\[\\begin{equation}     v ( s )     = \\mathbb{E} [ R_{t} \\vert S = s ]     \\qquad     q ( s , a )     = \\mathbb{E} [ R_{t} \\vert S = s , A = a ] \\end{equation}\\]  Where \\(v\\) is a state value function or state value, and \\(q\\) is a state-action value function or Q value. \\(v\\) quantifies how good a particular state is based on the expected return from the state following the policy while \\(q\\) deals with a state and action pair.   Thus, the optimal policy is expressed as:   \\[\\begin{align}     \\pi^{\\ast} ( a \\vert s )     = &amp; \\arg \\max_{\\pi} \\mathbb{E} [ R_{t} ]     \\\\     \\equiv &amp; \\arg \\max_{\\pi} \\mathbb{E} [ v ( s ) ]     = \\arg \\max_{\\pi} \\sum_{s \\in \\mathcal{S}} d ( s ) v ( s )     \\\\     \\equiv &amp; \\arg \\max_{\\pi} \\mathbb{E} [ q ( s , a ) ]     = \\arg \\max_{\\pi} \\sum_{s \\in \\mathcal{S}} d ( s ) \\sum_{a \\in \\mathcal{A}} \\pi ( a \\vert s ) q ( s , a ) \\end{align}\\]  An average-reward continuous MDP formalizes the value functions differently. It represents the expected average reward without a discount factor.   \\[\\begin{equation}     v ( s ) = \\sum_{t = 1}^{\\infty} \\mathbb{E} \\left[ r_{t} - R_{t} \\vert S = s \\right]     \\qquad     q ( s , a ) = \\sum_{t = 1}^{\\infty} \\mathbb{E} \\left[ r_{t} - R_{t} \\vert S = s , A = a \\right] \\end{equation}\\]  Bellman Equation   In an MDP, the Bellman equation is a recursion of value functions. The fundamental idea is more abstract, where it deals with a necessary condition for optimality in dynamic programming, but here, I will just cover the simplest application of the Bellman equation in an MDP.   The Bellman expectation equation is a type of Bellman equations that decomposes the value function recursively into a reward and discounted future value function. Consider a white circle as a state and a black dot as an action:                              State value                               Q value                                                                                                                 $$             \\begin{align}                 v (s) = \\sum_{a \\in A} \\pi (a \\vert s) q (s, a)             \\end{align}             $$                               $$             \\begin{align}                 q (s, a) = \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r ( s , a , s' ) + \\gamma v (s') \\right)             \\end{align}             $$                            Bellman expectation equation from                      David Silver's lecture                 Two value functions can be combined to become recursive.                              State value                               Q value                                                                                                                 $$             \\begin{align}                 v (s)                 = \\sum_{a \\in A} \\pi (a \\vert s)                 \\left(                 \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r ( s , a , s' ) + \\gamma v (s') \\right)                 \\right)             \\end{align}             $$                               $$             \\begin{align}                 q (s, a)                 = \\sum_{s' \\in S} p (s' \\vert s , a)                 \\left(                 r ( s , a , s' ) + \\gamma \\sum_{a' \\in A} \\pi (a' \\vert s') q (s', a')                 \\right)             \\end{align}             $$                            Bellman expectation equation 2 from                      David Silver's lecture                 The Bellman optimality equation is another Bellman equation, which is basically the Bellman expectation equation with the greedy policy.   \\[\\begin{equation}     \\pi^{\\ast} ( a \\vert s )     = \\begin{cases}       1 &amp; \\arg \\max_{a} q ( s , a ) \\\\       0 &amp; \\text{otherwise}     \\end{cases} \\end{equation}\\]  The greedy policy is a type of deterministic policies, driven by only the maximum \\(q ( s , a )\\). Then, the two value functions can be formulated as:   \\[\\begin{equation}     v^{\\ast} (s)     = \\sum_{a \\in A} \\pi^{\\ast} (a \\vert s) q^{\\ast} (s, a)     = \\max_{a} q^{\\ast} (s, a)     \\\\     q^{\\ast} (s, a)     = \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r ( s , a , s' ) + \\gamma v^{\\ast} (s') \\right) \\end{equation}\\]  Furthermore, recursively:   \\[\\begin{equation}     v^{\\ast} (s)     = \\max_a     \\left(     \\sum_{s' \\in S} p (s' \\vert s , a) ( r ( s , a , s' ) + \\gamma v^{\\ast} (s'))     \\right)     \\\\     q^{\\ast} (s, a)     = \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r ( s , a , s' ) + \\gamma \\max_{a'} q^{\\ast} (s', a') \\right) \\end{equation}\\]  Often notations vary:     \\(d^{\\pi}\\): The state stationary distribution following the policy, for instance, \\(d^{\\pi} ( s ) \\neq d^{\\pi'} ( s )\\).   \\(v_{\\pi}\\): The state value following the policy, for instance, \\(v_{\\pi} ( s ) \\neq v_{\\pi'} ( s )\\).   \\(R ( s_{t} , a_{t} )\\): The return at \\(s_{t}\\) and \\(a_{t}\\), for instance, \\(R_{t} = R ( s_{t} , a_{t} )\\).   \\(\\mathbb{E}_{ \\tau \\sim \\pi }\\): The expectation such that the trajectories are sampled following the policy, for instance, \\(v_{\\pi} (s) = \\mathbb{E}_{ \\tau \\sim \\pi } [ R_{t} \\vert S = s ]\\).   \\(\\mathbb{E}_{ s \\sim d^{\\pi} , a \\sim \\pi }\\): The expectation such that the state is sampled from the state stationary distribution and the action is from the policy, for instance, \\(\\mathbb{E}_{ \\tau \\sim \\pi } [ R_{t} \\vert S = s ] = \\mathbb{E}_{ s \\sim d^{\\pi} , a \\sim \\pi } [ R_{t} \\vert S = s ]\\).   The mathematical expressions of the Bellman expectation and optimality equations might be different from other articles or lectures. This is because a reward function takes different inputs, i.e. \\(r ( s , a )\\) or \\(r ( s )\\), but there is no practical difference.   The exact same notations are applied to the Q value.   Extensions to Markov Decision Process   An MDP is a fully observable fixed environment with only a single player. There are a number of extended MDPs commonly used in reinforcement learning.   Partial Observability   A partially observable Markov decision process (POMDP) is an MDP with partial observability. Instead of observing states directly, the agent receives an observation, infers a state and executes an action. A POMDP can be seen as an HMM version of an MDP, a tuple of \\(( \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{O}, \\mathcal{P}_{o}, \\mathcal{R}, \\gamma )\\), where \\(\\mathcal{O}\\) is the observation space and \\(\\mathcal{P}_{o}\\) is the conditional observation probability. \\(\\mathcal{P}_{o}\\) also carries the Markov property.   \\[\\begin{align}     \\Pr ( O_{t} = o_{t} \\vert S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} )     \\equiv &amp; p_{o} ( O_{t} = o_{t} \\vert S_{t} = s_{t} , A_{t-1} = a_{t-1} )     \\\\     = &amp; p_{o} ( o_{t} \\vert s_{t} , a_{t} ) \\end{align}\\]  The conditional observation probability can be written as \\(\\mathcal{P}_{o} : \\mathcal{S} \\rightarrow \\mathcal{O}\\) or \\(\\mathcal{P}_{o} : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{O}\\), but others share the same with MDP.   In the classic POMDP, a belief, \\(b (s)\\), is used to infer the state, which is a probability distribution over states. Then, the state can be inferred from:   \\[\\begin{equation}     b'( s' ) = \\eta \\cdot p_{o} ( o' \\vert s' , a ) \\sum_{ s \\in \\mathcal{S} } p ( s' \\vert s , a ) b ( s ) \\end{equation}\\]  Where \\(\\eta = \\frac{1}{ \\Pr ( o \\vert b , a ) }\\) is a normalizing constant with:   \\[\\begin{equation}     \\Pr ( o \\vert b , a ) = \\sum_{ s' \\in \\mathcal{S} } p_{o} ( o' \\vert s' , a ) \\sum_{ s \\in \\mathcal{S} } p ( s' \\vert s , a ) b ( s ) \\end{equation}\\]     \\(\\sum_{ s \\in \\mathcal{S} } p ( s' \\vert s , a ) b ( s )\\): For an action that we executed, we multiply the state probability in the belief to the state transition probability for the next state, then we sum all across every state. This will give us the probability that we will reach at \\(s'\\) for a given \\(a\\), or \\(\\Pr ( s' \\vert a )\\).   \\(p_{o} ( o' \\vert s' , a ) \\Pr ( s' \\vert a )\\): For an acquired observation \\(o'\\), we can multiply the conditional observation probability of \\(o'\\) for \\(s'\\) and \\(a\\) pair to \\(\\Pr ( s' \\vert a )\\) to get the unnormalized probability that we are in \\(s'\\) for a given \\(o'\\) and \\(a\\). This is the unnormalized belief.   \\(b( s )\\): A probability distribution over states that determines where the agent would be at the current time step.   However, in deep reinforcement learning, instead, we often use a neural network as a function that maps from the observation space to the state space, \\(f : \\mathcal{O} \\rightarrow \\mathcal{S}\\).   Other Environments   A more general setting to a POMDP is a decentralized POMDP (Dec-POMDP), where multiple agents are involved in decision making. It considers uncertainty in outcomes, sensors and communications. A Dec-POMDP is a 7-tuple \\(( \\mathcal{S}, \\{ \\mathcal{A}_{i} \\}, \\mathcal{P}, \\{ \\mathcal{O}_{i} \\}, \\mathcal{P}_{o}, \\mathcal{R}, \\gamma )\\), where there are \\(i\\) number of agents, so \\(i\\) number of acquirable observations and executable actions. Hence, a POMDP is a special case of a Dec-POMDP with a single agent.   Further generalization on Dec-POMDP is a partially observable stochastic game (POSG). The agents in a POSG hold different rewards, \\(( \\mathcal{S}, \\{ \\mathcal{A}_{i} \\}, \\mathcal{P}, \\{ \\mathcal{O}_{i} \\}, \\mathcal{P}_{o}, \\{ \\mathcal{R}_{i} \\}, \\gamma )\\), so they either compete or cooperate in pursuing their assigned rewards. The terminology is from a game theory, where a stochastic game, or Markov game, refers to a repeated game with probabilistic transitions played by one or more players while partially observable is equivalent to adding observations and emission probabilities to the stochastic game.   An MDP can also hold non-stationarity, referred to as a non-stationary MDP (NSMDP). It is simply a varying MDP over time or epoch, formally \\(\\mathcal{M}^{(i)} = ( \\mathcal{S}^{(i)}, \\mathcal{A}^{(i)}, \\mathcal{P}^{(i)}, \\mathcal{R}^{(i)}, \\gamma^{(i)} )\\), where there is \\(i \\in \\mathbb{R}\\). A special case of an NSMDP is used for multi-task reinforcement learning, meta reinforcement learning, and continual reinforcement learning, i.e. \\(\\mathcal{M}^{(i)} =  ( \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}^{(i)}, \\gamma )\\) if the environment does not change. Note that this can also be applied to a POMDP, \\(\\mathcal{M}^{(i)} = ( \\mathcal{S}^{(i)}, \\mathcal{A}^{(i)}, \\mathcal{P}^{(i)}, \\mathcal{O}^{(i)}, \\mathcal{P}_{o}^{(i)}, \\mathcal{R}^{(i)}, \\gamma^{(i)} )\\).   If you are interested in further generalizations in an MDP, please read this blog.     Environments   Environments can be modelled with few properties.      Environment can be:            Stochastic: Multiple states can be reached via an action for a given state, or there exist two or more states that have \\(p ( s' \\vert s , a ) \\in (0, 1)\\).       Deterministic: Only a single state can be reached via an action for a given state, or there exists a single state that has \\(p ( s' \\vert s , a ) = 1\\).           A state, action and time can be:            Discrete: State space, action space and time are countable, or there exists \\(\\mathcal{S} = \\{ s_{i} \\}_{i = 0}^{N}\\), \\(\\mathcal{A} = \\{ a_{i} \\}_{i = 0}^{M}\\), and \\(\\mathcal{T} = \\{ t_{i} \\}_{i = 0}^{K}\\), where \\(\\sum_{s \\in \\mathcal{S}} \\sum_{a \\in \\mathcal{A}} p (s' \\vert s , a) = 1\\) and \\(\\sum_{a \\in \\mathcal{A}} \\pi (s \\vert a) = 1\\).       Continuous: State space, action space and time are measurable, or there exists \\(\\mathcal{S} = \\mathbb{R}^{n}\\), \\(\\mathcal{A} = \\mathbb{R}^{m}\\), and \\(\\mathcal{T} = \\mathbb{R}\\), where \\(\\int_{S} \\int_{A} p (s' \\vert s , a) \\text{ d}a \\text{ d}s = 1\\) and \\(\\int_{A} \\pi (s \\vert a) \\text{ d}a = 1\\).           Note that discrete state space is interpreted as there exists \\(N\\) number of states while in continuous, an infinitely many (uncountable) number of states exist in \\(n\\) space.   Fully Observable Environments   There are two typical fully observable environments.      Gridworld: Fully Observable Discrete Deterministic MDP            The agent ought to navigate (action) for a given location of the player (state) to solve the game (reward).                              Gridworld from                      MathWorks                    Game Go in the two-player board game: Fully Observable Discrete Stochastic MDP            The agent ought to place a stone (action) for a given location of all the stones (state) to win the game (reward).       Many multi-player board games, including game Go and Chess, can be seen as either deterministic with multi-agent or stochastic with single-agent settings. For instance, the opponents can be seen as environment dynamics that induce stochasticity in the environment or other agents acting on the deterministic environment.       Many multi-player card games are often considered as discrete stochastic MDP with partial observability, a.k.a. a POMDP, or with no observation until the game finishes, a.k.a. an episodic MDP with a return at the end of the game.                              Game Go from                      Wikimedia Commons                 Partially Observable Environments  If the agent takes the state as an input, the environment becomes fully observable.      Zork I in the text-based game: Discrete Stochastic POMDP            The agent ought to execute a textual command (action) for a given textual description of the world (observation) generated from the world graph (state) to solve puzzles in the game (reward).       Many text-based games are either deterministic or mild stochastic as they are designed to solve a set of specific tasks with limited causal relationships between entities. It is practically impossible to create infinitely many tasks with infinitely many causal relationships between entities.                              Zork I from                      Old Games                    Space Invader in the arcade learning environment: Continuous state and Discrete action Stochastic POMDP            The agent ought to navigate or attack (action) for a given image of the world (observation) generated from the location of the player, enemies and items (state) to defeat the enemies (reward).       Similar to the game Go, every enemy can be seen as another fixed agent, but the environment is still stochastic since where the enemies and items appear is probabilistic.                              Space invader from                      Wikipedia                    Robot Locomotion in the Open AI Gym: Continuous Deterministic POMDP.            The agent ought to move joints (action) for a given image of the world (observation) generated from the coordinates of the joints (state) to locomote as intended (reward).       If realistic factors are accounted, i.e. winds and terrains, it becomes stochastic.       Similar to the space invader, it can be a multi-agent setting if two or more robots are used to interact.                              Robot locomotion from                      Wikimedia Commons                 A time is discretized so that the agent can make a decision per time step. Note that by changing some factors, the environment can hold any property, i.e. gridworld with multi-agent setting or robot locomotion with a discrete action space \\(\\mathcal{A} = \\{ 5 \\text{m/s}, 10 \\text{m/s} \\}\\). However, regardless of these properties, the policy and return are applied, so in practice:     All the environments are formalized as a stochastic setting.   In discrete MDP, the state is embedded using a high dimensional vector.   In POMDP, the agent infers a high dimensional state, i.e. embedding or vector space, from observation using an encoder.   Summary   In this post, Markov models with a controllable system in a discrete-time setting are covered.   An MDP is a tuple of \\(( \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma )\\), where a controllable system, or an agent, ought to 1) execute actions, \\(a \\in \\mathcal{A}\\); 2) following the policy, \\(\\pi \\in \\Pi\\); 3) to navigate to the states, \\(s \\in \\mathcal{S}\\); 4) through state transition probabilities, \\(p \\in \\mathcal{P}\\); 5) that hold positive rewards, \\(r \\in \\mathcal{R}\\).   A policy, state transition probability and reward in an MDP follow the Markov property.   \\[\\begin{equation}     \\Pr ( A_{t} = a_{t} \\vert S_{0:t} = s_{0:t} )     \\equiv \\pi ( A_{t} = a_{t} \\vert S_{t} = s_{t} )     = \\pi ( a_{t} \\vert s_{t} , a_{t-1} )     \\\\     \\Pr ( S_{t+1} = s_{t+1} \\vert S_{0:t} = s_{0:t} , A_{0:t} = a_{0:t} )     \\equiv p ( S_{t+1} = s_{t+1} \\vert S_{t} = s_{t} , A_{t} = a_{t} )     = p ( s_{t+1} \\vert s_{t} , a_{t} )     \\\\     r ( S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} )     \\equiv r ( S_{t} = s_{t} , A_{t-1} = a_{t-1} , S_{t-1} = s_{t-1} )     = r ( s_{t} , a_{t-1} , s_{t-1} ) \\end{equation}\\]  A collection of states and actions that the agent travelled for a given maximum time, \\(T\\), is referred to as a trajectory, \\(\\{ S_{0} = s_{0} , A_{1} = a_{1} , \\cdots  A_{T-1} = a_{T-1} , S_{T} = s_{T} \\}\\).     If \\(T \\rightarrow \\infty\\), an MDP is referred to as a continuous average-reward MDP, where the task may continue forever.   If \\(T &lt; \\infty\\), an MDP is referred to as an episodic start-state MDP, where the task has a clear ending.   An MDP is all about the policy. A policy can be:     Deterministic: The choice of action is non-probabilistic, so the next action is produced for a given state, or \\(a = \\pi ( a \\vert s )\\). Ideal for the optimal world, where environment dynamics are fully observed.   Stochastic: The choice of action is probabilistic, so the next action is sampled from policy distribution, or \\(a \\sim \\pi ( a \\vert s ) \\in \\mathbb{R}^{m}\\), where \\(\\sum_{a \\in \\mathcal{A}} \\pi ( a \\vert s ) = 1\\). Ideal for the real world, where environment dynamics are not fully observed.   An MDP also has the concept of ergodicity, but the definition varies by literature.     There exists a policy \\(\\pi\\) for a unique stationary distribution, \\(d ( s )\\), such that \\(d ( s ) &gt; 0\\) from Moldovan.   For every policy \\(\\pi\\), a unique stationary distribution exists, \\(d ( s )\\), such that \\(d ( s ) &gt; 0\\) from Puterman.   For every policy \\(\\pi\\), a unique stationary distribution exists, \\(d ( s )\\), such that \\(d ( s ) \\geq 0\\) from Sutton.   Where:            A state stationary distribution, stationary distribution or steady-state distribution, \\(d ( s )\\):       \\[\\begin{align}       d ( s )       = &amp; \\lim_{t \\rightarrow \\infty} \\Pr (S_{t} = s \\vert s_{0} , \\pi)       \\equiv \\sum_{k=0}^{\\infty} \\Pr (s_{0} \\rightarrow s \\vert k , \\pi)       \\\\       = &amp; \\sum_{s' \\in \\mathcal{S}} d ( s' )       \\sum_{a' \\in \\mathcal{A}} \\pi ( a' \\vert s' ) p ( s \\vert s' , a' )   \\end{align}\\]             A state visitation probability, \\(\\Pr (s_{0} \\rightarrow s \\vert k , \\pi )\\):       \\[\\begin{align}       \\Pr (s \\rightarrow s' \\vert 1 , \\pi )       = \\sum_{a \\in \\mathcal{A}} \\pi ( a \\vert s ) p ( s' \\vert s , a )       \\qquad       \\Pr (s \\rightarrow s'' \\vert k , \\pi )       = \\sum_{s' \\in \\mathcal{S}} \\Pr (s \\rightarrow s' \\vert n, \\pi ) \\cdot \\Pr (s' \\rightarrow s'' \\vert k - n , \\pi )   \\end{align}\\]      A policy is all about the rewards.          The optimal policy is the policy that acquires the highest total cumulative rewards.   \\[\\begin{equation}      \\pi^{\\ast} ( a \\vert s ) = \\arg \\max_{\\pi} \\mathbb{E} [ \\sum_{t = 0}^{\\infty} r ( s_{t} ) ]  \\end{equation}\\]           The optimal policy is the policy that acquires the highest total cumulative rewards in the shortest travel distance. A return is to comprehend both the total cumulative rewards and the travel length into the optimal policy.   \\[\\begin{equation}      \\pi^{\\ast} ( a \\vert s )      = \\arg \\max_{\\pi} \\mathbb{E} [ R_{t} ]      = \\arg \\max_{\\pi} \\mathbb{E} [ \\sum_{k = 0}^{\\infty} \\gamma^{k} r ( s_{t + k + 1} ) ]  \\end{equation}\\]             If \\(\\gamma = 0\\), the agent only cares about rewards in a single step, or short-sighted, which may result in the agent not learning anything if the rewards are sparse.       If \\(\\gamma = 1\\), the agent cares about rewards of the infinite horizon, or long-sighted, which may fall into an infinite loop or care about high but far-to-reach rewards.                To make the mathematical expression of the optimization more flexible for reinforcement learning algorithms, we use the expectation of a return, or a value function. There are two types of value functions, a state value, \\(v ( s ) = \\mathbb{E} [ R_{t} \\vert S = s ]\\), and Q value, \\(q ( s , a ) = \\mathbb{E} [ R_{t} \\vert S = s , A = a ]\\). \\(v\\) quantifies how good a particular state is based on the expected return from the state following the policy while \\(q\\) deals with a state and action pair.   \\[\\begin{align}      \\pi^{\\ast} ( a \\vert s )      \\equiv &amp; \\arg \\max_{\\pi} \\mathbb{E} [ v ( s ) ]      = \\arg \\max_{\\pi} \\sum_{s \\in \\mathcal{S}} d ( s ) v ( s )      \\\\      \\equiv &amp; \\arg \\max_{\\pi} \\mathbb{E} [ q ( s , a ) ]      = \\arg \\max_{\\pi} \\sum_{s \\in \\mathcal{S}} d ( s ) \\sum_{a \\in \\mathcal{A}} \\pi ( a \\vert s ) q ( s , a )  \\end{align}\\]             The Bellman expectation equation decomposes the value function recursively into a reward and discounted future value function.       \\[\\begin{align}      v (s)      = &amp; \\sum_{a \\in A} \\pi (a \\vert s) q (s, a)      = \\sum_{a \\in A} \\pi (a \\vert s)      \\left(      \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r ( s , a , s' ) + \\gamma v (s') \\right)      \\right)      \\\\      q (s, a)      = &amp; \\sum_{s' \\in S} p (s' \\vert s , a)      \\left(      r ( s , a , s' ) + \\gamma v (s')      \\right)      = \\sum_{s' \\in S} p (s' \\vert s , a)      \\left(      r ( s , a , s' ) + \\gamma \\sum_{a' \\in A} \\pi (a' \\vert s') q (s', a')      \\right)  \\end{align}\\]             The Bellman optimality equation is the Bellman expectation equation with the greedy policy.       \\[\\begin{align}      v^{\\ast} (s)      = &amp; \\sum_{a \\in A} \\pi^{\\ast} (a \\vert s) q^{\\ast} (s, a)      = \\max_{a} q^{\\ast} (s, a)      = \\max_a      \\left(      \\sum_{s' \\in S} p (s' \\vert s , a) ( r ( s , a , s' ) + \\gamma v^{\\ast} (s'))      \\right)      \\\\      q^{\\ast} (s, a)      = &amp; \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r ( s , a , s' ) + \\gamma v^{\\ast} (s') \\right)      = \\sum_{s' \\in S} p (s' \\vert s , a) \\left( r ( s , a , s' ) + \\gamma \\max_{a'} q^{\\ast} (s', a') \\right)  \\end{align}\\]      Some MDP variants are:          Partially observable Markov decision process (POMDP): A tuple of \\(( \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{O}, \\mathcal{P}_{o}, \\mathcal{R}, \\gamma )\\)   \\[\\begin{align}       \\Pr ( O_{t} = o_{t} \\vert S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} , O_{0:t-1} = o_{0:t-1} )       \\equiv p_{o} ( O_{t} = o_{t} \\vert S_{t} = s_{t} , A_{t-1} = a_{t-1} )       = p_{o} ( o_{t} \\vert s_{t} , a_{t} )   \\end{align}\\]             A belief is used to infer the state.       \\[\\begin{equation}       b'( s' ) = \\eta p_{o} ( o' \\vert s' , a ) \\sum_{ s \\in \\mathcal{S} } p ( s' \\vert s , a ) b ( s )       \\quad       \\text{where, } \\eta = \\frac{1}{ \\Pr ( o \\vert b , a ) }   \\end{equation}\\]  \\[\\begin{equation}       \\Pr ( o \\vert b , a ) = \\sum_{ s' \\in \\mathcal{S} } p_{o} ( o' \\vert s' , a ) \\sum_{ s \\in \\mathcal{S} } p ( s' \\vert s , a ) b ( s )   \\end{equation}\\]      Decentralized POMDP (Dec-POMDP): A tuple of \\(( \\mathcal{S}, \\{ \\mathcal{A}_{i} \\}, \\mathcal{P}, \\{ \\mathcal{O}_{i} \\}, \\mathcal{P}_{o}, \\mathcal{R}, \\gamma )\\)   Partially observable stochastic game (POSG): A tuple of \\(( \\mathcal{S}, \\{ \\mathcal{A}_{i} \\}, \\mathcal{P}, \\{ \\mathcal{O}_{i} \\}, \\mathcal{P}_{o}, \\{ \\mathcal{R}_{i} \\}, \\gamma )\\)   Non-stationary MDP (NSMDP): A tuple of \\(\\mathcal{M}^{(i)} = ( \\mathcal{S}^{(i)}, \\mathcal{A}^{(i)}, \\mathcal{P}^{(i)}, \\mathcal{R}^{(i)}, \\gamma^{(i)} )\\)   Reference      Lecture 1 and 2 of David Silver’s lecture   Chapter 3 in Reinforcement Learning: An Introduction   Other helpful resources are:     MDP Cheatsheet Reference from John Schulman  ","categories": ["Mathematics&#x003a; Markov Model"],
        "tags": [],
        "url": "/posts/mathematics/markov_model_2_markov_decision_process/",
        "teaser": null
      },{
        "title": "Neuron and Neural Tissue",
        "excerpt":"     Perception is reality.           Lee Atwater   Perception is not true reality, but at least, it is our reality. We have no mean to apprehend the true reality, but only to merely estimate the reality from the observation in a limited and distorted way using our brain.   Neuroscience is the study of the human brain and nervous system, which includes 1) sensing the world, 2) feeling the emotion, 3) forming the memory, 4) processing the logic, and 5) moving the muscle. It is said that if the body is the hardware, the nervous system is the software. Neuroscience combines knowledge from various disciplines to explore the complexity of the human brain and nervous system.   In the field of neuroscience, there are specific biological components that play unique roles. Neurons are specialized cells dedicated to transmitting and processing information. They form neural tissues, which operate collectively to manage information. These tissues assemble to manage information in clusters and contribute to the formation of the spinal cord and brain, which serve as specialized organs in the nervous system.                  Cell                       Neuron,             Neuroglia                             Tissue                       Nerve,             Ganglion,             White Matter,             Gray Matter                             Organ                       Peripheral Nerve,             Brain,             Spinal Cord                             Organ System                       Peripheral Nervous System,             Central Nervous System,                           Somatic Nervous System,             Autonomic Nervous System                 A neuron acts as an electrical device that sends a signal by inlet and outlet of charged ions through ion channels on the membrane. Together with neuroglia, they make up neural tissues that transfer information or process information. This post aims to provide a tutorial on the two lower levels of body organization in neuroscience, namely the cell level and the tissue level.   Neuron   A neuron, or nerve cell or neuronal cell or neurocyte, is the fundamental unit of the nervous system, responsible for sending and receiving electrochemical signals to interact with the world. It is the smallest component that is responsible for everything that involves perception, cognition and consciousness.                      A neuron from                      Wikimedia Commons                 A neuron is structurally unique.     Dendrite is a tree branch-like shape filament that receives input signals from other neurons. Usually, it outreaches from the cell body. which contains nucleus in its centre.   Axon, or nerve fibre is a long tube-like shape filament that passes the received signals from dendrites to the other end. It is one of the commonly damanged parts, since it is approximately 50 times thinner than hair.   Myelin sheath, or myelination, is a lipid-rich material of neuroglia, such as Schewan cell, that wraps the axon to protect it from physical trauma. Node of Ranvier is the gap between myelin sheaths. Myelin sheath is not part of neuron and is not found in every neuron.   Axon terminal is a splitted axon at the other end that sends the passed signals to other neurons.   A neuron is also functionally unique.     Neurotransmission: The chemical signal between axon terminal of one neuron and dendrite of another neuron.   Action potential propagation: The electrical signal through axon from dendrite to axon termianl within the neuron.   In this section, I will briefly introduce the types of neurons, then, focus on the two core functions of neurons, neurotransmission and action potential propagation.   Neuron Type  There are many different ways to differentiate neurons. Morphologically, four types of neuron exist.     Unipolar: One axon extending from the cell body to dendrites.   Bipolar: Two axons extending from the cell body to dendrites and axon terminals separately.   Multipolar: Dendrites on the cell body and one axon extending from the cell body to axon terminals.   Pseudounipolar: One axon extending from the cell body splited into two branches in different lengths, one for dendrites and the other for axon terminals.                      A neuron type from                      Wikimedia Commons                 Unipolar neuron is only present in invertebrates, such as insects, so humans only have neurons of bipolar, multipolar and pseudopolar. Multipolar neuron has two variants in human.     Pyramidal: Multipolar with dendrites in pyramidal shape for excitatory neurotransmitter.   Purkinje: Multipolar with dendrites in planar fan-shaped for inhibitory neurotransmitter.   The morphology of neurons usually follows its functionality. There are three functional classes of neuron.     Sensory neuron: Passes the sensation signal from the external world to the brain. Most are bipolar or pseudounipolar.   Interneuron: Connects sensory neurons, interneurons and/or motor neurons, forming circuits of various complexity. Most are multipolar, pyramidal, or Purkinje.   Motor neuron: Passes the innervation signal from the brain to muscles and glands. Most are also multipolar.   The functional types of neurons vary drastically by the location.     Neurons found in the peripheral nervous system (PNS) are generally sensory neurons or motor neurons.            Sensory neuron branches out towards the sensory organs (long) and the CNS (short) separately.           Neurons found in the central nervous system (CNS) are generally interneurons.            There are extensive number of neuron types in the brain, distinguished by neurotransmitter type, structure, synapse location, electrical properties, gene expressions, projection patterns and many more. Due to this, yet community has not come to an agreement on how to distinguish them.           Thus, the process can be thought of as:     Receive the sensation signal through sensory neurons in the PNS.   Process the information through interneurons in the CNS.   Convey the innervation signal through motor neurons in the PNS.   Of course, there are neurons in the brain, involved in sensation, i.e. visual cortex or auditory cortex, or in movement, i.e. motor cortex, but they are composed of interneurons. Sensory neurons and motor neurons are usually present in the PNS to deliver signals between the body and the CNS. Most of the processing is done with interneurons in the CNS.   Neurotransmission   A neurotransmission is the chemical process that a neuron communicates with another neuron. The process involves two neurons, releasing and receiving signalling molecules, called neurotransmitters, at the junction between two neurons, called synapse. The two neurons are named based on whether to release or receive neurotransmitters, or the direction of the signal.     Presynaptic neuron: The neuron that sends the neurotransmitters from the axon terminal.   Postsynaptic neuron: The neuron that receives the neurotransmitters at the dendrite.   A neurotransmitter has two opposite roles, either excite (initiate) or inhibit (hinder) the activation of an action potential propagation in the postsynaptic neuron.     Excitatory neurotransmitter: Glutamate, noradrenaline and histamine.   Inhibitory neurotransmitter: Gamma-aminobutyric acid (GABA), serotonin and glycine.   Excitatory and inhibitory neurotransmitter: Acetylcholine and dopamine.            Acetylcholine is excitatory to contract skeletal muscles while inhibitory to slow the heart rate.           Glutamate and GABA predominate the neurotransmitters in the brain, 80\\% ~ 90\\% and 30\\% ~ 40\\% synapses, respectively. Note that a neuron may have more than one neurotransmitter types, so most of neurons with glutamate also carry other neurotransmitters in the brain.     Glutamate helps learning and memory whereas GABA produces a calming effect.   Glutamate is the metabolic precursor of GABA, where glutamate is decarboxylated to GABA and CO2. Which means GABA is produced from glutamate.   Since GABA is for calming effects, some people take GABA for anxiety relief or ADHD.   Excessive glutamate is known to cause overexcitement of neurons and may result in the death of neurons, or neuroapoptosis. This phenomenon is called excitotoxicity.   It is very important to balance excitatory and inhibitory neurotransmitters. In a cell level, it determines whether an action potential will result or not, and in an organ level, excess or lack of one may result in neurological or mental diseases.   The full process of a neurotransmission is:                      A neurotransmission from                      Wikimedia Commons                    Action potential travels through presynaptic neuron and reaches the axon terminal of the postsynaptic neuron.   Action potential at the axon terminal opens voltage-gated Ca2+ channels, allowing Ca2+ influx to the axon terminal.   Neurotransmitters are released from the axon terminals of the presynaptic neuron into the synaptic cleft.   Neurotransmitter binds to neurotransmitter receptors on the dendrites of the postsynaptic neuron.   This binding opens ion channels, positive (e.g. Na+, K+, Ca2+) or negative (e.g. Cl-) depending on the neurotransmitter type, allowing ion influx to the membrane of postsynaptic neuron.   Neurotransmitters attached to the dentrites of the postsynaptic neuron are detached and collected through neurotransmitter transporters.   Few details are,     (3) Synaptic cleft is a 20nm ~ 40nm gap between the presynaptic axon terminal and the postsynaptic dendrite. Synapse includes all the presynaptic axon terminal, the synaptic cleft and the postsynaptic dendrite.   (4, 5) Neurotransmitters are not the ones that travel into postsynaptic neurons, but ions are.   (8) Neurotransmitter transporters are responsible for maintaining appropriate levels of neurotransmitters inside and outside neurons.   A neurotransmission occurs between other cells in muscles and glands. In the case of muscle, it is called neuromuscular transmission, covered in the later section.   Action Potential Propagation   An action potential propagation is the electrical process that a signal is propagated within a single neuron from dendrites to axon terminals. The process involves the changes of electric potentials inside the membrane, called action potential, or nerve impulse or spike.   The full process of an action potential propagation is:                      An action potential propagation from                      Wikimedia Commons                    Membrane potential stays at the resting membrane potential. There is lower concentration of Na+ (a) and higher concentration of K+ inside the axon.   During neurotransmission, if more excitatory neurotransmitters than inhibitory neurotransmitters bind to the dendrites, the positive ions enter the membrane of the neuron, increasing the membrane potential.   If the membrane potential rises to the action potential threshold, the action potential propagation initiates. In other words, neuron fires.   An increased membrane potential activates voltage-gated Na+ channels (c) to inlet Na+ into the axon, increasing membrane potentials further, called depolarization.   Na+ influx activates voltage-gated K+ channels (d) to outlet K+ out of the axon, decreasing membrane potentials, called repolarization.   The outlet of K+ is more excessive than the inlet of Na+, causing undershoot of the membran potential to drop below the resting membrane potential, called hyperpolarization.   The resting potential is ultimately re-established by closing of all voltage-gated ion channels and the activitation of the Na+/K+ pump (e). This is called refractory period.   (4,5,6,7) occur consecutively through the axon until the axon terminal.   Few details are,     (1, 3) Membrane potential is the difference in electrical potential between inside and outside of a cell. The resting membrane potential and action potential threshold are usually -70 mV and -55 mV, but they vary drastically by neuron types.   (2) During neurotransmission, if more inhibitory neurotransmitters than excitatory neurotransmitters bind to the dendrites, the membrane potential decreases, and thus no action potential.   (3) If the membrane potential does not rise to the action potential threshold, no action potential propagation is initiated.   (6) Hyperpolarization is known to prevent action potential to travel backwards.   (4,5,6) are the process of action potential propagation.   Through action potential propagation, a neuron consecutively opens and closes electrically activated ion channels to pass electrical impulse from dendrites to axon terminals. Then, at the end of axon terminals, a neurotransmission occurs with a postsynaptic neuron. In the postsynaptic neuron, the sum total of dendritic inputs, excitatory as positive charges and inhibitory as negative charges, determines whether the neuron will fire an action potential.   Myelin sheath is a lipid-rich fatty substance of neuroglia that encases axons. It is very important in action potential propagation as it increases the rate of action potential propagation. This phenomenon is referred to as saltatory conduction.   The exchanges of ions can only occur at unmyelinated part of axon, so the increase in the conduction velocity of action potential propagation is because the action potential is skipped at myelinated areas. Due to this myelin sheaths are often seen as an insulator for action potential propagation.                      Saltatory conduction from                      Wikimedia Commons                 Nerve Fibre  A nerve fibre is another name for an axon that is commonly used to classify a neuron serving a particular function. This can be considered as a subdivision of sensory neurons and motor neurons. Here, the naming convention is based on the direction of the information flow, that is sensory neurons as afferent neurons and motor neurons as efferent neurons.   There are three subdivisions that exist in nerve fibre.     Location: A nerve fibre can be general or special. The former refers to a nerve fibre branching out from the spinal cord in the CNS and the latter from the brain in the CNS.   Function: A nerve fibre can be somatic or autonomic. Somatic refers to as voluntary and autonomic as involuntary. Visceral is another name for autonomic. Therefore, somatic nerve fibre is for voluntary system while visceral nerve fibre is for involuntary system.   Direction: A nerve fibre can be afferent or efferent. Afferent is to bring sensations to CNS and efferent is to send commands from CNS. Afferent is derived from the Latin afferre that means to bring towards and efferent is from the Latin efferre, to bring away from.   A nerve fibre is named with a combination of these three subdivisions, such as general somatic afferent nerve fibre or special visceral efferent nerve fibre. This naming convention is used to describe neural tissues while morphological and functional classifications are used more on neurons.   Herein, I will mainly focus on the general function and idea on afferent nerve fibre and efferent nerve fibre. Note that although there exist neurons that send and receive signals in the CNS, this content is mainly on the PNS.   Afferent Nerve Fibre   An afferent nerve fibre is an axon of afferent neuron, or sensory neuron, that brings the sensation signals from the body to the CNS. There are four types of afferent nerve fibres.     General somatic afferent (GSA) nerve fibre: Carry pain, touch or temperature from the skins, muscles, tendons or joints to the CNS through the spinal nerve.   General visceral afferent (GVA) nerve fibre: Carry pain or physiological sensation from the internal organs, glands or blood vessels to the CNS through the spinal nerve.   Special somatic afferent (SSA) nerve fibre: Carry vision, hearing or balance from the eyes, ears or body to the CNS through cranial nerve.   Special visceral afferent (SVA) nerve fibre: Carry smell or taste from the nose or tongue to the CNS through cranial nerve.   An sensory neuron is a neuron that is activated by sensory input from the environment or within the body. There are largely two sensations in the human body.     Exteroception: Sensation from outside the body.   Interoception: Sensation from inside the body.   The action potential propagation of an sensory neuron is initiated by four types of sensations of different stimuli.     Chemosensation: Induced by chemoreceptor that detects chemical molecule.   Mechanosensation: Induced by mechanoreceptor that detects mechanical force.   Thermalsensation: Induced by thermoreceptor that detects thermal excitation.   Photosensation: Induced by photoreceptor that detects photoexcitation.   The stimuli open ion channels and depolarize membrane potentials to initiate action potential propagation. Exteroception and interoception contain the sensations of different stimuli.     Exteroception            Chemosensation for smell and taste.       Mechanosensation for touch and hearing.       Thermalsensation for heat and chill.       Photosensation for sight.       Chemosensation, mechanosensation and thermalsensation for pain.           Interoception            Chemosensation for blood sugar level.       Mechanosensation for blood pressure and body position.           Exteroception covers the majority of conscious sensations we use to perceive the world. Interoception, on the other hand, covers both conscious and non-conscious sensations that are important to physiology. Due to this, major research on sensations is driven by exteroception, and only recently, researchers started focusing on interoception, such as the intersection with immune system or gastrointestinal system. Thus, interoception is still quite gray area.     Some literature puts the sense of body position as different sensation from exteroception and interoception, but here, I included in interoception.   Efferent Nerve Fibre   An efferent nerve fibre is an axon of efferent neuron, or motor neuron, that sends the innervation signals from the CNS to the body. There are three types of efferent nerve fibres.     General somatic efferent (GSE) nerve fibre: Carry motor innervation from the spinal cord to skeletal muscles through the spinal nerve.   General visceral efferent (GVE) nerve fibre: Carry motor innervation from the spinal cord to smooth muscles, cardiac muscles or glands through the spinal nerve.   Special visceral efferent (SVE) nerve fibre: Carry motor innervation from the brain to the muscles of the pharyngeal arches, which include head, neck and tongue through the cranial nerve.   There is no special somatic efferent nerve fibre, instead, SVE nerve fibre takes care of both somatic and visceral functions. Due to this, the neuron of SVE nerve fibre is often referred to as branchial motor neuron.   An motor neuron is a neuron that is activated by innervation output from the CNS. It can be divided based on the location or function.     Location: An upper motor neuron is between the brain and the spinal cord while a lower motor neuron is between the spinal cord to muscles or glands.   Function: A somatic motor neuron is to control skeletal muscle while a visceral motor neuron is for smooth muscle, cardiac muscle and glands.   Upper motor neurons are of the CNS, so GSE, GVE and SVE nerve fibres are part of lower motor neurons.   Muscle contraction is the main function of GSE fibres. They activate skeletal muscles through a process called neuromuscular transmission.                      A neuromuscular junction from                      Wikimedia Commons                    Action potential travels through motor neuron (A) and reaches the axon terminal of the motor neuron (B).   Depolarization at the axon terminal opens voltage-gated Ca2+ channels, allowing Ca2+ influx to the axon terminal.   Neurotransmitters, mostly acetylcholine (ACh), are released from the axon terminals of the motor neuron into the neuromuscular junction, or myoneural junction (C). This process is referred to as exocytosis.   ACh binds to post-synaptic receptors on the membrane of the muscle cell, or sarcolemma (D).   This binding opens Na+ channels, allowing Na+ influx to the membrane of muscle cell.   An increase in Na+ generates the action potential propagation in the muscle cell, resulting in muscle contraction via myofibril (E).   Neurotransmitters attached to the membrane of the muscle cell are detached and collected through neurotransmitter transporters.   Few details are,     (3) ACh is excitatory to contracts the skeletal muscles but inhibitory and excitatory for involuntary systems. It contracts smooth muscles (excitation) while slowing the heart rate (inhibition).   (6) Depolarization triggers the release of Ca2+. Then, it binds to troponin to initiate contraction, which is sustained by ATP. As long as Ca2+ remains bound to troponin and ATP is available, the contraction is maintained.   Relaxation is opposite to contraction, where either no more AChs produced from motor neurons or ATPs run out. Although this process is specific to somatic motor neuron stimulating skeletal muscle, the logic can be generally applies to others, such as visceral motor neuron stimulating glands.   Neural Tissue  A neural tissue, or nervous tissue, is a group of cells working together to perform a function in the nervous system. A neuron alone is nothing more than a cell, but a collection of neurons with neuroglia serves a purpose. Again, similar to neurons, neural tissues found in the PNS and the CNS are different.   There are two types of neural tissues in the PNS of the human.     Nerve: An enclosed bundle of axons, or nerve fibers. Action potentials are transmitted as a bundle to muscles, glands or CNS.   Ganglion: A cluster of dendrites, neuronal cell bodies, axon terminals and neuroglia. Neurotransmission occurs between a lot of neurons, exciting or inhibiting the activation of action potential propagation.   A nerve usually contains a mixture of different nerve fibres while a ganglion contains neurons of a particular function.   In the CNS, neural tissues share similar characteristics to nerve and ganglion.     White Matter: Nerve in CNS, but action potentials are transmitted between the various grey matter, and between the gray matter and the rest of the body along with myelination of oligodendrocytes. Myelin sheath is what makes white matter white.   Grey Matter: Ganglion in CNS. Grey matter is actually more pink-coloured than grey because of the abundant blood supply.   White matter is where the information is transferred while grey matter is where the information is processed. Their structures and functions can be somewhat generalized as nerve and ganglion, but in details, they vary drastically by location.   This section mostly focuses on the neural tissues in the PNS. Thus, I will cover basics of neuroglia, then introduce nerve and ganglion.   Neuroglia   Neuroglia, or glia or glial cell, is a cell that provides support for neurons. Their functions vary by a cell type, from metabolic to physiological function. Largely, there are six types of neuroglia.                      A neuroglia type from                      Wikimedia Commons                    Ependymal cell: Surface for neural tissues in the CNS.   Astrocyte: Metabolism in the CNS.   Oligodendrocyte: Myelination in the CNS.   Microglia: Immune system in the CNS.   Satellite cell: Metabolism in the PNS.   Schwann cell: Myelination in the PNS.   Although neuroglia appear to be less important than neurons, they are believed to contribute in learning and memory.   Neurons and neuroglia make up extracellular matrix with a composition of proteins.                      An extracellular matrix in the brain from                      Wikimedia Commons                    Ependymal cell is pink.   Astrocyte is green.   Oligodendrocyte is blue.   Microglia is red.   Extracellular matrix is important to maintain the shape and regulate the processes, which are done by many different proteins. We refer to a group of neurons, glia and extracellular matrix as neural tissue.   There are many proteins that provide structural and biochemical supports to cells, but some unique and important proteins in the extracellular matrix of the neural tissue are:     Neurotrophin, or neurotrophic factor: Regulating survival, development and function of neurons, which belongs to the growth factor family. It is one of the most widely studied proteins, where nerve growth factor (NGF), brain-derived neurotrophic factor (BDNF), neurotrophin-3 (NT-3) and neurotrophin-4 (NT-4) belong to this.   Neuronal apoptosis inhibitory protein (NAIP): Regulating an apoptosis of a neuron, which belongs to the inhibiter of apoptosis (IAP) family.   Hippocalcin: Allowing Ca2+ to bind to interact with other proteins and regulate various processes, which belongs to the neuronal calcium sensor (NCS) family.   Nerve   A nerve is composed of nerve fibres that bring the sensation signals from the body to the CNS (afferent) and that send the innervation signals from the CNS to the body (efferent), and neuroglia that wraps nerve fibres. There are two large classifications.     Spinal nerve: Branching out from the spinal cord, so it contains general nerve fibres, such as GSA, GVA, GSE, and GVE nerve fibres. There are 31 pairs of spinal nerves in the human.   Cranial nerve: Branching out from the brain, so it contains special nerve fibres, such as SSA, SVA, and SVE nerve fibres. There are 12 pairs of cranial nerves in the human.   Note that this classification is little vague, where some describe them as tissue while others describe them as peripheral organ, but the general idea is that any form of a bundle of axons in the PNS is considered as nerve.   A spinal nerve can be anatomically divided as:                      A nerve anatomy from                      Wikimedia Commons                    Nerve fascicle: A bundle of axons filled with endoneurium and enclosed by perineurium. The term fascicle is also used for muscles, for instance, muscle fascicle.   Fasciculus: A bundle of fascicles with blood vessel and epineurium. The spinal nerve is a fasciculus outreaching from the spinal cord.   The anatomy of nerve differ extremely per location, i.e. in cranium or in the CNS, but nerve fascicle and fasciculus are generally present in all of them.     In the PNS, a nerve consists of a combination of nerve fibres of different directions, afferent or efferent, and different functions, somatic or visceral, with or without myelinations of Schwann cells.   While nerves in the PNS are fairly well-known, white matters in the CNS are still grey area. However, at least, we know that most of them are wrapped with myelinations of oligodendrocytes.   Ganglion  A ganglion is composed of dendrites, neuronal cell bodies, axon terminals and neuroglia. There are four large classifications.     Dorsal root ganglion, or spinal ganglion: The cell bodies of GSA and GVA, located next to spinal cord. There is not neurotransmissions inside the dorsal root ganglia. For each spinal nerve, dorsal root ganglion is held next to the spinal cord.   Cranial nerve ganglion: The cell bodies of SSA, SVA and SVE, located near the brain. Some cranial nerve ganglia contain synapses for neurotransmissions and some not. Not every cranial nerve contains cranial nerve ganglion. Some cranial nerves have their cell bodies inside the brain.   Autonomic ganglion: The cell bodies of GVA and GVE, located in near the spinal cord or peripheral organs. Many autonomic ganglia contain synapses for neurotransmissions. Often autonomic ganglions lie near the organs that they are responsible of.   Ventral ganglion: The cell bodies of GSE nerve, located in the ventral horn inside spinal cord. The cell bodies of the GSE are not part of the PNS. They reside inside the spinal cord, so often ventral ganglion refers to ventral nucleus instead.   Some ganglia act as a synaptic relay station between neurons since they contain presynaptic axon terminals and postsynaptic dendrites. The information enters to the ganglia from presynaptic neurons, excites the postsynaptic neurons in the ganglia and then exits through the nerve fibres of postsynaptic neurons. However, this is not the case for every ganglion. For instance, the cell bodies of GVA are located in either dorsal root ganglion or autonomic ganglion. This is why some autonomic functions are viable without the CNS.   Not all of these four ganglia are part of the PNS.     Dorsal root ganglion, cranial nerve ganglion and autonomic ganglion belong to the PNS.   Ventral ganglion belongs to the CNS. A ganglion is often referred to as nucleus in the CNS, i.e. motor nucleus as ventral ganglion.   The main differences between ganglion and grey matter is:     The neuroglia in ganglion are mostly satellite cells.   The neuroglia in grey matter are astrocytes and oligodendrocytes.   Summary  Neuron     Neuron is anatomically divided as dendrite, cell body, axon and axon terminal   Neuron types vary morphologically, unipolar &amp; bipolar &amp; pseudounipolar &amp; multipolar &amp; pyramidal &amp; Purkinje   Neuron types vary functionally, sensory neuron &amp; interneuron &amp; motor neuron   A neurotransmission is a chemical process that transfers neurotransmitters from the presynaptic neuron and the postsynaptic neuron for action potential propagations            The presynaptic neuron releases neurotransmitters and the postsynaptic neuron takes the neurotransmitters       Neurotransmitters are commonly glutamate for excitation and GABA for inhibition       A neurotransmission either initiate or inhibit action potential propagation in the postsynaptic neuron by neurotransmitters opening the ion channels, allowing ion influx           An action potential propagation is a electrical process that propagates action potential from the dendrites to the axon terminals for neurotransmissions            Action potential is a phenomenon that membrane potential rises and falls, from depolarization, repolarization to hyperpolarization       The changes of membrane potential is due to voltage-gated ion channels, which allows the ions to pass when the membrane potential reaches a particular voltage       Saltatory conduction is a phenomenon that an action potential propagation is speed up with myelincation since voltage-gated ion channels only appear on nodes of Ranvier           Nerve Fibre     Nerve fibre types vary by location (general and special), function (somatic and autonomic) and direction (afferent and efferent)   An afferent nerve fibre is part of a sensory neuron            An afferent neuron is a synonym for sensory neuron       Afferent nerve fibre types are general somatic Afferent (GSA), General Visceral Afferent (GVA), Special Somatic Afferent (SSA) and Special Visceral Afferent (SVA)       Sensations are classified based on where the stimuli arise, exteroception and interoception       Sensations are classified based on the types of stimuli, chemosensation, mechanosensation, thermalsensation and photosensation           An efferent Nerve Fibre is part of a motor neuron            An efferent neuron is a synonym for motor neuron       Efferent nerve fibre types are General Somatic Efferent (GSE), General Visceral Efferent (GVE) and Special Visceral Efferent (SVE)       Motor neuron types vary by Location (Upper and Lower) and Function (Somatic and Visceral)       Muscle Contraction is a result of neuromuscular transmission between motor neuron and muscle cell           Neural Tissue     Neural tissue is formed by neurons, neuroglia and extracellular matrix   A bundle of axons form nerve in the PNS or white matter in the CNS   A cluster of dendrites, cell bodies, axon terminals and neuroglia form ganglion in the PNS or gray matter in the CNS   Neuroglia            Neuroglia types are ependymal cell, astrocyte, oligodendrocyte, microglia, satellite cell and Schwann cell       Extracellular matrix is formed by neuron, neuroglia and proteins       Some unique and important proteins found in the neural tissue are neurotrophin, neuronal apoptosis inhibitory protein (NAIP) and hippocalcin           Nerve            Equivalent to white matter in the CNS       Any form of a bundle of axons in the PNS is considered as nerve       Generally, nerve contains nerve fascicle and fasciculus       Spinal nerve and cranial nerve are used to describe nerve types, but often they are grouped as peripheral nerve and classified as an organ           Ganglion            Equivalent to gray matter in the CNS       Ganglion types are dorsal root ganglion, containing GSA and GVA, cranial nerve ganglion, containing SSA, SVA and SVE, autonomic ganglion, containing GVA and GVE, and ventral ganglion, containing GSE       Dorsal root ganglion, cranial nerve ganglion and autonomic ganglion belong to the PNS while ventral ganglion belongs to the CNS           Overview of Somatic Process  The generic description of somatic process is as follows.                      Information flow in the nervous system from                      Oregon State University                 Well, the figure is self-explanatory, but just to point out:     Sensory neurons detect stimuli from sensory receptors.   Sensation signals from stimuli travel to the dorsal root ganglion through the afferent nerve fibres of the sensory neurons.   Sensation signals is transferred to the neurons of sensory pathways to travel to the brain through the spinal cord.   The brain processes the signals with interneurons and generates innervation signals.   The brain sends innervation signals to the spinal cord through upper motor neurons.   Innervation signals are sent from the spinal cord to the neuromuscular junctions through efferent nerve fibres of the lower motor neurons.   Neuromuscular transmission occurs at the neuromuscular junctions, resulting in muscle contraction.   Other processes follow the same procedures, but just involve different nerve and ganglion, and perhaps additional components.   Reference      https://en.m.wikipedia.org/wiki/Glia  ","categories": ["Neuroscience&#x003a; From Cells to Systems"],
        "tags": [],
        "url": "/posts/neuroscience/neuron_and_neural_tissue/",
        "teaser": null
      },{
        "title": "Spinal Cord, Brain and Nervous System",
        "excerpt":"     Perception is reality.           Lee Atwater   Perception is not true reality, but at least, it is our reality. We have no mean to apprehend the true reality, but only to merely estimate the reality from the observation in a limited and distorted way using our brain.   Neuroscience is the study of the human brain and nervous system, which includes 1) sensing the world, 2) feeling the emotion, 3) forming the memory, 4) processing the logic, and 5) moving the muscle. It is said that if the body is the hardware, the nervous system is the software. Neuroscience combines knowledge from various disciplines to explore the complexity of the human brain and nervous system.   In the field of neuroscience, there are specific biological components that play unique roles. Neurons are specialized cells dedicated to transmitting and processing information. They form neural tissues, which operate collectively to manage information. These tissues assemble to manage information in clusters and contribute to the formation of the spinal cord and brain, which serve as specialized organs in the nervous system.                  Cell                       Neuron,             Neuroglia                             Tissue                       Nerve,             Ganglion,             White Matter,             Gray Matter                             Organ                       Peripheral Nerve,             Brain,             Spinal Cord                             Organ System                       Peripheral Nervous System,             Central Nervous System,                           Somatic Nervous System,             Autonomic Nervous System                 The brain serves as the primary organ for processing information within the human body while the spinal cord facilitates the transmission of information to and from the brain. Along with the peripheral nerves that branch out to the body, they comprise the nervous system. This post aims to provide a tutorial on the two higher levels of body organization in neuroscience, namely the organ level and the organ system level.   Anatomical Division in Nervous System  The nervous system is a type of organ system, responsible for sensation, integration and response. It can be anatomically decomposed into the peripheral nervous system (PNS) and the central nervous system (CNS). The PNS senses the world and sends the status to the CNS and the CNS interprets the signals and sends the commands to the PNS to innervate organs. From robotics point-of-view, if muscles are the actuators, the PNS can be seen as sensors and wires, while the CNS as the central processing unit.   Peripheral Nervous System   The peripheral nervous system (PNS) transmits information from sensation to the central nervous system and the central nervous system to movement.     The PNS is divided into 12 pairs of cranial nerves from the brain and 31 pairs of spinal nerves from the spinal cord that reach out to the entire body.        There is very vague definition of an organ in the PNS, so I will group spinal nerves and cranial nerves as peripheral nerves and classify them as organs in the PNS, so there are total 43 organs in the PNS.                                A spinal nerve from                          Wikimedia Commons                                  Dorsal root and ventral root form a spinal nerve, then branch out to dorsal ramus and ventral ramus.       White ramus and gray ramus are responsible for visceral nerves in one spinal nerve to communicate with visceral nerves in neighbouring spinal nerves.           Each peripheral nerve consists of nerves and ganglia. It contains nerve fibres responsible for somatic and autonomic as well as afferent and efferent. Thus, it manages one or more functions of the body.   The PNS is not enclosed by bone, so it is susceptible to trauma, such as mechanical injuries and toxins, and thus, it easily disconnects from the CNS, resulting in a loss of body function.   Central Nervous System   The central nervous system (CNS) processes every information between sensation and movement.     The CNS is composed of two organs, the spinal cord inside the spine and the brain inside the head.   Both the spinal cord and the brain consist of white matter and gray matter, and are the processing centre of the body. They are the only organs in the CNS.                      The nervous system from                      Wikimedia Commons                    Unlike the PNS, the CNS is physically protected by the vertebrae and skull. There is also a transparent fluid that circulates the CNS, called cerebrospinal fluid (CSF), which serves as a shock absorber and a waste cleaner.        There are two physical characteristics on the surface of the CNS.                                A gyrus and sulcus from                          Wikimedia Commons                                  Gyrus is a fold, ridge or bump and sulcus is a depression or groove. Gyrus is generally surrounded by one or more sulci. Together, they create the folded appearance of the CNS. A larger sulcus is usually called fissure, which is used to divide the CNS.           In neuroanatomy, there are few terms that appear frequently.     Anterior is derived from the Latin, towards the front, and posterior is from the Latin, coming after.   Dorsal is derived from the Latin, back part of the body, and ventral is from the Latin, front part of the body.   Often the CNS uses anterior and posterior while the PNS uses dorsal and ventral.   Therefore, the PNS serves as the conjunction between the CNS and other body parts. The PNS senses the world and sends the status to the CNS via sensory nerves, and the CNS interprets the signals and sends the commands to the PNS to tell the body to move or to conduct resting via motor nerves.   Spinal Cord  The spinal cord is a long tube-like band organ in the CNS, located inside the spine. It receives sensation signals from the PNS, communicates with the brain, and sends innervation signals back to the PNS.   Most of innervation signals are generated by the brain, but there are two types of motor outputs that the spinal cord is responsible of:     Somatic Reflex: Automatic, involuntary responses to sensations, such as knee-jerk reflex and withdrawal reflex. This involves a reflex arc, a circuit made up of spinal interneurons. This is how we maintain the posture with disturbance from external world, like wind or vibration.   Rhythmic Movement: A periodic process that the body repeatedly returns to its starting condition in the absence of rhythmic input, such as walking, swimming, breathing, or chewing. This involves a central pattern generator, a circuit made up of spinal interneurons. This is how we can breath, walk and talk at the same time.   The spinal cord is protected by the spine, which is composed of the vertebrae, or vertebral column, or backbone.                      The spinal cord with spine from                      Wikimedia Commons                    In the spinal cord, the grey matter is in butterfly-shape and the white matter covers the grey matter, so the processing is done at the inner layer of the spinal cord and is sent to either the brain or the spinal nerves at the outer layer.   Between two vertebrae, an intervertebral disk is present. Nerve roots are the nerves outreaching from the CNE, where for the spine, they are either dorsal roots or ventral roots.   Tract  The white matter in the spinal cord is sectioned as tracts, or pathways.                      The spinal tracts from                      Figure 14.5.5 in Anatomy &amp; Physiology                    Afferent pathways            Dorsal column medial lemniscus tract: Responsible for fine touch and conscious proprioception.       Spinothalamic tract: Responsible for crude touch, unconscious proprioception, pain and temperature.           Efferent pathways            Lateral corticospinal tract: Responsible for the movement of the limbs       Anterior corticospinal tract: Responsible for the movement of the axial muscles of the trunk           Where fine touch and crude touch are the two skin sensations.     Fine touch: Touch sensation that can localize where the stimulus is applied.   Crude touch: Touch sensation that cannot localize where the stimulus is applied.   Horizontal View  The horizontal sectional view of the spinal cord is:                      The horizontal sectional view of the spinal cord from                      Wikimedia Commons                    Spinal cord is bilaterally symmetrical, where the axis of symmetry is between anterior median fissure and posterior median sulcus.   Posterior gray commissure, anterior gray commissure and anterior white commissure are the connection between left side and right side of the spinal cord.   Dorsal root is a collection of afferent nerve fibres and ventral root is of efferent nerve fibres. They combine to form the spinal nerve.   Dorsal root ganglion is the ganglion that contains the cell bodies of dorsal root, that includes GSA and GVA.   Posterior gray horn, lateral gray horn and anterior gray horn  encapsulate sensory nuclei and motor nuclei. Motor nuclei is the ganglion of ventral root, or ventral ganglion.   Posterior white column, lateral white column and anterior white column encapsulate tracts that communicate with the brain and other body parts.   Vertical View  The vertial sectional view of the spinal cord is:                      The vertical sectional view of the spinal cord from                      Wikimedia Commons                 There are 31 pairs of spinal nerves branching out from the spinal cord. They are named after the regions of vertebrae:     8 cervical nerves (C1-C8) in 7 cervical vertebrae (C1-C7) for the neck, arms, eyes and glands   12 thoracic nerves (T1-T12) in 12 thoracic vertebrae (T1-T12) for the torso, heart, lung and digestive organs   5 lumbar nerves (L1-L5) in 5 lumbar vertebrae (L1-L5) for front legs, rectum and urinary organs   5 sacral nerves (S1-S5) in 5 sacral vertebrae (S1-S5) for behind legs, rectum and urinary organs   1 coccygeal nerve (Co) in 3 to 5 coccygeal vertebrae (Co1-Co5) for tailbone   To clarify, the number of nerves is different from the number of vertebrae.     One more nerve pair exists than the vertebrae in cervical.   There exists only a single nerve pair on coccygeal while the number of the coccygeal vertebrae varies by person.            Coccyx is the tailbone that is reduced during evolution in humans.           Brain   The brain is a sphere-shaped wrinkly organ in the CNS, located inside the head. It receives sensation signals from the spinal cord, processes them through different regions of the brain, and sends innervation signals back to the spinal cord. The brain is the most complex system in the human body, and some even say the most complex system we have discovered in our universe.   The brain is protected physically by the cranium and chemically by the blood-brain barrier.          Cranium is the bones enclosing the brain, excluding the bones of the face and jaw.                                The cranium from                          Wikimedia Commons                                  There are total six types of cranium bones, frontal bone (yellow), parietal bone (blue), sphenoid bone (purple), temporal bone (orange), occipital bone (green) and ethmoid bone (red).       There are total eight cranium bones, frontal bone (1), parietal bone (2), sphenoid bone (1), temporal bone (2), occipital bone (1) and ethmoid bone (1). Each bone bonds with thick connective tissues, called sutures.                Blood-brain barrier (BBB) is semipermeable barrier the allows the passage of some small moleclues, including ions and amino acids, while restricting the passage of pathogens.       In the brain, grey matter covers white matter, which means the processing is done at the outer layer of the brain and is exchanged between different regions of the brain.                      The white matter (yellow) and grey matter (red) inside the brain from                      Wikimedia Commons                 If more gyrus and sulcus are present, this increases the surface area of the brain, resulting in more gray matter. What this means is that the brain with more crumpled surface has higher capacity to process information.   Brain Division  The brain is divided into two cerebral hemispheres by a longitudinal fissure, or cerebral fissure or interhemispheric fissure. Each cerebral hemisphere is known to be responsible for:     Left cerebral hemisphere: Logics, facts, planning, analysis and right side of the body.   Right cerebral hemisphere: Arts, emotions, creativity, imagination and left side of the body.   They are connected by the corpus callosum.                          Left brain and right brain from                              Wikimedia Commons                                        Corpus callosum from                              Wikimedia Commons                                                                                                      The brain can be divided into several ways. The simplest way is:                              Cerebrum from                              Wikimedia Commons                                            Interbrain from                              Wikimedia Commons                                            Brainstem from                              Wikimedia Commons                                            Cerebellum from                              Wikimedia Commons                                                                                                                                                                       Cerebrum: is composed of cerebral cortex, hippocampus and amygdala. It is the largest part of the brain that forms the hemispheric structure. Cerebrum means brain in Latin.   Interbrain: is composed of thalamus, hypothalamus and pituitary gland. It reside under the cerebrum and usually works with the cerebrum.   Brainstem: is composed of midbrain, pons and medulla. It connects cerebrum, interbrain and spinal cord and serves as relay station.   Cerebellum: is usually considered as a standalone brain region. It is located at the back of the head, just behind the brainstem. Cerebellum means little brain in Latin.   There are other divisions commonly seen in literature:     Three embryonic brain division:            Forebrain: Cerebrum and interbrain.       Midbrain: Midbrain.       Hindbrain: Pons, medulla, and cerebellum.           Five embryonic brain division            Telencephalon: Synonym for cerebrum.       Diencephalon: Synonym for interbrain.       Mesencephalon: Synonym for midbrain.       Metencephalon: Pons and cerebellum.       Myelencephalon: Medulla.           I will stick with the first division,   Cerebrum   Cerebral cortex, or cerebral mantle, in the cerebrum can be further decomposed into:                              Frontal lobe from                              Wikimedia Commons                                            Parietal lobe from                              Wikimedia Commons                                            Temporal lobe from                              Wikimedia Commons                                            Occipital lobe from                              Wikimedia Commons                                                                                                                                                                    Their functions vary by locations.                      Functions of the cerebral cortex from                      Wikimedia Commons                    Frontal: Action processing   Parietal: Sensory processing   Occipital: Visual Processing   Temporal: Visual and auditory processing   Interbrain   The interbrain and other parts of the cerebrum are knwon to work together for the limbic system, which handles emotion, behavior, memory and autonomic responses.                              Thalamus from                              Wikimedia Commons                                            Hypothalamus from                              Wikimedia Commons                                            Hippocampus from                              Wikimedia Commons                                                                                                                                                             Amygdala from                              Wikimedia Commons                                            Pituitary gland from                              Wikimedia Commons                                            Striatum from                              Wikimedia Commons                                                                                                                                        Thalamus: Relay station   Hypothalamus: Gland regulation   Pituitary gland: Hormone production   Hippocampus: Learning and Memory   Amygdala: Emotion   Striatum: Reward   Note that pituitary gland is not part of the limbic system, but it is controlled by the limbic system.   These mostly work together to serve a function. For instance:     Hypothalamus tells pituitary gland to produce hormones, then the hormones will communicate with the body.   Amygdala modulates memory consolidation for emotional memories with hippocampus.   Thalamus and hypothalamus work together in sleeping mechanism.   Brainstem   Brainstem is the relay station between the cranial nerves, the brain and the spinal cord. The cranial nerves are divided as:                      Cranial nerves from                      Wikimedia Commons                    CN I: Olfactory nerve for smell   CN II: Optic nerve for vision   CN III: Oculomotor nerve for eye movement   CN IV: Trochlear nerve for eye movement (look down and inwards)   CN V: Trigeminal nerve for sensation on the face skin and chewing muscle   CN VI: Abducens nerve for eye movement (outwards)   CN VII: Facial nerve for facial expression and tase sensation   CN VIII: Vestibulocochlear nerve for hearing and balance   CN IX: Glossopharyngeal nerve for oral sensation, taste sensation and salivation   CN X: Vagus nerve for autonomic system   CN XI: Accessory nerve for shoulder and head movement   CN XII: Hypoglossal nerve for tongue movement   Similar to the spinal nerves, cranial nerves are considered as the organs in the PNS.   Different regions of the brainstem facilitates different types of cranial nerves, so the functions vary by the cranial nerves.                              Midbrain from                              Wikimedia Commons                                            Pons from                              Wikimedia Commons                                            Medulla from                              Wikimedia Commons                                                                                                                                        Midbrain: CN III and CN IV   Pons: CN V, CN VI, CN VII and CN VIII   Mendulla: CN IX, CN X, CN XI and CN XII   The last two cranial nerves are attached to the other brain regions:     Olfactory cortex in the temporal lobe: CN I   Lateral geniculate nucleus in thalamus: CN II   Cerebellum   Cerebellum is involved in a variety of functions, including coordinating movement, motor learning and language. Despite its size, as name suggests, little brain, it holds more than half of neurons in the brain in a very densely compacted manner.                              Cerebellum from                              Wikimedia Commons                                                                       Therefore, the brain does most of the heavy jobs while the spinal cord generally acts as a pathway to pass messages between the brain and the PNS.   Functional Division in Nervous System  The nervous system can be anatomically decomposed into the PNS and the CNS, but they work together to serve a variety of body functions, which can be largely divided into the somatic nervous system (SNS) and the autonomic nervous system (ANS). These two nervous systems are often considered as sub-divisions of the PNS, but in this section, I explain them with both the PNS and the CNS.   Somatic Nervous System  The somatic nervous system (SNS), or voluntary nervous system, produces the voluntary movements from sensation, such as walking or lifting.     It takes five sensation signals, that are sight, sound, smell, taste and touch, to the CNS through afferent nerves.   It sends innervation signal from the CNS to the skeletal muscles through efferent nerves.   How the commands are sent to different body parts vary by the location and the function.     Voluntary skeletal movements of the head, neck and tongue from the brain directly through the special visceral neurons.   Voluntary skeletal movements of the torso and limbs from the brain through the upper motor neurons and the lower motor neurons.   Involuntary skeletal movements of the body from the spinal cord through the spinal interneurons and the lower motor neurons.   The cerebral cortex processes voluntary skeletal movements, mapping from the sensation in the parietal lobe to the innervation in the frontal lobe.                              Sensory homunculus on the parietal lobe from                              Wikimedia Commons                                            Motor homunculus on the frontal lobe from                              Wikimedia Commons                                                                                                      The signals pass differently based on the location.     The spinal nerves pass signals from the spinal cord to the brainstem.   The cranial nerves pass signals directly through the brainstem.   Thus, while the cerebral cortex does all the processing to integrate the sensation and the innervation, the brainstem acts as the pathway for receiving and sending signals for conscious voluntary skeletal movements.   For involuntary skeletal movements, the spinal cord is responsible for all the processing without involving the brain, that includes receiving signals from the PNS, integrating the sensation and the innervation, and sending signals to the PNS.   Furthermore, there are two divisions to map the SNS.     Dermatome: An area of skin that sensory neurons of a single spinal nerve is connected to.                      Dermatome from                      Wikimedia Commons                    Myotome: An area of muscle that motor neurons of a single spinal nerve is connected to.            C1, C2: Neck flexion and neck extension       C3: Lateral neck flexion       C4: Shoulder elevation       C5: Shoulder abduction       C6: Elbow flexion and wrist extension       C7: Elbow extension and wrist flexion       C8: Thumb extension       T1: Finger abduction and finger adduction       L1, L2: Hip flexion       L3: Knee extension       L4: Ankle dorsi-flexion       L5: Great toe extension       S1: Hip extension, ankle plantar-flexion and ankle eversion       S2: Knee flexion       S3, S4: Anal wink           Similarly, cranial nerves also manage some parts of body sensation and innervation.   Autonomic Nervous System   Autonomic nervous system (ANS), or visceral nervous system, controls the involuntary physiological processes of internal organs, such as cardiac function, respiration, digestion, sexual arousal, and other reflexes, including vomiting, coughing, and sneezing.   Similar to the SNS, it passes sensations, such as visceral pain, to the CNS through afferent nerves, but it regulates involuntary physiologic processes from the hypothalamus to the PNS through efferent nerves. This is why hypothalamus is known as the centre of the ANS.   The ANS can be functionally divided into two nervous systems of opposite role.     Sympathetic nervous system (SympNS) regulates the fight-or-flight response, which is an acute response that takes place in case of an imminent harmful event or intense mental distress. It is responsible for the physiological events that prepare the body for self-defence through a fight or an escape. This activates the blood flow in skeletal muscles and lungs, dilates lungs and blood vessels, and raises the heart rate. Its ganglion is generally located next to the spinal cord.   Parasympathetic nervous system (PSympNS) regulates the resting response, which is basically the opposite response to the fight-or-flight. This slows down heart rate and digestion, and reduces salivation and lacrimation, with the only exception being sexual arousal. Its ganglion is generally located near peripheral organs that they innervate.                      Diagram of sympathetic nervous system (red) and parasympathetic nervous system (blue) from                      Wikimedia Commons                 Both SympNS and PSympNS have excitatory and inhibitory functions.     The SympNS increases heart rate while the PSympNS decreases heart rate.   The SympNS decreases digestive processes while the PSympNS increases digestive processes.   In recent years, some functions that manage the gut from SympNS and PSympNS are grouped as the enteric nervous system (ENS). It controls the digestive system (gastric acid secretion and gastrointestinal tract movement), hormones release and immune system in the gut. Its ganglion is located near digestion system, i.e. gut and intestine.   Another categorization is based on the involvement of the CNS.     Both SympNS and PSympNS has to integrate the CNS, so they are considered as the extrinsic component.   On the contrary, the ENS is capable of operating independently of the CNS. Due to this, the ENS is considered as the intrinsic component, and sometimes referred as the second brain or the brain in the gut.   Summary   Anatomical Division in Nervous System     The nervous system is anatomically divided into the PNS and the CNS   The PNS transmits information from sensation to the CNS and the CNS to movement            The PNS is composed of 43 peripheral nerves, 12 pairs of cranial nerves from the brain and 31 pairs of spinal nerves from the spinal cord       Each peripheral nerve consists of nerves and ganglia, managing one or more functions of the body, i.e. somatic, autonomic, afferent and efferent       The PNS is not protected both physically and chemically, so it is susceptible to trauma           The CNS processes every information between sensation and movement            The CNS is composed of two organs, the spinal cord inside the spine and the brain inside the head       Both the spinal cord and the brain consist of white matter and gray matter       Unlike the PNS, the CNS is physically protected by the vertebrae, the skull and the CSF       Two physical characteristics that appear on the surface of the CNS are gyrus and sulcus                    A larger sulcus is usually called fissure, which is used to divide the CNS                           Spinal Cord     The spinal cord is responsible for somatic reflex and rhythmic movement   The spinal cord is protected by vertebrae physically and by CSF chemically   In the spinal cord, the white matter covers the gray matter, forming butterfly shape   Tract, or pathway, is the white matter in the spinal cord that transfers signals received from the PNS to the brain            Afferent pathway includes dorsal column medial lemniscus tract and spinothalamic tract       Efferent pathway includes lateral corticospinal tract and anterior corticospinal tract           The spinal cord can be bilaterally divided symmetrically by anterior median fissure and posterior median sulcus            The left side and the right side of the spinal cord are connected by posterior gray commissure, anterior gray commissure and anterior white commissure       The region of the white matter in the spinal cord can be divided by posterior white column, lateral white column and anterior white column       The region of the gray matter in the spinal cord can be divided by posterior gray horn, lateral gray horn and anterior gray horn           Vertically, the spinal cord is divided by its neighbouring vertebrae            8 cervical nerves (C1-C8) in 7 cervical vertebrae (C1-C7) for the neck, arms, eyes and glands       12 thoracic nerves (T1-T12) in 12 thoracic vertebrae (T1-T12) for the torso, heart, lung and digestive organs       5 lumbar nerves (L1-L5) in 5 lumbar vertebrae (L1-L5) for front legs, rectum and urinary organs       5 sacral nerves (S1-S5) in 5 sacral vertebrae (S1-S5) for behind legs, rectum and urinary organs       1 coccygeal nerve (Co) in 3 to 5 coccygeal vertebrae (Co1-Co5) for tailbone           Brain     The brain is responsible for almost everything that is related to perception, cognition and consciouness   In the brain, the gray matter covers the white matter, which means denser the gyrus and sulcus are, more the gray matter   The brain is splitted the left and the right cerebral hemispheres with corpus callosum that connects the two cerebral hemispheres            The brain can be divided into cerebrum, interbrain, brainstem and cerebellum                    Cerebrum is divided into the frontal lobe, the parietal lobe, the temporal lobe and the occipital lobe, responsible for action, sensory, visual and visual/auditory           Interbrain is divided into the thalamus, the hypothalamus, the pituitary gland, the hippocampus, the amygdala and the striatum, responsible for relay station, gland regulation, hormone production, learning and memory, emotion and reward, respectively           Brainstem is divided into the midbrain, the pons and the mendulla that facilitate CN III - IV, CN V - VIII and CN IX - XII, respectively                            CN I - II are facilitated in other brain parts                                   Cerebellum is responsible for coordinating movement, motor learning and language                           Functional Division in Nervous System     The nervous system is functionally divided into the SNS and the ANS   The SNS is mainly responsible for the voluntary movements            It takes five sensation signals, that are sight, sound, smell, taste and touch, and sends innervation signal to the skeletal muscles.       Voluntary skeletal movements are generated from the brain                    Sensory signals are processed on the parietal lobe of the brain whereas motor signals are processed on the frontal lobe           The spinal nerves pass signals from the spinal cord to the brainstem whereas the cranial nerves pass directly through the brainstem                       Involuntary skeletal movements are generated from the spinal cord       Dermatome is mapping the sensory parts of the SNS whereas myotome is mapping the motor parts           The ANS is mainly responsible for the involuntary physiological processes            It takes visceral pain signals and other sensation signals, and regulates the physiological processes of the internal organs using the hypothalamus       The ANS is further divided into SympNS that regulates the fight-or-flight response, PSympNS that regulates the resting response, and ENS that regulates the digestive system       The SympNS and PSympNS are considered as the extrinsic component whereas the ENS is considered as the intrinsic component           Nervous System Division   Anatomically:     The central nervous system in the spine and head            The spinal cord and the brain                    The white matter and gray matter                           The peripheral nervous system in the peripherals            The spinal nerves and the cranial nerves                    The nerves and ganglia                           Functionally:     The somatic nervous system that controls the skeletal muscles            Dematome and sensory homunculus       Myotome and motor homunculus           The autonomic nervous system that controls the smooth muscles and glands            The sympathetic nervous system       The parasympathetic nervous system       The enteric nervous system           Most organs require all the nervous systems to be integrated. For instance, an eye is an organ that uses a mixture of visual tracking (SNS), pupil dilation (SympNS) and pupil constriction (PSympNS).  ","categories": ["Neuroscience&#x003a; From Cells to Systems"],
        "tags": [],
        "url": "/posts/neuroscience/spinal_cord_brain_and_nervous_system/",
        "teaser": null
      }]
