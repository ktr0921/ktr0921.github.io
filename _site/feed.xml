<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-08-15T22:36:40+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kelâ€™Logg</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>D. K. Ryu</name></author><entry><title type="html">Neuron and Neural Tissue</title><link href="http://localhost:4000/posts/neuroscience/neuron_and_neural_tissue/" rel="alternate" type="text/html" title="Neuron and Neural Tissue" /><published>2023-04-10T00:00:00+09:00</published><updated>2023-04-15T00:00:00+09:00</updated><id>http://localhost:4000/posts/neuroscience/from_cells_to_systems_1_neuron_and_neural_tissue</id><content type="html" xml:base="http://localhost:4000/posts/neuroscience/neuron_and_neural_tissue/">&lt;blockquote&gt;
    Perception is reality.
    &lt;br /&gt;
    &lt;cite&gt;Lee Atwater&lt;/cite&gt;
&lt;/blockquote&gt;

&lt;p&gt;Perception is &lt;em&gt;not true reality&lt;/em&gt;, but at least, it is &lt;em&gt;our reality&lt;/em&gt;. We have no mean to apprehend the true reality, but only to merely estimate the reality from the observation in a limited and distorted way using &lt;em&gt;our brain&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Neuroscience&lt;/strong&gt; is the study of the human brain and nervous system, which includes 1) sensing the world, 2) feeling the emotion, 3) forming the memory, 4) processing the logic, and 5) moving the muscle. It is said that if the body is the hardware, the nervous system is the software. Neuroscience combines knowledge from various disciplines to explore the complexity of the human brain and nervous system.&lt;/p&gt;

&lt;p&gt;In the field of neuroscience, there are specific biological components that play unique roles. Neurons are specialized cells dedicated to transmitting and processing information. They form neural tissues, which operate collectively to manage information. These tissues assemble to manage information in clusters and contribute to the formation of the spinal cord and brain, which serve as specialized organs in the nervous system.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; &lt;b&gt;Cell&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 500px&quot;&gt;
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Neuron&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Neuroglia&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; &lt;b&gt;Tissue&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 500px&quot;&gt;
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Nerve&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Ganglion&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;White Matter&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Gray Matter&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; &lt;b&gt;Organ&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 500px&quot;&gt;
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Peripheral Nerve&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Brain&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Spinal Cord&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; &lt;b&gt;Organ System&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 500px&quot;&gt;
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Peripheral Nervous System&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Central Nervous System&lt;/a&gt;,
            &lt;br /&gt;
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Somatic Nervous System&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Autonomic Nervous System&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;A neuron acts as an electrical device that sends a signal by inlet and outlet of charged ions through ion channels on the membrane. Together with neuroglia, they make up neural tissues that transfer information or process information. This post aims to provide a tutorial on the two lower levels of body organization in neuroscience, namely the cell level and the tissue level.&lt;/p&gt;

&lt;h1 id=&quot;neuron&quot;&gt;Neuron&lt;/h1&gt;

&lt;p&gt;A &lt;strong&gt;neuron&lt;/strong&gt;, or nerve cell or neuronal cell or neurocyte, is the fundamental unit of the nervous system, responsible for sending and receiving electrochemical signals to interact with the world. It is the smallest component that is responsible for everything that involves perception, cognition and consciousness.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 400px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_1/1_1_neuron.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        A neuron from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Neuron.svg&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;A neuron is structurally unique.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dendrite&lt;/strong&gt; is a tree branch-like shape filament that receives input signals from other neurons. Usually, it outreaches from the &lt;em&gt;cell body&lt;/em&gt;. which contains &lt;em&gt;nucleus&lt;/em&gt; in its centre.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Axon&lt;/strong&gt;, or &lt;strong&gt;nerve fibre&lt;/strong&gt; is a long tube-like shape filament that passes the received signals from dendrites to the other end. It is one of the commonly damanged parts, since it is approximately 50 times thinner than hair.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Myelin sheath&lt;/strong&gt;, or &lt;strong&gt;myelination&lt;/strong&gt;, is a lipid-rich material of &lt;em&gt;neuroglia&lt;/em&gt;, such as &lt;em&gt;Schewan cell&lt;/em&gt;, that wraps the axon to protect it from physical trauma. &lt;strong&gt;Node of Ranvier&lt;/strong&gt; is the gap between myelin sheaths. Myelin sheath is not part of neuron and is not found in every neuron.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Axon terminal&lt;/strong&gt; is a splitted axon at the other end that sends the passed signals to other neurons.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A neuron is also functionally unique.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Neurotransmission&lt;/strong&gt;: The chemical signal between axon terminal of one neuron and dendrite of another neuron.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Action potential propagation&lt;/strong&gt;: The electrical signal through axon from dendrite to axon termianl within the neuron.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this section, I will briefly introduce the types of neurons, then, focus on the two core functions of neurons, neurotransmission and action potential propagation.&lt;/p&gt;

&lt;h2 id=&quot;neuron-type&quot;&gt;Neuron Type&lt;/h2&gt;
&lt;p&gt;There are many different ways to differentiate neurons. Morphologically, four types of neuron exist.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Unipolar&lt;/strong&gt;: One axon extending from the cell body to dendrites.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bipolar&lt;/strong&gt;: Two axons extending from the cell body to dendrites and axon terminals separately.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Multipolar&lt;/strong&gt;: Dendrites on the cell body and one axon extending from the cell body to axon terminals.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pseudounipolar&lt;/strong&gt;: One axon extending from the cell body splited into two branches in different lengths, one for dendrites and the other for axon terminals.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 300px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_1/1_2_neuron_type.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        A neuron type from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Neurons_uni_bi_multi_pseudouni.svg&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Unipolar neuron is only present in invertebrates, such as insects, so humans only have neurons of bipolar, multipolar and pseudopolar. Multipolar neuron has two variants in human.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Pyramidal&lt;/strong&gt;: Multipolar with dendrites in pyramidal shape for excitatory neurotransmitter.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Purkinje&lt;/strong&gt;: Multipolar with dendrites in planar fan-shaped for inhibitory neurotransmitter.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The morphology of neurons usually follows its functionality. There are three functional classes of neuron.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sensory neuron&lt;/strong&gt;: Passes the sensation signal from the external world to the brain. Most are bipolar or pseudounipolar.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interneuron&lt;/strong&gt;: Connects sensory neurons, interneurons and/or motor neurons, forming circuits of various complexity. Most are multipolar, pyramidal, or Purkinje.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Motor neuron&lt;/strong&gt;: Passes the innervation signal from the brain to muscles and glands. Most are also multipolar.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The functional types of neurons vary drastically by the location.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Neurons found in the peripheral nervous system (PNS) are generally sensory neurons or motor neurons.
    &lt;ul&gt;
      &lt;li&gt;Sensory neuron branches out towards the sensory organs (long) and the CNS (short) separately.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Neurons found in the central nervous system (CNS) are generally interneurons.
    &lt;ul&gt;
      &lt;li&gt;There are extensive number of neuron types in the brain, distinguished by neurotransmitter type, structure, synapse location, electrical properties, gene expressions, projection patterns and many more. Due to this, yet community has not come to an agreement on how to distinguish them.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, the process can be thought of as:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Receive the sensation signal through sensory neurons in the PNS.&lt;/li&gt;
  &lt;li&gt;Process the information through interneurons in the CNS.&lt;/li&gt;
  &lt;li&gt;Convey the innervation signal through motor neurons in the PNS.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Of course, there are neurons in the brain, involved in sensation, i.e. visual cortex or auditory cortex, or in movement, i.e. motor cortex, but they are composed of interneurons. Sensory neurons and motor neurons are usually present in the PNS to deliver signals between the body and the CNS. Most of the processing is done with interneurons in the CNS.&lt;/p&gt;

&lt;h2 id=&quot;neurotransmission&quot;&gt;Neurotransmission&lt;/h2&gt;

&lt;p&gt;A &lt;strong&gt;neurotransmission&lt;/strong&gt; is the chemical process that a neuron communicates with another neuron. The process involves two neurons, releasing and receiving signalling molecules, called &lt;strong&gt;neurotransmitters&lt;/strong&gt;, at the junction between two neurons, called &lt;strong&gt;synapse&lt;/strong&gt;. The two neurons are named based on whether to release or receive neurotransmitters, or &lt;em&gt;the direction of the signal&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Presynaptic neuron&lt;/strong&gt;: The neuron that sends the neurotransmitters from the axon terminal.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Postsynaptic neuron&lt;/strong&gt;: The neuron that receives the neurotransmitters at the dendrite.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A neurotransmitter has two opposite roles, either &lt;em&gt;excite (initiate)&lt;/em&gt; or &lt;em&gt;inhibit (hinder)&lt;/em&gt; the activation of an action potential propagation in the postsynaptic neuron.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Excitatory neurotransmitter: &lt;strong&gt;Glutamate&lt;/strong&gt;, noradrenaline and histamine.&lt;/li&gt;
  &lt;li&gt;Inhibitory neurotransmitter: &lt;strong&gt;Gamma-aminobutyric acid (GABA)&lt;/strong&gt;, serotonin and glycine.&lt;/li&gt;
  &lt;li&gt;Excitatory and inhibitory neurotransmitter: Acetylcholine and dopamine.
    &lt;ul&gt;
      &lt;li&gt;Acetylcholine is excitatory to contract skeletal muscles while inhibitory to slow the heart rate.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Glutamate and GABA predominate the neurotransmitters in the brain, 80\% ~ 90\% and 30\% ~ 40\% synapses, respectively. Note that a neuron may have more than one neurotransmitter types, so most of neurons with glutamate also carry other neurotransmitters in the brain.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Glutamate helps learning and memory whereas GABA produces a calming effect.&lt;/li&gt;
  &lt;li&gt;Glutamate is the metabolic precursor of GABA, where glutamate is decarboxylated to GABA and CO2. Which means GABA is produced from glutamate.&lt;/li&gt;
  &lt;li&gt;Since GABA is for calming effects, some people take GABA for anxiety relief or ADHD.&lt;/li&gt;
  &lt;li&gt;Excessive glutamate is known to cause overexcitement of neurons and may result in the death of neurons, or neuroapoptosis. This phenomenon is called &lt;strong&gt;excitotoxicity&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is very important to balance excitatory and inhibitory neurotransmitters. In a cell level, it determines whether an action potential will result or not, and in an organ level, excess or lack of one may result in neurological or mental diseases.&lt;/p&gt;

&lt;p&gt;The full process of a neurotransmission is:&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 400px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_1/1_3_neurotransmission.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        A neurotransmission from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:SynapseSchematic_en.svg&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;Action potential travels through presynaptic neuron and reaches the axon terminal of the postsynaptic neuron.&lt;/li&gt;
  &lt;li&gt;Action potential at the axon terminal opens voltage-gated Ca&lt;sup&gt;2+&lt;/sup&gt; channels, allowing Ca&lt;sup&gt;2+&lt;/sup&gt; influx to the axon terminal.&lt;/li&gt;
  &lt;li&gt;Neurotransmitters are released from the axon terminals of the presynaptic neuron into theÂ synaptic cleft.&lt;/li&gt;
  &lt;li&gt;Neurotransmitter binds to neurotransmitter receptors on the dendrites of the postsynaptic neuron.&lt;/li&gt;
  &lt;li&gt;This binding opens ion channels, positive (e.g. Na&lt;sup&gt;+&lt;/sup&gt;, K&lt;sup&gt;+&lt;/sup&gt;, Ca&lt;sup&gt;2+&lt;/sup&gt;) or negative (e.g. Cl&lt;sup&gt;-&lt;/sup&gt;) depending on the neurotransmitter type, allowing ion influx to the membrane of postsynaptic neuron.&lt;/li&gt;
  &lt;li&gt;Neurotransmitters attached to the dentrites of the postsynaptic neuron are detached and collected through neurotransmitter transporters.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Few details are,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(3) Synaptic cleft is a 20nm ~ 40nm gap between theÂ presynapticÂ axonÂ terminal and theÂ postsynaptic dendrite. Synapse includes all the presynapticÂ axonÂ terminal, the synaptic cleft and theÂ postsynaptic dendrite.&lt;/li&gt;
  &lt;li&gt;(4, 5) Neurotransmitters are not the ones that travel into postsynaptic neurons, but ions are.&lt;/li&gt;
  &lt;li&gt;(8) Neurotransmitter transporters are responsible for maintaining appropriate levels of neurotransmitters inside and outside neurons.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A neurotransmission occurs between other cells in muscles and glands. In the case of muscle, it is called &lt;em&gt;neuromuscular transmission&lt;/em&gt;, covered in the later section.&lt;/p&gt;

&lt;h2 id=&quot;action-potential-propagation&quot;&gt;Action Potential Propagation&lt;/h2&gt;

&lt;p&gt;An &lt;strong&gt;action potential propagation&lt;/strong&gt; is the electrical process that a signal is propagated within a single neuron from dendrites to axon terminals. The process involves the changes of electric potentials inside the membrane, called &lt;strong&gt;action potential&lt;/strong&gt;, or nerve impulse or spike.&lt;/p&gt;

&lt;p&gt;The full process of an action potential propagation is:&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 750px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_1/1_4_action_potential_propagation.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        An action potential propagation from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Membrane_Permeability_of_a_Neuron_During_an_Action_Potential.svg&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;Membrane potential stays at the &lt;strong&gt;restingÂ membrane potential&lt;/strong&gt;. There is lower concentration of Na&lt;sup&gt;+&lt;/sup&gt; (a) and higher concentration of K&lt;sup&gt;+&lt;/sup&gt; inside the axon.&lt;/li&gt;
  &lt;li&gt;During neurotransmission, if more excitatory neurotransmitters than inhibitory neurotransmitters bind to the dendrites, the positive ions enter the membrane of the neuron, increasing the membrane potential.&lt;/li&gt;
  &lt;li&gt;If the membrane potential rises to the &lt;strong&gt;action potential threshold&lt;/strong&gt;, the action potential propagation initiates. In other words, neuron fires.&lt;/li&gt;
  &lt;li&gt;An increased membrane potential activates voltage-gated Na&lt;sup&gt;+&lt;/sup&gt; channels (c) to inlet Na&lt;sup&gt;+&lt;/sup&gt; into the axon, increasing membrane potentials further, called &lt;strong&gt;depolarization&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Na&lt;sup&gt;+&lt;/sup&gt; influx activates voltage-gated K&lt;sup&gt;+&lt;/sup&gt; channels (d) to outlet K&lt;sup&gt;+&lt;/sup&gt; out of the axon, decreasing membrane potentials, called &lt;strong&gt;repolarization&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;The outlet of K&lt;sup&gt;+&lt;/sup&gt; is more excessive than the inlet of Na&lt;sup&gt;+&lt;/sup&gt;, causing undershoot of the membran potential to drop below the resting membrane potential, called &lt;strong&gt;hyperpolarization&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;The resting potential is ultimately re-established by closing of all voltage-gated ion channels and the activitation of the Na&lt;sup&gt;+&lt;/sup&gt;/K&lt;sup&gt;+&lt;/sup&gt; pump (e). This is called &lt;strong&gt;refractory period&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;(4,5,6,7) occur consecutively through the axon until the axon terminal.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Few details are,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1, 3) Membrane potential is the difference in electrical potential between inside and outside of a cell. The restingÂ membrane potential and action potential thresholdÂ are usually -70 mV and -55 mV, but they vary drastically by neuron types.&lt;/li&gt;
  &lt;li&gt;(2) During neurotransmission, if more inhibitory neurotransmitters than excitatory neurotransmitters bind to the dendrites, the membrane potential decreases, and thus no action potential.&lt;/li&gt;
  &lt;li&gt;(3) If the membrane potential does not rise to the action potential threshold, no action potential propagation is initiated.&lt;/li&gt;
  &lt;li&gt;(6) Hyperpolarization is known to prevent action potential to travel backwards.&lt;/li&gt;
  &lt;li&gt;(4,5,6) are the process of action potential propagation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Through action potential propagation, a neuron consecutively opens and closes electrically activated ion channels to pass electrical impulse from dendrites to axon terminals. Then, at the end of axon terminals, a neurotransmission occurs with a postsynaptic neuron. In the postsynaptic neuron, the sum total of dendritic inputs, excitatory as positive charges and inhibitory as negative charges, determines whether the neuron will fire an action potential.&lt;/p&gt;

&lt;p&gt;Myelin sheath is a lipid-rich fatty substance of neuroglia that encases axons. It is very important in action potential propagation as it increases the rate of action potential propagation. This phenomenon is referred to as &lt;strong&gt;saltatory conduction&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The exchanges of ions can only occur at unmyelinated part of axon, so the increase in the conduction velocity of action potential propagation is because the action potential is &lt;em&gt;skipped&lt;/em&gt; at myelinated areas. Due to this myelin sheaths are often seen as an insulator for action potential propagation.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 350px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_1/1_5_saltatory_conduction.gif&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Saltatory conduction from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Saltatory_Conduction.gif&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h1 id=&quot;nerve-fibre&quot;&gt;Nerve Fibre&lt;/h1&gt;
&lt;p&gt;A &lt;strong&gt;nerve fibre&lt;/strong&gt; is another name for an &lt;em&gt;axon&lt;/em&gt; that is commonly used to classify a neuron serving a particular function. This can be considered as a subdivision of sensory neurons and motor neurons. Here, the naming convention is based on the direction of the information flow, that is sensory neurons as &lt;em&gt;afferent neurons&lt;/em&gt; and motor neurons as &lt;em&gt;efferent neurons&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;There are three subdivisions that exist in nerve fibre.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Location: A nerve fibre can be &lt;strong&gt;general&lt;/strong&gt; or &lt;strong&gt;special&lt;/strong&gt;. The former refers to a nerve fibre branching out from the spinal cord in the CNS and the latter from the brain in the CNS.&lt;/li&gt;
  &lt;li&gt;Function: A nerve fibre can be &lt;strong&gt;somatic&lt;/strong&gt; or &lt;strong&gt;autonomic&lt;/strong&gt;. Somatic refers to as &lt;em&gt;voluntary&lt;/em&gt; and autonomic as &lt;em&gt;involuntary&lt;/em&gt;. Visceral is another name for autonomic. Therefore, somatic nerve fibre is for voluntary system while visceral nerve fibre is for involuntary system.&lt;/li&gt;
  &lt;li&gt;Direction: A nerve fibre can be &lt;strong&gt;afferent&lt;/strong&gt; or &lt;strong&gt;efferent&lt;/strong&gt;. Afferent is to bring sensations to CNS and efferent is to send commands from CNS. Afferent is derived from the Latin &lt;em&gt;afferre&lt;/em&gt; that means &lt;em&gt;to bring towards&lt;/em&gt; and efferent is from the Latin &lt;em&gt;efferre&lt;/em&gt;, &lt;em&gt;to bring away from&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A nerve fibre is named with a combination of these three subdivisions, such as &lt;em&gt;general somatic afferent nerve fibre&lt;/em&gt; or &lt;em&gt;special visceral efferent nerve fibre&lt;/em&gt;. This naming convention is used to describe neural tissues while morphological and functional classifications are used more on neurons.&lt;/p&gt;

&lt;p&gt;Herein, I will mainly focus on the general function and idea on afferent nerve fibre and efferent nerve fibre. Note that although there exist neurons that send and receive signals in the CNS, this content is mainly on the PNS.&lt;/p&gt;

&lt;h2 id=&quot;afferent-nerve-fibre&quot;&gt;Afferent Nerve Fibre&lt;/h2&gt;

&lt;p&gt;An &lt;strong&gt;afferent nerve fibre&lt;/strong&gt; is an axon of &lt;strong&gt;afferent neuron&lt;/strong&gt;, or sensory neuron, that brings the sensation signals from the body to the CNS. There are four types of afferent nerve fibres.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;General somatic afferent (GSA) nerve fibre&lt;/strong&gt;: Carry pain, touch or temperature from the skins, muscles, tendons or joints to the CNS through the spinal nerve.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;General visceral afferent (GVA) nerve fibre&lt;/strong&gt;: Carry pain or physiological sensation from the internal organs, glands or blood vessels to the CNS through the spinal nerve.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Special somatic afferent (SSA) nerve fibre&lt;/strong&gt;: Carry vision, hearing or balance from the eyes, ears or body to the CNS through cranial nerve.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Special visceral afferent (SVA) nerve fibre&lt;/strong&gt;: Carry smell or taste from the nose or tongue to the CNS through cranial nerve.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An sensory neuron is a neuron that is activated by sensory input from the environment or within the body. There are largely two sensations in the human body.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Exteroception&lt;/strong&gt;: Sensation from outside the body.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interoception&lt;/strong&gt;: Sensation from inside the body.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The action potential propagation of an sensory neuron is initiated by four types of sensations of different stimuli.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Chemosensation&lt;/strong&gt;: Induced by &lt;strong&gt;chemoreceptor&lt;/strong&gt; that detects chemical molecule.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mechanosensation&lt;/strong&gt;: Induced by &lt;strong&gt;mechanoreceptor&lt;/strong&gt; that detects mechanical force.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Thermalsensation&lt;/strong&gt;: Induced by &lt;strong&gt;thermoreceptor&lt;/strong&gt; that detects thermal excitation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Photosensation&lt;/strong&gt;: Induced by &lt;strong&gt;photoreceptor&lt;/strong&gt; that detects photoexcitation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The stimuli open ion channels and depolarize membrane potentials to initiate action potential propagation. Exteroception and interoception contain the sensations of different stimuli.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Exteroception
    &lt;ul&gt;
      &lt;li&gt;Chemosensation for smell and taste.&lt;/li&gt;
      &lt;li&gt;Mechanosensation for touch and hearing.&lt;/li&gt;
      &lt;li&gt;Thermalsensation for heat and chill.&lt;/li&gt;
      &lt;li&gt;Photosensation for sight.&lt;/li&gt;
      &lt;li&gt;Chemosensation, mechanosensation and thermalsensation for pain.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Interoception
    &lt;ul&gt;
      &lt;li&gt;Chemosensation for blood sugar level.&lt;/li&gt;
      &lt;li&gt;Mechanosensation for blood pressure and body position.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Exteroception covers the majority of conscious sensations we use to perceive the world. Interoception, on the other hand, covers both conscious and non-conscious sensations that are important to physiology. Due to this, major research on sensations is driven by exteroception, and only recently, researchers started focusing on interoception, such as the intersection with immune system or gastrointestinal system. Thus, interoception is still quite gray area.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Some literature puts the sense of body position as different sensation from exteroception and interoception, but here, I included in interoception.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;efferent-nerve-fibre&quot;&gt;Efferent Nerve Fibre&lt;/h2&gt;

&lt;p&gt;An &lt;strong&gt;efferent nerve fibre&lt;/strong&gt; is an axon of &lt;strong&gt;efferent neuron&lt;/strong&gt;, or motor neuron, that sends the innervation signals from the CNS to the body. There are three types of efferent nerve fibres.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;General somatic efferent (GSE) nerve fibre&lt;/strong&gt;: Carry motor innervation from the spinal cord to skeletal muscles through the spinal nerve.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;General visceral efferent (GVE) nerve fibre&lt;/strong&gt;: Carry motor innervation from the spinal cord to smooth muscles, cardiac muscles or glands through the spinal nerve.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Special visceral efferent (SVE) nerve fibre&lt;/strong&gt;: Carry motor innervation from the brain to the muscles of the pharyngeal arches, which include head, neck and tongue through the cranial nerve.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There is no special somatic efferent nerve fibre, instead, SVE nerve fibre takes care of both somatic and visceral functions. Due to this, the neuron of SVE nerve fibre is often referred to as &lt;strong&gt;branchial motor neuron&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;An motor neuron is a neuron that is activated by innervation output from the CNS. It can be divided based on the location or function.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Location: An &lt;strong&gt;upper motor neuron&lt;/strong&gt; is between the brain and the spinal cord while a &lt;strong&gt;lower motor neuron&lt;/strong&gt; is between the spinal cord to muscles or glands.&lt;/li&gt;
  &lt;li&gt;Function: A &lt;strong&gt;somatic motor neuron&lt;/strong&gt; is to control skeletal muscle while a &lt;strong&gt;visceral motor neuron&lt;/strong&gt; is for smooth muscle, cardiac muscle and glands.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Upper motor neurons are of the CNS, so GSE, GVE and SVE nerve fibres are part of lower motor neurons.&lt;/p&gt;

&lt;p&gt;Muscle contraction is the main function of GSE fibres. They activate skeletal muscles through a process called &lt;strong&gt;neuromuscular transmission&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 700px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_1/2_1_neuromuscular_junction.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        A neuromuscular junction from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:The_Muscle_Contraction_Process.png&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ol&gt;
  &lt;li&gt;Action potential travels through motor neuron (A) and reaches the axon terminal of the motor neuron (B).&lt;/li&gt;
  &lt;li&gt;Depolarization at the axon terminal opens voltage-gated Ca&lt;sup&gt;2+&lt;/sup&gt; channels, allowing Ca&lt;sup&gt;2+&lt;/sup&gt; influx to the axon terminal.&lt;/li&gt;
  &lt;li&gt;Neurotransmitters, mostly acetylcholine (ACh), are released from the axon terminals of the motor neuron into theÂ &lt;strong&gt;neuromuscular junction&lt;/strong&gt;, or myoneural junction (C). This process is referred to as &lt;strong&gt;exocytosis&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;ACh binds to post-synaptic receptors on the membrane of the muscle cell, or sarcolemma (D).&lt;/li&gt;
  &lt;li&gt;This binding opens Na&lt;sup&gt;+&lt;/sup&gt; channels, allowing Na&lt;sup&gt;+&lt;/sup&gt; influx to the membrane of muscle cell.&lt;/li&gt;
  &lt;li&gt;An increase in Na&lt;sup&gt;+&lt;/sup&gt; generates the action potential propagation in the muscle cell, resulting in muscle contraction via myofibril (E).&lt;/li&gt;
  &lt;li&gt;Neurotransmitters attached to the membrane of the muscle cell are detached and collected through neurotransmitter transporters.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Few details are,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(3) ACh is excitatory to contracts the skeletal muscles but inhibitory and excitatory for involuntary systems. It contracts smooth muscles (excitation) while slowing the heart rate (inhibition).&lt;/li&gt;
  &lt;li&gt;(6) Depolarization triggers the release of Ca&lt;sup&gt;2+&lt;/sup&gt;. Then, it binds to troponin to initiate contraction, which is sustained by ATP. As long as Ca&lt;sup&gt;2+&lt;/sup&gt; remains bound to troponin and ATP is available, the contraction is maintained.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Relaxation is opposite to contraction, where either no more AChs produced from motor neurons or ATPs run out. Although this process is specific to somatic motor neuron stimulating skeletal muscle, the logic can be generally applies to others, such as visceral motor neuron stimulating glands.&lt;/p&gt;

&lt;h1 id=&quot;neural-tissue&quot;&gt;Neural Tissue&lt;/h1&gt;
&lt;p&gt;A &lt;strong&gt;neural tissue&lt;/strong&gt;, or nervous tissue, is a group of cells working together to perform a function in the nervous system. A neuron alone is nothing more than a cell, but a collection of neurons with neuroglia serves a purpose. Again, similar to neurons, neural tissues found in the PNS and the CNS are different.&lt;/p&gt;

&lt;p&gt;There are two types of neural tissues in the PNS of the human.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Nerve&lt;/strong&gt;: An enclosed bundle of axons, or nerve fibers. Action potentials are transmitted as a bundle to muscles, glands or CNS.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ganglion&lt;/strong&gt;: A cluster of dendrites, neuronal cell bodies, axon terminals and neuroglia. Neurotransmission occurs between a lot of neurons, exciting or inhibiting the activation of action potential propagation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A nerve usually contains a mixture of different nerve fibres while a ganglion contains neurons of a particular function.&lt;/p&gt;

&lt;p&gt;In the CNS, neural tissues share similar characteristics to nerve and ganglion.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;White Matter&lt;/strong&gt;: Nerve in CNS, but action potentials are transmitted between the various grey matter, and between the gray matter and the rest of the body along with myelination of oligodendrocytes. Myelin sheath is what makes white matter white.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Grey Matter&lt;/strong&gt;: Ganglion in CNS. Grey matter is actually more pink-coloured than grey because of the abundant blood supply.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;White matter is where the information is transferred while grey matter is where the information is processed. Their structures and functions can be somewhat generalized as nerve and ganglion, but in details, they vary drastically by location.&lt;/p&gt;

&lt;p&gt;This section mostly focuses on the neural tissues in the PNS. Thus, I will cover basics of neuroglia, then introduce nerve and ganglion.&lt;/p&gt;

&lt;h2 id=&quot;neuroglia&quot;&gt;Neuroglia&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Neuroglia&lt;/strong&gt;, or glia or glial cell, is a cell that provides support for neurons. Their functions vary by a cell type, from metabolic to physiological function. Largely, there are six types of neuroglia.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 500px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_1/3_1_neuroglia_type.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        A neuroglia type from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Blausen_0870_TypesofNeuroglia.png&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Ependymal cell&lt;/strong&gt;: Surface for neural tissues in the CNS.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Astrocyte&lt;/strong&gt;: Metabolism in the CNS.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Oligodendrocyte&lt;/strong&gt;: Myelination in the CNS.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Microglia&lt;/strong&gt;: Immune system in the CNS.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Satellite cell&lt;/strong&gt;: Metabolism in the PNS.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Schwann cell&lt;/strong&gt;: Myelination in the PNS.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although neuroglia appear to be less important than neurons, they are believed to contribute in learning and memory.&lt;/p&gt;

&lt;p&gt;Neurons and neuroglia make up &lt;strong&gt;extracellular matrix&lt;/strong&gt; with a composition of proteins.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 400px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_1/3_2_neuroglia_type_brain.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        An extracellular matrix in the brain from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Glial_Cell_Types.png&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Ependymal cell is pink.&lt;/li&gt;
  &lt;li&gt;Astrocyte is green.&lt;/li&gt;
  &lt;li&gt;Oligodendrocyte is blue.&lt;/li&gt;
  &lt;li&gt;Microglia is red.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Extracellular matrix is important to maintain the shape and regulate the processes, which are done by many different proteins. We refer to a group of neurons, glia and extracellular matrix as &lt;em&gt;neural tissue&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;There are many proteins that provide structural and biochemical supports to cells, but some unique and important proteins in the extracellular matrix of the neural tissue are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Neurotrophin&lt;/strong&gt;, or neurotrophic factor: Regulating survival, development and function of neurons, which belongs to the growth factor family. It is one of the most widely studied proteins, where &lt;em&gt;nerve growth factor (NGF)&lt;/em&gt;, &lt;em&gt;brain-derived neurotrophic factor (BDNF)&lt;/em&gt;, neurotrophin-3 (NT-3) and neurotrophin-4 (NT-4) belong to this.&lt;/li&gt;
  &lt;li&gt;Neuronal apoptosis inhibitory protein (NAIP): Regulating an apoptosis of a neuron, which belongs to the inhibiter of apoptosis (IAP) family.&lt;/li&gt;
  &lt;li&gt;Hippocalcin: Allowing Ca&lt;sup&gt;2+&lt;/sup&gt; to bind to interact with other proteins and regulate various processes, which belongs to the neuronal calcium sensor (NCS) family.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nerve&quot;&gt;Nerve&lt;/h2&gt;

&lt;p&gt;A &lt;strong&gt;nerve&lt;/strong&gt; is composed of nerve fibres that bring the sensation signals from the body to the CNS (afferent) and that send the innervation signals from the CNS to the body (efferent), and neuroglia that wraps nerve fibres. There are two large classifications.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Spinal nerve&lt;/strong&gt;: Branching out from the spinal cord, so it contains &lt;em&gt;general nerve fibres&lt;/em&gt;, such as GSA, GVA, GSE, and GVE nerve fibres. There are 31 pairs of spinal nerves in the human.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cranial nerve&lt;/strong&gt;: Branching out from the brain, so it contains &lt;em&gt;special nerve fibres&lt;/em&gt;, such as SSA, SVA, and SVE nerve fibres. There are 12 pairs of cranial nerves in the human.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that this classification is little vague, where some describe them as tissue while others describe them as peripheral organ, but the general idea is that any form of a bundle of axons in the PNS is considered as nerve.&lt;/p&gt;

&lt;p&gt;A spinal nerve can be anatomically divided as:&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 600px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_1/3_3_nerve_anatomy.jpg&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        A nerve anatomy from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:1319_Nerve_StructureN.jpg&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Nerve fascicle&lt;/strong&gt;: A bundle of axons filled with &lt;strong&gt;endoneurium&lt;/strong&gt; and enclosed by &lt;strong&gt;perineurium&lt;/strong&gt;. The term fascicle is also used for muscles, for instance, muscle fascicle.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fasciculus&lt;/strong&gt;: A bundle of fascicles with blood vessel and &lt;strong&gt;epineurium&lt;/strong&gt;. The spinal nerve is a fasciculus outreaching from the spinal cord.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The anatomy of nerve differ extremely per location, i.e. in cranium or in the CNS, but nerve fascicle and fasciculus are generally present in all of them.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In the PNS, a nerve consists of a combination of nerve fibres of different directions, afferent or efferent, and different functions, somatic or visceral, with or without myelinations of Schwann cells.&lt;/li&gt;
  &lt;li&gt;While nerves in the PNS are fairly well-known, white matters in the CNS are still grey area. However, at least, we know that most of them are wrapped with myelinations of oligodendrocytes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ganglion&quot;&gt;Ganglion&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;ganglion&lt;/strong&gt; is composed of dendrites, neuronal cell bodies, axon terminals and neuroglia. There are four large classifications.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dorsal root ganglion&lt;/strong&gt;, or spinal ganglion: The cell bodies of GSA and GVA, located next to spinal cord. There is not neurotransmissions inside the dorsal root ganglia. For each spinal nerve, dorsal root ganglion is held next to the spinal cord.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cranial nerve ganglion&lt;/strong&gt;: The cell bodies of SSA, SVA and SVE, located near the brain. Some cranial nerve ganglia contain synapses for neurotransmissions and some not. Not every cranial nerve contains cranial nerve ganglion. Some cranial nerves have their cell bodies inside the brain.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Autonomic ganglion&lt;/strong&gt;: The cell bodies of GVA and GVE, located in near the spinal cord or peripheral organs. Many autonomic ganglia contain synapses for neurotransmissions. Often autonomic ganglions lie near the organs that they are responsible of.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Ventral ganglion&lt;/strong&gt;: The cell bodies of GSE nerve, located in the ventral horn inside spinal cord. The cell bodies of the GSE are not part of the PNS. They reside inside the spinal cord, so often ventral ganglion refers to &lt;strong&gt;ventral nucleus&lt;/strong&gt; instead.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some ganglia act as a synaptic relay station between neurons since they contain presynaptic axon terminals and postsynaptic dendrites. The information enters to the ganglia from presynaptic neurons, excites the postsynaptic neurons in the ganglia and then exits through the nerve fibres of postsynaptic neurons. However, this is not the case for every ganglion. For instance, the cell bodies of GVA are located in either dorsal root ganglion or autonomic ganglion. This is why some autonomic functions are viable without the CNS.&lt;/p&gt;

&lt;p&gt;Not all of these four ganglia are part of the PNS.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dorsal root ganglion, cranial nerve ganglion and autonomic ganglion belong to the PNS.&lt;/li&gt;
  &lt;li&gt;Ventral ganglion belongs to the CNS. A ganglion is often referred to as nucleus in the CNS, i.e. motor nucleus as ventral ganglion.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The main differences between ganglion and grey matter is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The neuroglia in ganglion are mostly satellite cells.&lt;/li&gt;
  &lt;li&gt;The neuroglia in grey matter are astrocytes and oligodendrocytes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;
&lt;p&gt;Neuron&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Neuron is anatomically divided as dendrite, cell body, axon and axon terminal&lt;/li&gt;
  &lt;li&gt;Neuron types vary morphologically, unipolar &amp;amp; bipolar &amp;amp; pseudounipolar &amp;amp; multipolar &amp;amp; pyramidal &amp;amp; Purkinje&lt;/li&gt;
  &lt;li&gt;Neuron types vary functionally, sensory neuron &amp;amp; interneuron &amp;amp; motor neuron&lt;/li&gt;
  &lt;li&gt;A neurotransmission is a chemical process that transfers neurotransmitters from the presynaptic neuron and the postsynaptic neuron for action potential propagations
    &lt;ul&gt;
      &lt;li&gt;The presynaptic neuron releases neurotransmitters and the postsynaptic neuron takes the neurotransmitters&lt;/li&gt;
      &lt;li&gt;Neurotransmitters are commonly glutamate for excitation and GABA for inhibition&lt;/li&gt;
      &lt;li&gt;A neurotransmission either initiate or inhibit action potential propagation in the postsynaptic neuron by neurotransmitters opening the ion channels, allowing ion influx&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;An action potential propagation is a electrical process that propagates action potential from the dendrites to the axon terminals for neurotransmissions
    &lt;ul&gt;
      &lt;li&gt;Action potential is a phenomenon that membrane potential rises and falls, from depolarization, repolarization to hyperpolarization&lt;/li&gt;
      &lt;li&gt;The changes of membrane potential is due to voltage-gated ion channels, which allows the ions to pass when the membrane potential reaches a particular voltage&lt;/li&gt;
      &lt;li&gt;Saltatory conduction is a phenomenon that an action potential propagation is speed up with myelincation since voltage-gated ion channels only appear on nodes of Ranvier&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nerve Fibre&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Nerve fibre types vary by location (general and special), function (somatic and autonomic) and direction (afferent and efferent)&lt;/li&gt;
  &lt;li&gt;An afferent nerve fibre is part of a sensory neuron
    &lt;ul&gt;
      &lt;li&gt;An afferent neuron is a synonym for sensory neuron&lt;/li&gt;
      &lt;li&gt;Afferent nerve fibre types are general somatic Afferent (GSA), General Visceral Afferent (GVA), Special Somatic Afferent (SSA) and Special Visceral Afferent (SVA)&lt;/li&gt;
      &lt;li&gt;Sensations are classified based on where the stimuli arise, exteroception and interoception&lt;/li&gt;
      &lt;li&gt;Sensations are classified based on the types of stimuli, chemosensation, mechanosensation, thermalsensation and photosensation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;An efferent Nerve Fibre is part of a motor neuron
    &lt;ul&gt;
      &lt;li&gt;An efferent neuron is a synonym for motor neuron&lt;/li&gt;
      &lt;li&gt;Efferent nerve fibre types are General Somatic Efferent (GSE), General Visceral Efferent (GVE) and Special Visceral Efferent (SVE)&lt;/li&gt;
      &lt;li&gt;Motor neuron types vary by Location (Upper and Lower) and Function (Somatic and Visceral)&lt;/li&gt;
      &lt;li&gt;Muscle Contraction is a result of neuromuscular transmission between motor neuron and muscle cell&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Neural Tissue&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Neural tissue is formed by neurons, neuroglia and extracellular matrix&lt;/li&gt;
  &lt;li&gt;A bundle of axons form nerve in the PNS or white matter in the CNS&lt;/li&gt;
  &lt;li&gt;A cluster of dendrites, cell bodies, axon terminals and neuroglia form ganglion in the PNS or gray matter in the CNS&lt;/li&gt;
  &lt;li&gt;Neuroglia
    &lt;ul&gt;
      &lt;li&gt;Neuroglia types are ependymal cell, astrocyte, oligodendrocyte, microglia, satellite cell and Schwann cell&lt;/li&gt;
      &lt;li&gt;Extracellular matrix is formed by neuron, neuroglia and proteins&lt;/li&gt;
      &lt;li&gt;Some unique and important proteins found in the neural tissue are neurotrophin, neuronal apoptosis inhibitory protein (NAIP) and hippocalcin&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Nerve
    &lt;ul&gt;
      &lt;li&gt;Equivalent to white matter in the CNS&lt;/li&gt;
      &lt;li&gt;Any form of a bundle of axons in the PNS is considered as nerve&lt;/li&gt;
      &lt;li&gt;Generally, nerve contains nerve fascicle and fasciculus&lt;/li&gt;
      &lt;li&gt;Spinal nerve and cranial nerve are used to describe nerve types, but often they are grouped as peripheral nerve and classified as an organ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Ganglion
    &lt;ul&gt;
      &lt;li&gt;Equivalent to gray matter in the CNS&lt;/li&gt;
      &lt;li&gt;Ganglion types are dorsal root ganglion, containing GSA and GVA, cranial nerve ganglion, containing SSA, SVA and SVE, autonomic ganglion, containing GVA and GVE, and ventral ganglion, containing GSE&lt;/li&gt;
      &lt;li&gt;Dorsal root ganglion, cranial nerve ganglion and autonomic ganglion belong to the PNS while ventral ganglion belongs to the CNS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;overview-of-somatic-process&quot;&gt;Overview of Somatic Process&lt;/h2&gt;
&lt;p&gt;The generic description of somatic process is as follows.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 850px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_1/4_information_flow.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Information flow in the nervous system from
        &lt;a href=&quot;https://open.oregonstate.education/aandp/chapter/12-3-the-function-of-nervous-tissue/&quot;&gt;
            Oregon State University
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Well, the figure is self-explanatory, but just to point out:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sensory neurons detect stimuli from sensory receptors.&lt;/li&gt;
  &lt;li&gt;Sensation signals from stimuli travel to the dorsal root ganglion through the afferent nerve fibres of the sensory neurons.&lt;/li&gt;
  &lt;li&gt;Sensation signals is transferred to the neurons of sensory pathways to travel to the brain through the spinal cord.&lt;/li&gt;
  &lt;li&gt;The brain processes the signals with interneurons and generates innervation signals.&lt;/li&gt;
  &lt;li&gt;The brain sends innervation signals to the spinal cord through upper motor neurons.&lt;/li&gt;
  &lt;li&gt;Innervation signals are sent from the spinal cord to the neuromuscular junctions through efferent nerve fibres of the lower motor neurons.&lt;/li&gt;
  &lt;li&gt;Neuromuscular transmission occurs at the neuromuscular junctions, resulting in muscle contraction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other processes follow the same procedures, but just involve different nerve and ganglion, and perhaps additional components.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://en.m.wikipedia.org/wiki/Glia&lt;/li&gt;
&lt;/ul&gt;</content><author><name>D. K. Ryu</name></author><category term="Neuroscience&amp;#x003a; From Cells to Systems" /><summary type="html">In the field of neuroscience, there are specific biological components that play unique roles. A neuron acts as an electrical device that sends a signal by inlet and outlet of charged ions through ion channels on the membrane. Together with neuroglia, they make up neural tissues that transfer information or process information. This post aims to provide a tutorial on the two lower levels of body organization in neuroscience, namely the cell level and the tissue level.</summary></entry><entry><title type="html">Spinal Cord, Brain and Nervous System</title><link href="http://localhost:4000/posts/neuroscience/spinal_cord_brain_and_nervous_system/" rel="alternate" type="text/html" title="Spinal Cord, Brain and Nervous System" /><published>2023-04-10T00:00:00+09:00</published><updated>2023-04-15T00:00:00+09:00</updated><id>http://localhost:4000/posts/neuroscience/from_cells_to_systems_2_spinal_cord_brain_and_nervous_system</id><content type="html" xml:base="http://localhost:4000/posts/neuroscience/spinal_cord_brain_and_nervous_system/">&lt;blockquote&gt;
    Perception is reality.
    &lt;br /&gt;
    &lt;cite&gt;Lee Atwater&lt;/cite&gt;
&lt;/blockquote&gt;

&lt;p&gt;Perception is &lt;em&gt;not true reality&lt;/em&gt;, but at least, it is &lt;em&gt;our reality&lt;/em&gt;. We have no mean to apprehend the true reality, but only to merely estimate the reality from the observation in a limited and distorted way using &lt;em&gt;our brain&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Neuroscience&lt;/strong&gt; is the study of the human brain and nervous system, which includes 1) sensing the world, 2) feeling the emotion, 3) forming the memory, 4) processing the logic, and 5) moving the muscle. It is said that if the body is the hardware, the nervous system is the software. Neuroscience combines knowledge from various disciplines to explore the complexity of the human brain and nervous system.&lt;/p&gt;

&lt;p&gt;In the field of neuroscience, there are specific biological components that play unique roles. Neurons are specialized cells dedicated to transmitting and processing information. They form neural tissues, which operate collectively to manage information. These tissues assemble to manage information in clusters and contribute to the formation of the spinal cord and brain, which serve as specialized organs in the nervous system.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; &lt;b&gt;Cell&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 500px&quot;&gt;
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Neuron&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Neuroglia&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; &lt;b&gt;Tissue&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 500px&quot;&gt;
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Nerve&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Ganglion&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;White Matter&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Gray Matter&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; &lt;b&gt;Organ&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 500px&quot;&gt;
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Peripheral Nerve&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Brain&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Spinal Cord&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; &lt;b&gt;Organ System&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 500px&quot;&gt;
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Peripheral Nervous System&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Central Nervous System&lt;/a&gt;,
            &lt;br /&gt;
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Somatic Nervous System&lt;/a&gt;,
            &lt;a href=&quot;/posts/neuroscience/neuron_and_neural_tissue&quot;&gt;Autonomic Nervous System&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The brain serves as the primary organ for processing information within the human body while the spinal cord facilitates the transmission of information to and from the brain. Along with the peripheral nerves that branch out to the body, they comprise the nervous system. This post aims to provide a tutorial on the two higher levels of body organization in neuroscience, namely the organ level and the organ system level.&lt;/p&gt;

&lt;h1 id=&quot;anatomical-division-in-nervous-system&quot;&gt;Anatomical Division in Nervous System&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;nervous system&lt;/strong&gt; is a type of organ system, responsible for sensation, integration and response. It can be anatomically decomposed into the peripheral nervous system (PNS) and the central nervous system (CNS). The PNS senses the world and sends the status to the CNS and the CNS interprets the signals and sends the commands to the PNS to innervate organs. From robotics point-of-view, if muscles are the actuators, the PNS can be seen as sensors and wires, while the CNS as the central processing unit.&lt;/p&gt;

&lt;h2 id=&quot;peripheral-nervous-system&quot;&gt;Peripheral Nervous System&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;peripheral nervous system (PNS)&lt;/strong&gt; transmits information from sensation to the central nervous system and the central nervous system to movement.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The PNS is divided into &lt;em&gt;12 pairs of cranial nerves from the brain&lt;/em&gt; and &lt;em&gt;31 pairs of spinal nerves from the spinal cord&lt;/em&gt; that reach out to the entire body.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There is very vague definition of an organ in the PNS, so I will group spinal nerves and cranial nerves as peripheral nerves and classify them as organs in the PNS, so there are total 43 organs in the PNS.&lt;/p&gt;

    &lt;figure class=&quot;align-center&quot; style=&quot;width: 450px&quot;&gt;
      &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/0_0_spinal_nerve.png&quot; /&gt;
      &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
          A spinal nerve from
          &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Spinal_nerve.svg&quot;&gt;
              Wikimedia Commons
          &lt;/a&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;

    &lt;ul&gt;
      &lt;li&gt;Dorsal root and ventral root form a spinal nerve, then branch out to dorsal ramus and ventral ramus.&lt;/li&gt;
      &lt;li&gt;White ramus and gray ramus are responsible for visceral nerves in one spinal nerve to communicate with visceral nerves in neighbouring spinal nerves.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Each peripheral nerve consists of nerves and ganglia. It contains nerve fibres responsible for somatic and autonomic as well as afferent and efferent. Thus, it manages one or more functions of the body.&lt;/li&gt;
  &lt;li&gt;The PNS is not enclosed by bone, so it is susceptible to trauma, such as mechanical injuries and toxins, and thus, it easily disconnects from the CNS, resulting in a loss of body function.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;central-nervous-system&quot;&gt;Central Nervous System&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;central nervous system (CNS)&lt;/strong&gt; processes every information between sensation and movement.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The CNS is composed of two organs, &lt;em&gt;the spinal cord inside the spine&lt;/em&gt; and &lt;em&gt;the brain inside the head&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Both the spinal cord and the brain consist of white matter and gray matter, and are the processing centre of the body. They are the only organs in the CNS.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 450px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/0_1_nervous_system.jpg&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        The nervous system from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:1201_Overview_of_Nervous_System.jpg&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Unlike the PNS, the CNS is physically protected by the vertebrae and skull. There is also a transparent fluid that circulates the CNS, called &lt;strong&gt;cerebrospinal fluid (CSF)&lt;/strong&gt;, which serves as a shock absorber and a waste cleaner.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;There are two physical characteristics on the surface of the CNS.&lt;/p&gt;

    &lt;figure class=&quot;align-center&quot; style=&quot;width: 300px&quot;&gt;
      &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/0_2_gyrus_sulcus.png&quot; /&gt;
      &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
          A gyrus and sulcus from
          &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Gyrus_sulcus.png&quot;&gt;
              Wikimedia Commons
          &lt;/a&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Gyrus&lt;/strong&gt; is a fold, ridge or bump and &lt;strong&gt;sulcus&lt;/strong&gt; is a depression or groove. Gyrus is generally surrounded by one or more sulci. Together, they create the folded appearance of the CNS. A larger sulcus is usually called &lt;strong&gt;fissure&lt;/strong&gt;, which is used to divide the CNS.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In neuroanatomy, there are few terms that appear frequently.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Anterior&lt;/em&gt; is derived from the Latin, &lt;em&gt;towards the front&lt;/em&gt;, and &lt;em&gt;posterior&lt;/em&gt; is from the Latin, &lt;em&gt;coming after&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Dorsal&lt;/em&gt; is derived from the Latin, &lt;em&gt;back part of the body&lt;/em&gt;, and &lt;em&gt;ventral&lt;/em&gt; is from the Latin, &lt;em&gt;front part of the body&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Often the CNS uses anterior and posterior while the PNS uses dorsal and ventral.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Therefore, the PNS serves as the conjunction between the CNS and other body parts. The PNS senses the world and sends the status to the CNS via sensory nerves, and the CNS interprets the signals and sends the commands to the PNS to tell the body to move or to conduct resting via motor nerves.&lt;/p&gt;

&lt;h1 id=&quot;spinal-cord&quot;&gt;Spinal Cord&lt;/h1&gt;
&lt;p&gt;The &lt;strong&gt;spinal cord&lt;/strong&gt; is a long tube-like band organ in the CNS, located inside the spine. It receives sensation signals from the PNS, communicates with the brain, and sends innervation signals back to the PNS.&lt;/p&gt;

&lt;p&gt;Most of innervation signals are generated by the brain, but there are two types of motor outputs that the spinal cord is responsible of:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Somatic Reflex&lt;/strong&gt;: Automatic, involuntary responses to sensations, such as knee-jerk reflex and withdrawal reflex. This involves a &lt;strong&gt;reflex arc&lt;/strong&gt;, a circuit made up of &lt;strong&gt;spinal interneurons&lt;/strong&gt;. This is how we maintain the posture with disturbance from external world, like wind or vibration.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Rhythmic Movement&lt;/strong&gt;: A periodic process that the body repeatedly returns to its starting condition in the absence of rhythmic input, such as walking, swimming, breathing, or chewing. This involves a &lt;strong&gt;central pattern generator&lt;/strong&gt;, a circuit made up of spinal interneurons. This is how we can breath, walk and talk at the same time.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The spinal cord is protected by the spine, which is composed of the &lt;strong&gt;vertebrae&lt;/strong&gt;, or vertebral column, or backbone.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 300px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/1_1_spinal_cord_spine.jpg&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        The spinal cord with spine from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Spinewithcord.jpg&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;In the spinal cord, the grey matter is in butterfly-shape and the white matter covers the grey matter, so the processing is done at the inner layer of the spinal cord and is sent to either the brain or the spinal nerves at the outer layer.&lt;/li&gt;
  &lt;li&gt;Between two vertebrae, an intervertebral disk is present. Nerve roots are the nerves outreaching from the CNE, where for the spine, they are either dorsal roots or ventral roots.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tract&quot;&gt;Tract&lt;/h2&gt;
&lt;p&gt;The white matter in the spinal cord is sectioned as &lt;strong&gt;tracts&lt;/strong&gt;, or pathways.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 500px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/1_2_spinal_tract.jpeg&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        The spinal tracts from
        &lt;a href=&quot;https://open.oregonstate.education/aandp/chapter/14-5-sensory-and-motor-pathways/&quot;&gt;
            Figure 14.5.5 in Anatomy &amp;amp; Physiology
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Afferent pathways
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Dorsal column medial lemniscus tract&lt;/strong&gt;: Responsible for fine touch and conscious proprioception.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Spinothalamic tract&lt;/strong&gt;: Responsible for crude touch, unconscious proprioception, pain and temperature.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Efferent pathways
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Lateral corticospinal tract&lt;/strong&gt;: Responsible for the movement of the limbs&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Anterior corticospinal tract&lt;/strong&gt;: Responsible for the movement of the axial muscles of the trunk&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Where fine touch and crude touch are the two skin sensations.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Fine touch: Touch sensation that can localize where the stimulus is applied.&lt;/li&gt;
  &lt;li&gt;Crude touch: Touch sensation that cannot localize where the stimulus is applied.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;horizontal-view&quot;&gt;Horizontal View&lt;/h2&gt;
&lt;p&gt;The horizontal sectional view of the spinal cord is:&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 500px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/1_3_horizontal_spinal_cord.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        The horizontal sectional view of the spinal cord from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Spinal_Cord_Sectional_Anatomy.png&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Spinal cord is bilaterally symmetrical, where the axis of symmetry is between &lt;strong&gt;anterior median fissure&lt;/strong&gt; and &lt;strong&gt;posterior median sulcus&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Posterior gray commissure&lt;/strong&gt;, &lt;strong&gt;anterior gray commissure&lt;/strong&gt; and &lt;strong&gt;anterior white commissure&lt;/strong&gt; are the connection between left side and right side of the spinal cord.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dorsal root&lt;/strong&gt; is a collection of afferent nerve fibres and &lt;strong&gt;ventral root&lt;/strong&gt; is of efferent nerve fibres. They combine to form the spinal nerve.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dorsal root ganglion&lt;/strong&gt; is the ganglion that contains the cell bodies of dorsal root, that includes GSA and GVA.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Posterior gray horn&lt;/strong&gt;, &lt;strong&gt;lateral gray horn&lt;/strong&gt; and &lt;strong&gt;anterior gray horn&lt;/strong&gt;  encapsulate &lt;strong&gt;sensory nuclei&lt;/strong&gt; and &lt;strong&gt;motor nuclei&lt;/strong&gt;. Motor nuclei is the ganglion of ventral root, or ventral ganglion.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Posterior white column&lt;/strong&gt;, &lt;strong&gt;lateral white column&lt;/strong&gt; and &lt;strong&gt;anterior white column&lt;/strong&gt; encapsulate tracts that communicate with the brain and other body parts.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vertical-view&quot;&gt;Vertical View&lt;/h2&gt;
&lt;p&gt;The vertial sectional view of the spinal cord is:&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 750px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/1_4_vertical_spinal_cord.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        The vertical sectional view of the spinal cord from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Spinal_Cord_Segments_and_body_representation.png&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There are 31 pairs of spinal nerves branching out from the spinal cord. They are named after the regions of vertebrae:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;8 cervical nerves (C1-C8) in 7 cervical vertebrae (C1-C7) for the neck, arms, eyes and glands&lt;/li&gt;
  &lt;li&gt;12 thoracic nerves (T1-T12) in 12 thoracic vertebrae (T1-T12) for the torso, heart, lung and digestive organs&lt;/li&gt;
  &lt;li&gt;5 lumbar nerves (L1-L5) in 5 lumbar vertebrae (L1-L5) for front legs, rectum and urinary organs&lt;/li&gt;
  &lt;li&gt;5 sacral nerves (S1-S5) in 5 sacral vertebrae (S1-S5) for behind legs, rectum and urinary organs&lt;/li&gt;
  &lt;li&gt;1 coccygeal nerve (Co) in 3 to 5 coccygeal vertebrae (Co1-Co5) for tailbone&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To clarify, the number of nerves is different from the number of vertebrae.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;One more nerve pair exists than the vertebrae in cervical.&lt;/li&gt;
  &lt;li&gt;There exists only a single nerve pair on coccygeal while the number of the coccygeal vertebrae varies by person.
    &lt;ul&gt;
      &lt;li&gt;Coccyx is the tailbone that is reduced during evolution in humans.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;brain&quot;&gt;Brain&lt;/h1&gt;

&lt;p&gt;The &lt;strong&gt;brain&lt;/strong&gt; is a sphere-shaped wrinkly organ in the CNS, located inside the head. It receives sensation signals from the spinal cord, processes them through different regions of the brain, and sends innervation signals back to the spinal cord. The brain is the most complex system in the human body, and some even say &lt;em&gt;the most complex system we have discovered in our universe&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The brain is protected physically by the cranium and chemically by the blood-brain barrier.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Cranium&lt;/strong&gt; is the bones enclosing the brain, excluding the bones of the face and jaw.&lt;/p&gt;

    &lt;figure class=&quot;align-center&quot; style=&quot;width: 200px&quot;&gt;
      &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_0_cranium.gif&quot; /&gt;
      &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
          The cranium from
          &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Neurocranium_-_superior_view_-_animation02.gif&quot;&gt;
              Wikimedia Commons
          &lt;/a&gt;
      &lt;/figcaption&gt;
  &lt;/figure&gt;

    &lt;ul&gt;
      &lt;li&gt;There are total six types of cranium bones, frontal bone (yellow), parietal bone (blue), sphenoid bone (purple), temporal bone (orange), occipital bone (green) and ethmoid bone (red).&lt;/li&gt;
      &lt;li&gt;There are total eight cranium bones, frontal bone (1), parietal bone (2), sphenoid bone (1), temporal bone (2), occipital bone (1) and ethmoid bone (1). Each bone bonds with thick connective tissues, called sutures.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Blood-brain barrier (BBB)&lt;/strong&gt; is semipermeable barrier the allows the passage of some small moleclues, including ions and amino acids, while restricting the passage of pathogens.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the brain, grey matter covers white matter, which means the processing is done at the outer layer of the brain and is exchanged between different regions of the brain.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 250px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_0_white_gray_brain.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        The white matter (yellow) and grey matter (red) inside the brain from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Human_cerebral_cortex.png&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;If more gyrus and sulcus are present, this increases the surface area of the brain, resulting in more gray matter. What this means is that the brain with more crumpled surface has higher capacity to process information.&lt;/p&gt;

&lt;h2 id=&quot;brain-division&quot;&gt;Brain Division&lt;/h2&gt;
&lt;p&gt;The brain is divided into two &lt;strong&gt;cerebral hemispheres&lt;/strong&gt; by a &lt;strong&gt;longitudinal fissure&lt;/strong&gt;, or cerebral fissure or interhemispheric fissure. Each cerebral hemisphere is known to be responsible for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Left cerebral hemisphere: Logics, facts, planning, analysis and right side of the body.&lt;/li&gt;
  &lt;li&gt;Right cerebral hemisphere: Arts, emotions, creativity, imagination and left side of the body.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They are connected by the &lt;strong&gt;corpus callosum&lt;/strong&gt;.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 250px&quot;&gt;
        Left brain and right brain from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Cerebral_hemisphere_-_animation.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 250px&quot;&gt;
        Corpus callosum from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Corpus_callosum.png&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 250px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_1_left_right_brain.gif&quot; width=&quot;150px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 250px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_1_corpus_callosum.gif&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The brain can be divided into several ways. The simplest way is:&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Cerebrum from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Cerebrum_animation_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Interbrain from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Diencephalon_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Brainstem from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Brainstem_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Cerebellum from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Cerebellum_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_2_cerebrum.gif&quot; width=&quot;125px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_2_interbrain.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_2_brainstem.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_2_cerebellum.gif&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Cerebrum&lt;/strong&gt;: is composed of cerebral cortex, hippocampus and amygdala. It is the largest part of the brain that forms the hemispheric structure. Cerebrum means &lt;em&gt;brain&lt;/em&gt; in Latin.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Interbrain&lt;/strong&gt;: is composed of thalamus, hypothalamus and pituitary gland. It reside under the cerebrum and usually works with the cerebrum.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Brainstem&lt;/strong&gt;: is composed of midbrain, pons and medulla. It connects cerebrum, interbrain and spinal cord and serves as relay station.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cerebellum&lt;/strong&gt;: is usually considered as a standalone brain region. It is located at the back of the head, just behind the brainstem. Cerebellum means &lt;em&gt;little brain&lt;/em&gt; in Latin.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are other divisions commonly seen in literature:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Three embryonic brain division:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Forebrain&lt;/strong&gt;: Cerebrum and interbrain.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Midbrain&lt;/strong&gt;: Midbrain.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Hindbrain&lt;/strong&gt;: Pons, medulla, and cerebellum.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Five embryonic brain division
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Telencephalon&lt;/strong&gt;: Synonym for cerebrum.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Diencephalon&lt;/strong&gt;: Synonym for interbrain.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Mesencephalon&lt;/strong&gt;: Synonym for midbrain.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Metencephalon&lt;/strong&gt;: Pons and cerebellum.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Myelencephalon&lt;/strong&gt;: Medulla.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I will stick with the first division,&lt;/p&gt;

&lt;h2 id=&quot;cerebrum&quot;&gt;Cerebrum&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Cerebral cortex&lt;/strong&gt;, or cerebral mantle, in the cerebrum can be further decomposed into:&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Frontal lobe from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Frontal_lobe_animation.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Parietal lobe from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Parietal_lobe_animation_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Temporal lobe from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Temporal_lobe_animation.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Occipital lobe from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Occipital_lobe_animation_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_3_frontal_lobe.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_3_parietal_lobe.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_3_temporal_lobe.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_3_occipital_lobe.gif&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Their functions vary by locations.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 600px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_3_cerebral_cortex_function.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Functions of the cerebral cortex from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Blausen_0102_Brain_Motor%26Sensory.png&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Frontal: Action processing&lt;/li&gt;
  &lt;li&gt;Parietal: Sensory processing&lt;/li&gt;
  &lt;li&gt;Occipital: Visual Processing&lt;/li&gt;
  &lt;li&gt;Temporal: Visual and auditory processing&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;interbrain&quot;&gt;Interbrain&lt;/h2&gt;

&lt;p&gt;The interbrain and other parts of the cerebrum are knwon to work together for the &lt;strong&gt;limbic system&lt;/strong&gt;, which handles emotion, behavior, memory and autonomic responses.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Thalamus from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Thalamus_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Hypothalamus from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Hypothalamus_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Hippocampus from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Hippocampus_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_4_thalamus.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_4_hypothalamus.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_4_hippocampus.gif&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Amygdala from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Amygdala_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Pituitary gland from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Pituitary_gland_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Striatum from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Striatum.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_4_amygdala.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_4_pituitary_gland.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_4_striatum.gif&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Thalamus: Relay station&lt;/li&gt;
  &lt;li&gt;Hypothalamus: Gland regulation&lt;/li&gt;
  &lt;li&gt;Pituitary gland: Hormone production&lt;/li&gt;
  &lt;li&gt;Hippocampus: Learning and Memory&lt;/li&gt;
  &lt;li&gt;Amygdala: Emotion&lt;/li&gt;
  &lt;li&gt;Striatum: Reward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that pituitary gland is not part of the limbic system, but it is controlled by the limbic system.&lt;/p&gt;

&lt;p&gt;These mostly work together to serve a function. For instance:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Hypothalamus tells pituitary gland to produce hormones, then the hormones will communicate with the body.&lt;/li&gt;
  &lt;li&gt;Amygdala modulates memory consolidation for emotional memories with hippocampus.&lt;/li&gt;
  &lt;li&gt;Thalamus and hypothalamus work together in sleeping mechanism.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;brainstem&quot;&gt;Brainstem&lt;/h2&gt;

&lt;p&gt;Brainstem is the relay station between the cranial nerves, the brain and the spinal cord. The cranial nerves are divided as:&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 350px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_5_cranial_nerve.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Cranial nerves from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Brain_human_normal_inferior_view_with_labels_en-2.svg&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;CN I: Olfactory nerve for smell&lt;/li&gt;
  &lt;li&gt;CN II: Optic nerve for vision&lt;/li&gt;
  &lt;li&gt;CN III: Oculomotor nerve for eye movement&lt;/li&gt;
  &lt;li&gt;CN IV: Trochlear nerve for eye movement (look down and inwards)&lt;/li&gt;
  &lt;li&gt;CN V: Trigeminal nerve for sensation on the face skin and chewing muscle&lt;/li&gt;
  &lt;li&gt;CN VI: Abducens nerve for eye movement (outwards)&lt;/li&gt;
  &lt;li&gt;CN VII: Facial nerve for facial expression and tase sensation&lt;/li&gt;
  &lt;li&gt;CN VIII: Vestibulocochlear nerve for hearing and balance&lt;/li&gt;
  &lt;li&gt;CN IX: Glossopharyngeal nerve for oral sensation, taste sensation and salivation&lt;/li&gt;
  &lt;li&gt;CN X: Vagus nerve for autonomic system&lt;/li&gt;
  &lt;li&gt;CN XI: Accessory nerve for shoulder and head movement&lt;/li&gt;
  &lt;li&gt;CN XII: Hypoglossal nerve for tongue movement&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similar to the spinal nerves, cranial nerves are considered as the &lt;em&gt;organs&lt;/em&gt; in the PNS.&lt;/p&gt;

&lt;p&gt;Different regions of the brainstem facilitates different types of cranial nerves, so the functions vary by the cranial nerves.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Midbrain from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Midbrain_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Pons from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Pons_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Medulla from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Medulla_oblongata_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_5_midbrain.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_5_pons.gif&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_5_medulla.gif&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Midbrain&lt;/strong&gt;: CN III and CN IV&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pons&lt;/strong&gt;: CN V, CN VI, CN VII and CN VIII&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mendulla&lt;/strong&gt;: CN IX, CN X, CN XI and CN XII&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The last two cranial nerves are attached to the other brain regions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Olfactory cortex in the temporal lobe: CN I&lt;/li&gt;
  &lt;li&gt;Lateral geniculate nucleus in thalamus: CN II&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cerebellum&quot;&gt;Cerebellum&lt;/h2&gt;

&lt;p&gt;Cerebellum is involved in a variety of functions, including coordinating movement, motor learning and language. Despite its size, as name suggests, &lt;em&gt;little brain&lt;/em&gt;, it holds more than half of neurons in the brain in a very densely compacted manner.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            Cerebellum from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Cerebellum_small.gif&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/2_6_cerebellum.gif&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Therefore, the brain does most of the heavy jobs while the spinal cord generally acts as a pathway to pass messages between the brain and the PNS.&lt;/p&gt;

&lt;h1 id=&quot;functional-division-in-nervous-system&quot;&gt;Functional Division in Nervous System&lt;/h1&gt;
&lt;p&gt;The nervous system can be anatomically decomposed into the PNS and the CNS, but they work together to serve a variety of body functions, which can be largely divided into the somatic nervous system (SNS) and the autonomic nervous system (ANS). These two nervous systems are often considered as sub-divisions of the PNS, but in this section, I explain them with both the PNS and the CNS.&lt;/p&gt;

&lt;h2 id=&quot;somatic-nervous-system&quot;&gt;Somatic Nervous System&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;somatic nervous system (SNS)&lt;/strong&gt;, or voluntary nervous system, produces the voluntary movements from sensation, such as walking or lifting.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It takes five sensation signals, that are sight, sound, smell, taste and touch, to the CNS through afferent nerves.&lt;/li&gt;
  &lt;li&gt;It sends innervation signal from the CNS to the skeletal muscles through efferent nerves.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;How the commands are sent to different body parts vary by the location and the function.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Voluntary skeletal movements of the head, neck and tongue from the brain directly through the special visceral neurons.&lt;/li&gt;
  &lt;li&gt;Voluntary skeletal movements of the torso and limbs from the brain through the upper motor neurons and the lower motor neurons.&lt;/li&gt;
  &lt;li&gt;Involuntary skeletal movements of the body from the spinal cord through the spinal interneurons and the lower motor neurons.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The cerebral cortex processes voluntary skeletal movements, mapping from the sensation in the parietal lobe to the innervation in the frontal lobe.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 450px&quot;&gt;
            Sensory homunculus on the parietal lobe from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Sensory_Homunculus-en.svg&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 450px&quot;&gt;
            Motor homunculus on the frontal lobe from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Motor_homunculus.svg&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 450px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/3_1_sensory_homunculus.png&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 450px&quot;&gt;
            &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/3_1_motor_homunculus.png&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The signals pass differently based on the location.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The spinal nerves pass signals from the spinal cord to the brainstem.&lt;/li&gt;
  &lt;li&gt;The cranial nerves pass signals directly through the brainstem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, while the cerebral cortex does all the processing to integrate the sensation and the innervation, the brainstem acts as the pathway for receiving and sending signals for conscious voluntary skeletal movements.&lt;/p&gt;

&lt;p&gt;For involuntary skeletal movements, the spinal cord is responsible for all the processing without involving the brain, that includes receiving signals from the PNS, integrating the sensation and the innervation, and sending signals to the PNS.&lt;/p&gt;

&lt;p&gt;Furthermore, there are two divisions to map the SNS.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dermatome&lt;/strong&gt;: An area of skin that sensory neurons of a single spinal nerve is connected to.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 700px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/3_1_dermatome.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Dermatome from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Dermatomes_labeled,_female_front-back_3d-shaded.svg&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Myotome&lt;/strong&gt;: An area of muscle that motor neurons of a single spinal nerve is connected to.
    &lt;ul&gt;
      &lt;li&gt;C1, C2: Neck flexion and neck extension&lt;/li&gt;
      &lt;li&gt;C3: Lateral neck flexion&lt;/li&gt;
      &lt;li&gt;C4: Shoulder elevation&lt;/li&gt;
      &lt;li&gt;C5: Shoulder abduction&lt;/li&gt;
      &lt;li&gt;C6: Elbow flexion and wrist extension&lt;/li&gt;
      &lt;li&gt;C7: Elbow extension and wrist flexion&lt;/li&gt;
      &lt;li&gt;C8: Thumb extension&lt;/li&gt;
      &lt;li&gt;T1: Finger abduction and finger adduction&lt;/li&gt;
      &lt;li&gt;L1, L2: Hip flexion&lt;/li&gt;
      &lt;li&gt;L3: Knee extension&lt;/li&gt;
      &lt;li&gt;L4: Ankle dorsi-flexion&lt;/li&gt;
      &lt;li&gt;L5: Great toe extension&lt;/li&gt;
      &lt;li&gt;S1: Hip extension, ankle plantar-flexion and ankle eversion&lt;/li&gt;
      &lt;li&gt;S2: Knee flexion&lt;/li&gt;
      &lt;li&gt;S3, S4: Anal wink&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similarly, cranial nerves also manage some parts of body sensation and innervation.&lt;/p&gt;

&lt;h2 id=&quot;autonomic-nervous-system&quot;&gt;Autonomic Nervous System&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Autonomic nervous system (ANS)&lt;/strong&gt;, or visceral nervous system, controls the involuntary physiological processes of internal organs, such as cardiac function, respiration, digestion, sexual arousal, and other reflexes, including vomiting, coughing, and sneezing.&lt;/p&gt;

&lt;p&gt;Similar to the SNS, it passes sensations, such as visceral pain, to the CNS through afferent nerves, but it regulates involuntary physiologic processes from the hypothalamus to the PNS through efferent nerves. This is why hypothalamus is known as the centre of the ANS.&lt;/p&gt;

&lt;p&gt;The ANS can be functionally divided into two nervous systems of opposite role.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sympathetic nervous system (SympNS)&lt;/strong&gt; regulates the &lt;strong&gt;fight-or-flight&lt;/strong&gt; response, which is an acute response that takes place in case of an imminent harmful event or intense mental distress. It is responsible for the physiological events that prepare the body for self-defence through a fight or an escape. This activates the blood flow in skeletal muscles and lungs, dilates lungs and blood vessels, and raises the heart rate. Its ganglion is generally located next to the spinal cord.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Parasympathetic nervous system (PSympNS)&lt;/strong&gt; regulates the &lt;strong&gt;resting&lt;/strong&gt; response, which is basically the opposite response to the fight-or-flight. This slows down heart rate and digestion, and reduces salivation and lacrimation, with the only exception being sexual arousal. Its ganglion is generally located near peripheral organs that they innervate.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 550px&quot;&gt;
    &lt;img src=&quot;/assets/neuroscience/from_cells_to_systems_2/3_2_ans.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Diagram of sympathetic nervous system (red) and parasympathetic nervous system (blue) from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Gray839.png&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Both SympNS and PSympNS have excitatory and inhibitory functions.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The SympNS increases heart rate while the PSympNS decreases heart rate.&lt;/li&gt;
  &lt;li&gt;The SympNS decreases digestive processes while the PSympNS increases digestive processes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In recent years, some functions that manage the gut from SympNS and PSympNS are grouped as the &lt;strong&gt;enteric nervous system (ENS)&lt;/strong&gt;. It controls the digestive system (gastric acid secretion and gastrointestinal tract movement), hormones release and immune system in the gut. Its ganglion is located near digestion system, i.e. gut and intestine.&lt;/p&gt;

&lt;p&gt;Another categorization is based on the involvement of the CNS.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Both SympNS and PSympNS has to integrate the CNS, so they are considered as the &lt;strong&gt;extrinsic component&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;On the contrary, the ENS is capable of operating independently of the CNS. Due to this, the ENS is considered as the &lt;strong&gt;intrinsic component&lt;/strong&gt;, and sometimes referred as the &lt;em&gt;second brain&lt;/em&gt; or &lt;em&gt;the brain in the gut&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;Anatomical Division in Nervous System&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The nervous system is anatomically divided into the PNS and the CNS&lt;/li&gt;
  &lt;li&gt;The PNS transmits information from sensation to the CNS and the CNS to movement
    &lt;ul&gt;
      &lt;li&gt;The PNS is composed of 43 peripheral nerves, 12 pairs of cranial nerves from the brain and 31 pairs of spinal nerves from the spinal cord&lt;/li&gt;
      &lt;li&gt;Each peripheral nerve consists of nerves and ganglia, managing one or more functions of the body, i.e. somatic, autonomic, afferent and efferent&lt;/li&gt;
      &lt;li&gt;The PNS is not protected both physically and chemically, so it is susceptible to trauma&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The CNS processes every information between sensation and movement
    &lt;ul&gt;
      &lt;li&gt;The CNS is composed of two organs, the spinal cord inside the spine and the brain inside the head&lt;/li&gt;
      &lt;li&gt;Both the spinal cord and the brain consist of white matter and gray matter&lt;/li&gt;
      &lt;li&gt;Unlike the PNS, the CNS is physically protected by the vertebrae, the skull and the CSF&lt;/li&gt;
      &lt;li&gt;Two physical characteristics that appear on the surface of the CNS are gyrus and sulcus
        &lt;ul&gt;
          &lt;li&gt;A larger sulcus is usually called fissure, which is used to divide the CNS&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Spinal Cord&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The spinal cord is responsible for somatic reflex and rhythmic movement&lt;/li&gt;
  &lt;li&gt;The spinal cord is protected by vertebrae physically and by CSF chemically&lt;/li&gt;
  &lt;li&gt;In the spinal cord, the white matter covers the gray matter, forming butterfly shape&lt;/li&gt;
  &lt;li&gt;Tract, or pathway, is the white matter in the spinal cord that transfers signals received from the PNS to the brain
    &lt;ul&gt;
      &lt;li&gt;Afferent pathway includes dorsal column medial lemniscus tract and spinothalamic tract&lt;/li&gt;
      &lt;li&gt;Efferent pathway includes lateral corticospinal tract and anterior corticospinal tract&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The spinal cord can be bilaterally divided symmetrically by anterior median fissure and posterior median sulcus
    &lt;ul&gt;
      &lt;li&gt;The left side and the right side of the spinal cord are connected by posterior gray commissure, anterior gray commissure and anterior white commissure&lt;/li&gt;
      &lt;li&gt;The region of the white matter in the spinal cord can be divided by posterior white column, lateral white column and anterior white column&lt;/li&gt;
      &lt;li&gt;The region of the gray matter in the spinal cord can be divided by posterior gray horn, lateral gray horn and anterior gray horn&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Vertically, the spinal cord is divided by its neighbouring vertebrae
    &lt;ul&gt;
      &lt;li&gt;8 cervical nerves (C1-C8) in 7 cervical vertebrae (C1-C7) for the neck, arms, eyes and glands&lt;/li&gt;
      &lt;li&gt;12 thoracic nerves (T1-T12) in 12 thoracic vertebrae (T1-T12) for the torso, heart, lung and digestive organs&lt;/li&gt;
      &lt;li&gt;5 lumbar nerves (L1-L5) in 5 lumbar vertebrae (L1-L5) for front legs, rectum and urinary organs&lt;/li&gt;
      &lt;li&gt;5 sacral nerves (S1-S5) in 5 sacral vertebrae (S1-S5) for behind legs, rectum and urinary organs&lt;/li&gt;
      &lt;li&gt;1 coccygeal nerve (Co) in 3 to 5 coccygeal vertebrae (Co1-Co5) for tailbone&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Brain&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The brain is responsible for almost everything that is related to perception, cognition and consciouness&lt;/li&gt;
  &lt;li&gt;In the brain, the gray matter covers the white matter, which means denser the gyrus and sulcus are, more the gray matter&lt;/li&gt;
  &lt;li&gt;The brain is splitted the left and the right cerebral hemispheres with corpus callosum that connects the two cerebral hemispheres
    &lt;ul&gt;
      &lt;li&gt;The brain can be divided into cerebrum, interbrain, brainstem and cerebellum
        &lt;ul&gt;
          &lt;li&gt;Cerebrum is divided into the frontal lobe, the parietal lobe, the temporal lobe and the occipital lobe, responsible for action, sensory, visual and visual/auditory&lt;/li&gt;
          &lt;li&gt;Interbrain is divided into the thalamus, the hypothalamus, the pituitary gland, the hippocampus, the amygdala and the striatum, responsible for relay station, gland regulation, hormone production, learning and memory, emotion and reward, respectively&lt;/li&gt;
          &lt;li&gt;Brainstem is divided into the midbrain, the pons and the mendulla that facilitate CN III - IV, CN V - VIII and CN IX - XII, respectively
            &lt;ul&gt;
              &lt;li&gt;CN I - II are facilitated in other brain parts&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Cerebellum is responsible for coordinating movement, motor learning and language&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Functional Division in Nervous System&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The nervous system is functionally divided into the SNS and the ANS&lt;/li&gt;
  &lt;li&gt;The SNS is mainly responsible for the voluntary movements
    &lt;ul&gt;
      &lt;li&gt;It takes five sensation signals, that are sight, sound, smell, taste and touch, and sends innervation signal to the skeletal muscles.&lt;/li&gt;
      &lt;li&gt;Voluntary skeletal movements are generated from the brain
        &lt;ul&gt;
          &lt;li&gt;Sensory signals are processed on the parietal lobe of the brain whereas motor signals are processed on the frontal lobe&lt;/li&gt;
          &lt;li&gt;The spinal nerves pass signals from the spinal cord to the brainstem whereas the cranial nerves pass directly through the brainstem&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Involuntary skeletal movements are generated from the spinal cord&lt;/li&gt;
      &lt;li&gt;Dermatome is mapping the sensory parts of the SNS whereas myotome is mapping the motor parts&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The ANS is mainly responsible for the involuntary physiological processes
    &lt;ul&gt;
      &lt;li&gt;It takes visceral pain signals and other sensation signals, and regulates the physiological processes of the internal organs using the hypothalamus&lt;/li&gt;
      &lt;li&gt;The ANS is further divided into SympNS that regulates the fight-or-flight response, PSympNS that regulates the resting response, and ENS that regulates the digestive system&lt;/li&gt;
      &lt;li&gt;The SympNS and PSympNS are considered as the extrinsic component whereas the ENS is considered as the intrinsic component&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nervous-system-division&quot;&gt;Nervous System Division&lt;/h2&gt;

&lt;p&gt;Anatomically:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The central nervous system in the spine and head
    &lt;ul&gt;
      &lt;li&gt;The spinal cord and the brain
        &lt;ul&gt;
          &lt;li&gt;The white matter and gray matter&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The peripheral nervous system in the peripherals
    &lt;ul&gt;
      &lt;li&gt;The spinal nerves and the cranial nerves
        &lt;ul&gt;
          &lt;li&gt;The nerves and ganglia&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Functionally:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The somatic nervous system that controls the skeletal muscles
    &lt;ul&gt;
      &lt;li&gt;Dematome and sensory homunculus&lt;/li&gt;
      &lt;li&gt;Myotome and motor homunculus&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The autonomic nervous system that controls the smooth muscles and glands
    &lt;ul&gt;
      &lt;li&gt;The sympathetic nervous system&lt;/li&gt;
      &lt;li&gt;The parasympathetic nervous system&lt;/li&gt;
      &lt;li&gt;The enteric nervous system&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Most organs require all the nervous systems to be integrated. For instance, an eye is an organ that uses a mixture of visual tracking (SNS), pupil dilation (SympNS) and pupil constriction (PSympNS).&lt;/p&gt;</content><author><name>D. K. Ryu</name></author><category term="Neuroscience&amp;#x003a; From Cells to Systems" /><summary type="html">In the field of neuroscience, there are specific biological components that play unique roles. The brain serves as the primary organ for processing information within the human body while the spinal cord facilitates the transmission of information to and from the brain. Along with the peripheral nerves that branch out to the body, they comprise the nervous system. This post aims to provide a tutorial on the two higher levels of body organization in neuroscience, namely the organ level and the organ system level.</summary></entry><entry><title type="html">Value-based Reinforcement Learning</title><link href="http://localhost:4000/posts/machine_learning/decision_1_value_based_reinforcement_learning/" rel="alternate" type="text/html" title="Value-based Reinforcement Learning" /><published>2022-09-15T00:00:00+09:00</published><updated>2023-04-15T00:00:00+09:00</updated><id>http://localhost:4000/posts/machine_learning/decision_1_value_based_reinforcement_learning</id><content type="html" xml:base="http://localhost:4000/posts/machine_learning/decision_1_value_based_reinforcement_learning/">&lt;div class=&quot;notice--primary&quot;&gt;
  
&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/posts/mathematics/markov_model_1_markov_chain&quot;&gt;Markov Chain&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/posts/mathematics/markov_model_2_markov_decision_process&quot;&gt;Markov Decision Process&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;

&lt;blockquote&gt;
    Machine intelligence is the last invention that humanity will ever need to make.
    &lt;br /&gt;
    &lt;cite&gt;Nick Bostrom&lt;/cite&gt;
&lt;/blockquote&gt;

&lt;p&gt;If the universe is constructed with patterns and humans can interpret this, then so may machines. If machines become intelligent over humans, &lt;em&gt;humans need not apply&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; is the study of algorithms that involve training a computer to learn patterns and relationships in data without being explicitly programmed. It can be divided into three types:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Supervised Learning&lt;/strong&gt;: The algorithm, or model, is trained on labeled data, where the goal is to map each input to the correct output.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unsupervised Learning&lt;/strong&gt;: The algorithm, or model, is trained on unlabeled data, where the goal is to identify patterns or structure in the data without being told what to look for.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt;: The algorithm, or agent, is trained on the interactive worlds, where the goal is to learn a policy that maximizes rewards and minimize penalties.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The problems in ML can be largely divided by four types:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;: The problems require machines to interpret and understand visual data, i.e. images and videos. They includes image classification, object localization, video analysis, image captioning and visual question answering.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Natural Language Processing&lt;/strong&gt;: The problems require machines to interpret and understand human language, i.e. words and documents. They includes text classification, translation, summarization, speech analysis, question answering and dialogue system.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Graph Analysis&lt;/strong&gt;: The problems require machines to interpret and understand graph data, i.e. nodes and edges in graphs. They includes graph classification, link prediction, knowledge graph, social network analysis, recommendation system, combinatorial optimization and path finding.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Decision Making&lt;/strong&gt;: The problems require machines to interact with the world and choose an appropriate action based on data and objectives. They include game playing, autonomous vehicles and robotics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Reinforcement learning&lt;/strong&gt; is a subfield of machine learning, popular in decision making problems. The algorithms are generally grouped by:&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot; colspan=&quot;2&quot;&gt; &lt;b&gt;Value-based&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot; rowspan=&quot;2&quot;&gt; &lt;b&gt;Policy-based&lt;/b&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt; &lt;b&gt;Model-based&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt; &lt;b&gt;Model-free&lt;/b&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt;
            &lt;b&gt;Classic&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning/#policy-iteration&quot;&gt;Policy Iteration&lt;/a&gt;,
            &lt;br /&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning/#value-iteration&quot;&gt;Value Iteration&lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning/#policy-evaluation&quot;&gt;Monte Carlo&lt;/a&gt;,
            &lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning/#sarsa&quot;&gt;SARSA&lt;/a&gt;,
            &lt;br /&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning/#q-learning&quot;&gt;Q Learning&lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_2_policy_based_reinforcement_learning/#policy-gradient&quot;&gt;
                Policy Gradient
            &lt;/a&gt;,
            &lt;a href=&quot;/posts/machine_learning/decision_2_policy_based_reinforcement_learning/#deterministic-policy-gradient&quot;&gt;
                Deterministic Policy Gradient
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt;
            &lt;b&gt;Deep&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot; colspan=&quot;2&quot;&gt;
            Deep Q Network, Double Deep Q Network,
            &lt;br /&gt;
            Duel Deep Q Network, Rainbow
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            Vanilla Policy Gradient, Natural Policy Gradient,
            &lt;br /&gt;
            Trust Region Policy Optimization,
            &lt;br /&gt;
            Proximal Policy Optimization
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt;
            &lt;b&gt;Classic&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 1000px&quot; colspan=&quot;4&quot;&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_2_policy_based_reinforcement_learning/#actor-critic&quot;&gt;Actor Critic&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt;
            &lt;b&gt;Deep&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 1000px&quot; colspan=&quot;4&quot;&gt;
            Deep Deterministic Policy Gradient, Twin Delayed Deep Deterministic Policy Gradient,
            &lt;br /&gt;
            Soft Q Learning, Soft Actor Critic
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Value-based reinforcement learning (RL)&lt;/strong&gt; is one of two fundamental classes of RL algorithms. It implicitly constructs the policy based on estimated value functions, so its objective is to correctly estimate value functions. The procedure is divided into two steps.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Policy evaluation&lt;/strong&gt;: A process of evaluating the policy by obtaining the Bellman equation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Policy improvement&lt;/strong&gt;: A process of obtaining the improved policy based on the Bellman equation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This post aims to provide a tutorial on value-based RL algorithms.&lt;/p&gt;

&lt;h1 id=&quot;policy-evaluation&quot;&gt;Policy Evaluation&lt;/h1&gt;

&lt;p&gt;The policy evaluation is a process of evaluating the policy by obtaining the Bellman equation. There are mainly three methods in the policy evaluation.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt; Dynamic Programming &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt; Temporal Difference &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt; Monte Carlo &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/1_1_dp.png&quot; style=&quot;width: 300px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/1_1_td.png&quot; style=&quot;width: 300px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/1_1_mc.png&quot; style=&quot;width: 300px&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Methods for policy evaluation from
        &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf&quot;&gt;
            David Silver's lecture
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/table&gt;

&lt;p&gt;Where the update for the value functions is:&lt;/p&gt;

\[\begin{align}
    \text{DP: } &amp;amp; v_{\pi}^{(i+1)} ( s ) \leftarrow \sum_{a \in A} \pi (a \vert s)
    \left(
    \sum_{s' \in S} p (s' \vert s , a) \left( r' + \gamma \cdot v_{\pi}^{(i)} (s') \right)
    \right)
    \\
    \text{TD: } &amp;amp; v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} )
    + \alpha \left( \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right) - v_{\pi}^{(i)} ( s_{t} ) \right)
    \\
    \text{MC: } &amp;amp; v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} )
    + \alpha \left( \left( \sum_{k = 0}^{\infty} \gamma^{k} r_{t + k + 1} \right) - v_{\pi}^{(i)} ( s_{t} ) \right)
\end{align}\]

&lt;p&gt;Where \(s \equiv s_{t}\) and \(s' \equiv s_{t+1}\). The notation \(s\) and \(s'\) signify the state and the neighbouring state while \(s_{t}\) and \(s_{t+1}\) signify the current state and the next state for a given trajectory, \(\tau = ( S_{0} = s_{0} , A_{0} = a_{0} , \cdots  A_{T-1} = a_{T-1} , S_{T} = s_{T} )\).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dynamic programming (DP)&lt;/strong&gt; is both a mathematical optimization method and a computer programming method. It is a general idea of expressing a complex problem into simpler sub-problems, but in this post, I will focus on reinforcement learning (RL), in particular solving a Markov decision process (MDP). Note that this method is developed by Richard Bellman, which the Bellman equation is named after. DP simply iterates through all the states and actions to recursively update the value functions with full access to the environment dynamics of the MDP.&lt;/p&gt;

&lt;p&gt;In practice, DP is not preferred as most of the environments are either 1) unknown MDPs, i.e. how the opponent will play the game Go is unknown, or 2) known but intractably large MDPs, i.e. the game Go has \(\vert \mathcal{S} \vert \approx 10^{170}\) and \(\vert \mathcal{A} \vert \approx 10^{360}\). To handle this, we often use sampling methods for the policy evaluation, which are divided into &lt;strong&gt;temporal difference (TD)&lt;/strong&gt; or &lt;strong&gt;Monte-Carlo (MC)&lt;/strong&gt;. They do not require to access the environment dynamics, but instead, for a given \(\tau\), the states and actions are sampled for value function updates.&lt;/p&gt;

&lt;h2 id=&quot;comparison&quot;&gt;Comparison&lt;/h2&gt;

&lt;p&gt;Policy evaluation methods can be compared with respect to 1) sampling and 2) bootstrapping.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 450px&quot;&gt;
    &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/1_2_dp_td_mc.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Comparison between methods from
        &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf&quot;&gt;
            David Silver's lecture
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Full backups account for every possible action to compute the Bellman equations while sample backups account for only sampled actions.&lt;/li&gt;
  &lt;li&gt;Deep backups account for the entire trajectory to compute the Bellman equations while shallow backups account for only a single time step ahead.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the term &lt;em&gt;full backups&lt;/em&gt; and &lt;em&gt;sample backups&lt;/em&gt; are used to describe the update method of the value functions, algorithms that utilize these techniques are referred differently:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Model-based&lt;/strong&gt;: The optimal policy is obtained using the environment dynamics, such as \(p ( s' \vert s , a )\).
    &lt;ul&gt;
      &lt;li&gt;DP is known as model-based while the heuristic search can be model-based or model-free.&lt;/li&gt;
      &lt;li&gt;Model-based algorithms include those models that do not access the environment dynamics, but learn and predict the environment dynamics. Thus, generally, any model-free algorithm with additional world predicting algorithms that help the update is considered model-based.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model-free&lt;/strong&gt;: The optimal policy is obtained using sampled trajectories \(\tau\).
    &lt;ul&gt;
      &lt;li&gt;Both TD and MC are considered model-free since they do not access nor predict environment dynamics.&lt;/li&gt;
      &lt;li&gt;Majority of algorithms used in RL are model-free, but as mentioned, if additional world predicting algorithms are employed to help the update, they are classified as model-based.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The standard RL usually refers to model-free RL. TD and MC can further be generalized as:&lt;/p&gt;

\[\begin{equation}
    v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} ) + \alpha \delta
    \quad
    \delta = \hat{v}_{\pi}^{(i)} ( s_{t} ) - v_{\pi}^{(i)} ( s_{t} )
    \\
    \hat{v}_{\pi}^{(i)} ( s_{t} ) =
    \begin{cases}
        r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) &amp;amp; \text{for TD} \\
        R_{t} = \sum_{k = 0}^{\infty} \gamma^{k} r_{t+k+1} &amp;amp; \text{for MC} \\
    \end{cases}
\end{equation}\]

&lt;p&gt;Where:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\hat{v}_{\pi}^{(i)}\) is the estimated target value and \(v_{\pi}^{(i)}\) is the value.&lt;/li&gt;
  &lt;li&gt;\(\delta\) is the difference between \(\hat{v}_{\pi}^{(i)}\) and \(v_{\pi}^{(i)}\), where \(\delta \rightarrow 0\) as \(i \rightarrow \infty\).&lt;/li&gt;
  &lt;li&gt;Higher \(\alpha\) results in faster convergence, but may induce oscillations during learning.&lt;/li&gt;
  &lt;li&gt;Lower \(\alpha\) results in slow convergence that may potentially result in falling into the local optima.&lt;/li&gt;
  &lt;li&gt;The difference between TD and MC is how \(\hat{v}_{\pi}^{(i)}\) is constructed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The idea of TD and MC is sampling techniques in the policy evaluation, where the update is done using trajectories that the agent travelled with the initial state distribution, \(p_{0}\), instead of accessing the state transition probabilities. While the policy evaluation differs, the policy improvement in TD and MC follows the greedy policy, the same as DP.&lt;/p&gt;

&lt;h2 id=&quot;bias-and-variance&quot;&gt;Bias and Variance&lt;/h2&gt;

&lt;p&gt;Model-free algorithms suffer from bias and variance problems since they use a sampling technique to update the value functions.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;MC has no bias: Since \(v_{\pi} ( s_{t} ) = \mathbb{E}_{ \tau \sim \pi } \left[ R_{t} \vert s_{t} \right]\):&lt;/p&gt;

\[\begin{equation}
     \mathbb{E}_{ \tau \sim \pi } \left[ \hat{v}_{\pi}^{(i)} ( s_{t} ) \right]
     = \mathbb{E}_{ \tau \sim \pi } \left[ R_{t} \vert s_{t} \right] \equiv v_{\pi} ( s_{t} )
 \end{equation}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TD has bias: Since \(\hat{v}_{\pi}^{(i)} ( s_{t} ) = r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} )\), \(\hat{v}_{\pi}^{(i)} ( s_{t} )\) follows \(v_{\pi}^{(i)} ( s_{t+1} )\), where \(v_{\pi}^{(i)} ( s_{t+1} )\) is an incorrect estimate. Thus, the initialization of the value functions causes bias in TD.&lt;/p&gt;

\[\require{color}
 \begin{align}
     v_{\pi}^{(i+1)} ( s_{t} )
     &amp;amp; \leftarrow v_{\pi}^{(i)} ( s_{t} ) + \alpha \left( \hat{v}_{\pi}^{(i)} ( s_{t} ) - v_{\pi}^{(i)} ( s_{t} ) \right)
     \quad
     \hat{v}_{\pi}^{(i)} ( s_{t} ) = r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} )
     \\
     &amp;amp; = \textcolor{red}{ v_{\pi}^{(i)} ( s_{t} ) + \alpha \left( \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right) - v_{\pi}^{(i)} ( s_{t} ) \right) }
     \\
     &amp;amp; = \textcolor{blue}{ ( 1 - \alpha ) \cdot v_{\pi}^{(i)} ( s_{t} )
     + \alpha \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right) }
     \\
     &amp;amp; = ( 1 - \alpha ) \cdot \left(
     \textcolor{red}{ v_{\pi}^{(i-1)} ( s_{t} ) + \alpha \left( \left( r_{t+1} + \gamma v_{\pi}^{(i-1)} ( s_{t+1} ) \right) - v_{\pi}^{(i-1)} ( s_{t} ) \right) }
     \right)
     + \alpha \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right)
     \\
     &amp;amp; = ( 1 - \alpha ) \cdot \left(
     \textcolor{blue}{ ( 1 - \alpha ) \cdot v_{\pi}^{(i-1)} ( s_{t} )
     + \alpha \left( r_{t+1} + \gamma v_{\pi}^{(i-1)} ( s_{t+1} ) \right) }
     \right)
     + \alpha \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right)
     \\
     &amp;amp; = ( 1 - \alpha )^{2} \cdot v_{\pi}^{(i-1)} ( s_{t} )
     + \alpha ( 1 - \alpha )^{1} \left( r_{t+1} + \gamma v_{\pi}^{(i-1)} ( s_{t+1} ) \right)
     + \alpha ( 1 - \alpha )^{0} \left( r_{t+1} + \gamma v_{\pi}^{(i-0)} ( s_{t+1} ) \right)
     \\
     &amp;amp; \; \vdots
     \\
     &amp;amp; = ( 1 - \alpha )^{i} \cdot v_{\pi}^{(0)} ( s_{t} )
     + \sum_{j = 0}^{i} \alpha ( 1 - \alpha )^{j} \left( r_{t+1} + \gamma v_{\pi}^{(i-j)} ( s_{t+1} ) \right)
 \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;If \(i \rightarrow \infty\), then \(( 1 - \alpha )^{i} \cdot v_{\pi}^{(0)} ( s_{t} ) \rightarrow 0\). However, the second term still includes the value functions of the early time steps, i.e. \(\alpha ( 1 - \alpha )^{i-1} \left( r_{t+1} + \gamma v_{\pi}^{(1)} ( s_{t+1} ) \right)\). Thus, the bias decreases over time, but will not be removed. This is based on
 &lt;a href=&quot;https://stats.stackexchange.com/questions/454856/why-is-temporal-difference-learning-biased-in-reinforcement-learning&quot;&gt;Why is temporal difference learning biased in reinforcement learning?&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MC has high variance: \(R_{t}\) at small \(t\) change drastically if \(a_{t}\) at small \(t\) change. Intuitively, in complex environments, there are infinitely many trajectories, but most of them are useless, so \(G_{0}\) for most trajectories will be zero or very small while there are some trajectories that produce relatively high \(G_{0}\).&lt;/p&gt;

\[\require{color}
 \begin{align}
     \text{Var} \left[ R_{t} \right]
     &amp;amp; = \text{Var} \left[ \sum_{k = 0}^{\infty} \gamma^{k} r_{t+k+1} \right]
     \\
     &amp;amp; = \sum_{k = 0}^{\infty} (\gamma^{k})^{2} \text{Var} \left[ r_{t+k+1} \right]
     + \sum_{i \neq j} \gamma^{i} \cdot \gamma^{j} \cdot \text{Cov} \left[ r_{t+i+1}, r_{t+j+1} \right]
     \quad
     \text{Cov} \left[ r_{t+i+1}, r_{t+j+1} \right] = 0
     \\
     &amp;amp; = \sum_{k = 0}^{\infty} (\gamma^{k})^{2} \text{Var} \left[ r_{t+k+1} \right]
     + \sum_{i \neq j} \gamma^{i} \cdot \gamma^{j} \cdot 0
     \\
     &amp;amp; = \sum_{k = 0}^{\infty} (\gamma^{k})^{2} \text{Var} \left[ r_{t+k+1} \right]
     \\
     &amp;amp; &amp;lt; \sum_{k = 0}^{\infty} \gamma^{k} \text{Var} \left[ r_{t+k+1} \right]
     \quad
     \text{where, } \gamma \in (0, 1)
 \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;Note that \(\text{Cov} \left[ r_{t+i+1}, r_{t+j+1} \right] = 0\) is not really true since rewards in different time steps are correlated, but this is to provide a simple intuitive derivation. This is based on
 &lt;a href=&quot;https://ai.stackexchange.com/questions/17810/how-does-monte-carlo-have-high-variance&quot;&gt;How does Monte Carlo have high variance?&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TD has low variance: This term â€˜lowâ€™ is relative to MC. When computing \(\hat{v}_{\pi}^{(i)} ( s_{t} ) = r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} )\), we do not account variance for rewards in the later time steps, and thus, the variance is \(\text{Var} \left[ r_{t+1} \right]\), which is smaller than \(\sum_{k = 0}^{\infty} \gamma^{k} \text{Var} \left[ r_{t+k+1} \right]\).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thus, TD is known to have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;high bias and low variance&lt;/code&gt;, while MC is known to have &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;low bias and high variance&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;n-step-bootstrapping&quot;&gt;\(n\)-Step Bootstrapping&lt;/h2&gt;

&lt;p&gt;In RL, the &lt;strong&gt;bootstrapping&lt;/strong&gt; is a process of computing \(\hat{v}_{\pi}^{(i)} ( s_{t} )\) based on \(v_{\pi}^{(i)} ( s_{t+1} )\). Instead of thoroughly relying on either TD or MC, we can use the \(n\)&lt;strong&gt;-step bootstrapping&lt;/strong&gt; to balance bias and variance.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 400px&quot;&gt;
    &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/1_3_td_mc.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Comparison between temporal difference and Monte Carlo methods from
        &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf&quot;&gt;
            David Silver's lecture
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Where the update for the value functions is done according to:&lt;/p&gt;

\[\require{color}
\begin{equation}
    \hat{v}_{\pi}^{(i)} ( s_{t} ) = R_{t}^{( \textcolor{red}{n} )} = \sum_{\textcolor{blue}{k} = 0}^{\textcolor{red}{n} - 1} \gamma^{\textcolor{blue}{k}} r_{t + \textcolor{blue}{k} + 1} + \gamma^{ \textcolor{red}{n} } v_{\pi}^{(i)} ( s_{t + \textcolor{red}{n}} )
\end{equation}\]

&lt;p&gt;The \(n\)-step bootstrapping with \(n = 1\) is the standard TD approach and \(n = \infty\) is equivalent to MC. Due to how it can balance bias and variance, the \(n\)-step bootstrapping with reasonable \(n\) is more preferred than MC.&lt;/p&gt;

&lt;h1 id=&quot;dynamic-programming&quot;&gt;Dynamic Programming&lt;/h1&gt;

&lt;p&gt;Since DP can provide nice intuition, I will go through some common algorithms in DP. There are two classes of value-based RL algorithms.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Policy iteration&lt;/strong&gt;: The policy evaluation with the Bellman expectation equation and the policy improvement.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Value iteration&lt;/strong&gt;: The policy evaluation with the Bellman optimality equation, which encapsulates the policy improvement with the greedy policy.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;policy-iteration&quot;&gt;Policy Iteration&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;policy iteration&lt;/strong&gt; is to find the optimal policy with the Bellman expectation equation. There exist two policy iteration algorithms based on the type of value functions used in the policy evaluation.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Generalized policy iteration&lt;/strong&gt;, or &lt;strong&gt;V-policy iteration&lt;/strong&gt; or iterative policy evaluation: The state value is used as a value function.&lt;/li&gt;
&lt;/ul&gt;

\[\begin{equation}
    v_{\pi}^{(i+1)} ( s )
    \leftarrow \sum_{a \in A} \pi (a \vert s)
    \left( \sum_{s' \in S} p (s' \vert s , a) \left( r' + \gamma v_{\pi}^{(i)} (s') \right) \right)
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Q-policy iteration&lt;/strong&gt;: The Q value is used as a value function.&lt;/li&gt;
&lt;/ul&gt;

\[\begin{equation}
    q_{\pi}^{(i+1)} ( s , a )
    \leftarrow \sum_{s' \in S} p (s' \vert s , a)
    \left( r' + \gamma \sum_{a' \in A} \pi (a' \vert s') q_{\pi}^{(i)} (s', a') \right)
\end{equation}\]

&lt;p&gt;While the policy evaluation differs, the policy improvement usually uses the greedy policy.&lt;/p&gt;

\[\begin{equation}
    \pi_{\text{greedy}} ( a \vert s )
    = \begin{cases}
      1 &amp;amp; \arg \max_{a} q_{\pi} ( s , a ) \\
      0 &amp;amp; \text{otherwise}
    \end{cases}
\end{equation}\]

&lt;p&gt;Where \(\pi_{\text{greedy}}\) is the most common example of the deterministic policy. Note that there are other ways to construct the policy in the policy improvement, i.e. the softmax policy, \(\pi_{\text{softmax}} ( a \vert s ) = \frac{ e^{ q_{\pi} ( s , a ) } }{ \sum_{a' \in \mathcal{A}} e^{ q_{\pi} ( s , a' ) } }\), for the stochastic policy.&lt;/p&gt;

&lt;p&gt;The procedure of the generalized policy iteration follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Policy evaluation: Obtain \(v_{\pi}^{(k)} ( s )\) for a given \(\pi^{(i)} ( a \vert s )\), where \(k\) is any reasonable number. Then, obtain \(q_{\pi}^{(i)} (s, a) = \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v_{\pi}^{(k)} (s') \right)\) for the policy improvement.&lt;/li&gt;
  &lt;li&gt;Policy improvement: Set \(\pi^{(i+1)} ( a \vert s )\) as the greedy policy for a given \(q_{\pi}^{(i)} (s, a)\).&lt;/li&gt;
  &lt;li&gt;Policy iteration: Repeat the policy evaluation and the policy improvement to obtain \(\pi^{(\infty)} ( a \vert s ) \equiv \pi^{\ast} ( a \vert s )\).&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 600px&quot;&gt;
    &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_1_pi.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Generalized policy iteration diagram from
        &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf&quot;&gt;
            David Silver's lecture
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Thus, this process is:&lt;/p&gt;

\[\begin{equation}
    \pi^{(0)} ( a \vert s )
    \rightarrow v_{\pi}^{(0 \rightarrow k)} ( s )
    \rightarrow q_{\pi}^{(1)} (s, a)
    \rightarrow \pi^{(1)} ( a \vert s ) \rightarrow \cdots
    \rightarrow \pi^{(\infty)} ( a \vert s )
\end{equation}\]

&lt;p&gt;The generalized policy iteration requires computing the Q value additionally to obtain the policy.&lt;/p&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;p&gt;Consider a 4-by-4 deterministic gridworld problem with action set, \(\mathcal{A} = \{ N, S, E, W \}\).&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 450px&quot;&gt;
    &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_2_gw.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Gridworld from
        &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf&quot;&gt;
            David Silver's lecture
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The grey regions are the terminal states and a reward is held in every state transition, \(r_{t+1} = r ( s_{t} , a_{t} , s_{t+1} ) = -1\). Let the initial policy be a uniform distribution over action, \(\pi^{(0)} ( a \vert s ) = \mathcal{U} ( \mathcal{A} )\), then, the value functions are computed by applying the Bellman expectation equation repetitively.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; $$ \pi^{(0)} ( a \vert s ) $$ &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; $$ v_{\pi}^{(0)} ( s ) $$ &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; $$ v_{\pi}^{(1)} ( s ) $$ &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; $$ v_{\pi}^{(2)} ( s ) $$ &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; $$ v_{\pi}^{(\infty)} ( s ) $$ &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt; $$ \pi^{(1)} ( a \vert s ) $$ &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt;
            &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_pi0.png&quot; style=&quot;width: 150px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt;
            &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_v0.png&quot; style=&quot;width: 150px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt;
            &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_v1.png&quot; style=&quot;width: 150px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt;
            &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_v2.png&quot; style=&quot;width: 150px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt;
            &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_v.png&quot; style=&quot;width: 150px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt;
            &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_pi1.png&quot; style=&quot;width: 150px&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Value and policy in gridworld with dynamic programming from
        &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf&quot;&gt;
            David Silver's lecture
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/table&gt;

&lt;ol&gt;
  &lt;li&gt;Policy evaluation: For a given \(\pi^{(0)} ( a \vert s )\), iterate value functions until it converges, \(v_{\pi}^{(0)} ( s ) \rightarrow v_{\pi}^{(\infty)} ( s )\),
    &lt;ul&gt;
      &lt;li&gt;The grey regions have \(v_{\pi}^{(\infty)} ( s ) = 0\) since it is the terminal state, no action is available.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Policy improvement: Update the policy with actions from low to high value functions, \(\pi^{(0)} ( a \vert s ) \rightarrow \pi^{(1)} ( a \vert s )\). For instance, define \(\pi ( a \vert s ) = \begin{bmatrix} p_{N} &amp;amp; p_{S} &amp;amp; p_{E} &amp;amp; p_{W} \end{bmatrix}^{\intercal}\):
 \(\begin{equation}
     \pi^{(0)} ( a \vert s = 1 )
     = \begin{bmatrix} \frac{1}{4} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} \end{bmatrix}^{\intercal}
     \quad \rightarrow \quad
     \pi^{(1)} ( a \vert s = 1 )
     = \begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}^{\intercal}
     \\
     \pi^{(0)} ( a \vert s = 2 )
     = \begin{bmatrix} \frac{1}{4} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} \end{bmatrix}^{\intercal}
     \quad \rightarrow \quad
     \pi^{(1)} ( a \vert s = 2 )
     = \begin{bmatrix} 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}^{\intercal}
     \\
     \pi^{(0)} ( a \vert s = 3 )
     = \begin{bmatrix} \frac{1}{4} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} &amp;amp; \frac{1}{4} \end{bmatrix}^{\intercal}
     \quad \rightarrow \quad
     \pi^{(1)} ( a \vert s = 3 )
     = \begin{bmatrix} 0 &amp;amp; \frac{1}{2} &amp;amp; 0 &amp;amp; \frac{1}{2} \end{bmatrix}^{\intercal}
 \end{equation}\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this example, \(k \rightarrow \infty\). In theory, under \(\pi^{(0)} ( a \vert s ) = \mathcal{U} ( \mathcal{A} )\), a single policy improvement with \(v_{\pi}^{(\infty)} ( s )\) is required for the optimal policy, \(\pi^{(1)} ( a \vert s ) \equiv \pi^{(\ast)} ( a \vert s )\). However, in practice, &lt;em&gt;iterating through the policy evaluation and the policy improvement is more efficient&lt;/em&gt;. Thus, the generalized policy iteration usually switches between the policy evaluation and the policy improvement with a reasonable \(k\).&lt;/p&gt;

&lt;h2 id=&quot;value-iteration&quot;&gt;Value Iteration&lt;/h2&gt;

&lt;p&gt;The policy iteration algorithms iterate both value function and policy. The &lt;strong&gt;value iteration&lt;/strong&gt; algorithms iterate only the value functions under the idea that the greedy policy, a.k.a. the Bellman optimality equation. Similar to the policy iteration, there exist two value iteration algorithms.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;V-value iteration&lt;/strong&gt;: The state value is used as a value function.&lt;/li&gt;
&lt;/ul&gt;

\[\begin{equation}
    v_{\pi}^{(i+1)} ( s )
    \leftarrow \max_{a}
    \left( \sum_{s' \in S} p (s' \vert s , a) \left( r' + \gamma v_{\pi}^{(i)} (s') \right) \right)
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Q-value iteration&lt;/strong&gt;: The Q value is used as a value function.&lt;/li&gt;
&lt;/ul&gt;

\[\begin{equation}
    q_{\pi}^{(i+1)} ( s , a )
    \leftarrow \sum_{s' \in S} p (s' \vert s , a)
    \left( r' + \gamma \max_{a'} q_{\pi}^{(i)} (s', a') \right)
\end{equation}\]

&lt;p&gt;The only difference between the policy iteration is that the value functions are computed following the greedy policy.&lt;/p&gt;

&lt;p&gt;The value iteration can be seen as performing the policy improvement for every single step of the policy evaluation, so both policy iteration and value iteration yield the same optimal policy. Due to this, the value iteration is usually considered more efficient.&lt;/p&gt;

&lt;p&gt;The process of the value iteration becomes much simpler than the policy iteration.&lt;/p&gt;

\[\begin{equation}
    \pi^{(0)} ( a \vert s )
    \rightarrow v_{\pi}^{(0 \rightarrow k)} ( s )
    \rightarrow q_{\pi}^{(1)} (s, a)
    \rightarrow \pi^{(\infty)} ( a \vert s )
\end{equation}\]

&lt;h1 id=&quot;temporal-difference-algorithm&quot;&gt;Temporal Difference Algorithm&lt;/h1&gt;

&lt;p&gt;TD algorithms are the most popular method to solve MDP in the real world scenario.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Since they sample trajectories, they are robust to complex unknown environments.&lt;/li&gt;
  &lt;li&gt;Due to their low variance and simplicity, they converge quickly.&lt;/li&gt;
  &lt;li&gt;The bias and variance can be balanced with \(n\)-step bootstrapping.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;strong&gt;temporal difference (TD) learning&lt;/strong&gt; is a TD version of the generalized policy iteration that follows the basic form:&lt;/p&gt;

\[\begin{equation}
    v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} ) + \alpha \left( \hat{v}_{\pi}^{(i)} ( s_{t} ) - v_{\pi}^{(i)} ( s_{t} ) \right)
    \\
    \hat{v}_{\pi}^{(i)} ( s_{t} ) = r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} )
\end{equation}\]

&lt;p&gt;It often requires more iterations than the generalized policy iteration as they sample the trajectories for value function update.&lt;/p&gt;

&lt;h2 id=&quot;sarsa&quot;&gt;SARSA&lt;/h2&gt;

&lt;p&gt;Similar to the Q-policy iteration, instead of using state value in the TD learning, we can directly evaluate the policy using the Q value and apply the policy improvement. This TD version of Q-policy iteration is referred to as the &lt;strong&gt;state-action-reward-state-action (SARSA)&lt;/strong&gt;, originated from how it samples a data point \(( s, a, r, s', a' )\).&lt;/p&gt;

\[\begin{equation}
    q_{\pi}^{(i+1)} ( s_{t} , a_{t} ) \leftarrow q_{\pi}^{(i)} ( s_{t} , a_{t} ) + \alpha \left( \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) - q_{\pi}^{(i)} ( s_{t} , a_{t} ) \right)
    \\
    \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) = r_{t+1} + \gamma q_{\pi}^{(i)} ( s_{t+1} , a_{t+1} )
\end{equation}\]

&lt;p&gt;Since the policy improvement follows the greedy policy, the agent only selects actions with the highest Q value. Due to this, both TD learning and SARSA suffer from poor exploration. To remedy this, we introduce:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;On-policy&lt;/strong&gt;: How the agent explores is the same as what it learns, or \(a \sim \pi ( a \vert s )\) where \(\pi ( a \vert s ) = \arg \max_{a} q_{\pi} ( s , a )\).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Off-policy&lt;/strong&gt;: How the agent explores is different from what it learns, or \(a \sim \beta ( a \vert s )\) and \(\pi ( a \vert s ) = \arg \max_{a} q_{\pi} ( s , a )\), where \(\beta ( a \vert s )\) is referred to as the &lt;strong&gt;behaviour policy&lt;/strong&gt; and \(\pi ( a \vert s )\) is referred to as the &lt;strong&gt;target policy&lt;/strong&gt; such that \(\beta ( a \vert s ) \neq \pi ( a \vert s )\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\(\beta\) is used to act on the environment and collect trajectories while \(\pi\) is the policy updated with the collected trajectories. In general, \(\beta\) uses \(\pi\) partially, either with probability or smoothing. The most common off-policy algorithm is \(\epsilon\)&lt;strong&gt;-greedy exploration&lt;/strong&gt;:&lt;/p&gt;

\[\begin{equation}
    \pi ( a \vert s )
    = \begin{cases}
      1 &amp;amp; \arg \max_{a} q_{\pi} ( s , a ) \\
      0 &amp;amp; \text{otherwise}
    \end{cases}
    \quad
    \beta ( a \vert s )
    =
    \begin{cases}
        1 - \epsilon + \frac{ \epsilon }{ \vert \mathcal{A} \vert } &amp;amp; \arg \max_{a} q_{\pi} ( s , a ) \\
        \frac{ \epsilon }{ \vert \mathcal{A} \vert } &amp;amp; \text{otherwise}
    \end{cases}
\end{equation}\]

&lt;!-- - Note that in order for off-policy to work, an additional condition is that $$ \beta ( a \vert s ) $$ must not deviate much from $$ \pi ( a \vert s ) $$. If $$ \beta ( a \vert s ) $$ is very different from $$ \pi ( a \vert s ) $$, i.e. $$ \beta ( a \vert s ) = \arg \min_{a} q_{\pi} ( s , a ) $$, off-policy would not work. --&gt;

&lt;p&gt;\(\beta\) selects any action with \(\frac{ \epsilon }{ \vert \mathcal{A} \vert }\), resulting in diverse action selection, and thus, better exploration.&lt;/p&gt;

&lt;p&gt;However, the off-policy method introduces the &lt;strong&gt;distribution shift&lt;/strong&gt;, where the distribution we are following, \(\beta\), is not the same as the distribution we are updating, \(\pi\). To diminish this effect, we use the &lt;strong&gt;importance sampling (IS)&lt;/strong&gt;, or importance sampling correction.&lt;/p&gt;

\[\require{color}
\begin{align}
    \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} )
    &amp;amp; = \textcolor{blue}{\prod_{l=0}^{\infty}}
    \frac{ \pi ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    { \beta ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    \left( \textcolor{green}{\sum_{k=0}^{\infty}} \gamma^{\textcolor{green}{k}} r_{t+\textcolor{green}{k}+1}^{[\beta]} \right)
    = \textcolor{blue}{\prod_{l=0}^{\infty}}
    \frac{ \pi ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    { \beta ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    R_{t}^{[\beta]}
    = R_{t}^{[\pi]}
    \\
    &amp;amp; \equiv \textcolor{blue}{\prod_{l=0}^{\textcolor{red}{n}-1}}
    \frac{ \pi ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    { \beta ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    \left( \textcolor{green}{\sum_{k=0}^{\textcolor{red}{n}-1}} \gamma^{\textcolor{green}{k}} r_{t+\textcolor{green}{k}+1}^{[\beta]}
    + \gamma^{\textcolor{red}{n}} q_{\pi}^{(i)} ( s_{t+\textcolor{red}{n}} , a_{t+\textcolor{red}{n}} ) \right)
    \\
    &amp;amp; \; \vdots
    \\
    &amp;amp; \equiv
    \frac{ \pi ( a_{t} \vert s_{t} ) }{ \beta ( a_{t} \vert s_{t} ) }
    \frac{ \pi ( a_{t+1} \vert s_{t+1} ) }{ \beta ( a_{t+1} \vert s_{t+1} ) }
    \left( r_{t+1}^{[\beta]} + \gamma r_{t+2}^{[\beta]} + \gamma^{2} q_{\pi}^{(i)} ( s_{t+2} , a_{t+2} ) \right)
    \\
    &amp;amp; \equiv
    \frac{ \pi ( a_{t} \vert s_{t} ) }{ \beta ( a_{t} \vert s_{t} ) }
    \left( r_{t+1}^{[\beta]} + \gamma q_{\pi}^{(i)} ( s_{t+1} , a_{t+1} ) \right)
\end{align}\]

&lt;p&gt;Where \(\prod_{l=0}^{\infty} \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }{ \beta ( a_{t+l} \vert s_{t+l} ) }\) is the IS and \(r^{[\beta]}\) signifies the reward acquired by \(a \sim \beta ( a \vert s )\). Applying the IS transforms \(r^{[\beta]} \rightarrow r^{[\pi]}\), so that they can be used to update \(\pi ( a \vert s )\). In other words, \(\tau\) sampled from \(\beta\) can be transformed into \(\tau\) sampled from \(\pi\) with the IS, and thus, updating \(q_{\pi}^{(i)}\) becomes valid. Therefore:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The SARSA follows \(\hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) \equiv \frac{ \pi ( a_{t} \vert s_{t} ) }{ \beta ( a_{t} \vert s_{t} ) } \left( r_{t+1}^{[\beta]} + \gamma q_{\beta}^{(i)} ( s_{t+1} , a_{t+1} ) \right)\).&lt;/li&gt;
  &lt;li&gt;The \(n\)-step TD method follows \(\hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) \equiv \prod_{l=0}^{n-1} \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }{ \beta ( a_{t+l} \vert s_{t+l} ) } \left( \sum_{k=0}^{n-1} \gamma^{k} r_{t+k+1}^{[\beta]} + \gamma^{n} q_{\pi}^{(i)} ( s_{t+n} , a_{t+n} ) \right)\).&lt;/li&gt;
  &lt;li&gt;The MC method follows \(\hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) \equiv \prod_{l=0}^{\infty} \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }{ \beta ( a_{t+l} \vert s_{t+l} ) } R_{t}^{[\beta]}\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The idea of IS can be used for re-using previously acquired experiences from old policies (experience replay) or learning from experiences collected by different agents (offline learning).&lt;/p&gt;

&lt;h2 id=&quot;q-learning&quot;&gt;Q Learning&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;Q learning (QL)&lt;/strong&gt; is a TD version of the Q-value iteration. It is the most popular value-based RL algorithm as it naturally uses the off-policy mechanism to update the policy, but without the IS. The QL updates the Q value with respect to the maximum Q value at the next time step. In other words, the greedy policy is used as the target policy.&lt;/p&gt;

\[\begin{equation}
    q_{\pi}^{(i+1)} ( s_{t} , a_{t} ) \leftarrow q_{\pi}^{(i)} ( s_{t} , a_{t} ) + \alpha \left( \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) - q_{\pi}^{(i)} ( s_{t} , a_{t} ) \right)
    \\
    \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} )
    = r_{t+1}^{(\beta)} + \gamma \max_{a'} q^{(i)} ( s_{t+1} , a' )
    \equiv r_{t+1}^{(\beta)} + \gamma q_{\pi}^{(i)} ( s_{t+1} , a_{t+1} )
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;\(\max_{a'} q^{(i)} ( s_{t+1} , a' )\) means that \(a'\) has been sampled in a way that obtains the maximum Q value at \(s_{t+1}\), which is &lt;em&gt;equivalent to using the greedy target policy&lt;/em&gt;, so \(\max_{a'} q^{(i)} ( s_{t+1} , a' ) \equiv q_{\pi}^{(i)} ( s_{t+1} , a_{t+1} )\).&lt;/li&gt;
  &lt;li&gt;This also means that the QL requires to compute \(q^{(i)} ( s_{t+1} , a' )\) for every \(a' \in \mathcal{A}\) during the update while the TD learning and SARSA do not.&lt;/li&gt;
  &lt;li&gt;The IS can be ignored in the QL because:
    &lt;ul&gt;
      &lt;li&gt;\(\max_{a'} q^{(i)} ( s_{t+1} , a' )\) is equivalent to \(\gamma q_{\pi}^{(i)} ( s_{t+1} , a_{t+1} )\) with the greedy policy.&lt;/li&gt;
      &lt;li&gt;\(r_{t+1}^{(\beta)}\) is used to update the Q value of the action that acquired the reward.&lt;/li&gt;
      &lt;li&gt;Some discussions on stack exchange are &lt;a href=&quot;https://stats.stackexchange.com/questions/335396/why-dont-we-use-importance-sampling-for-one-step-q-learning&quot;&gt;Why donâ€™t we use importance sampling for one step Q-learning?&lt;/a&gt; and &lt;a href=&quot;https://ai.stackexchange.com/questions/21859/why-we-dont-use-importance-sampling-in-tabular-q-learning&quot;&gt;Why we donâ€™t use importance sampling in tabular Q-Learning?&lt;/a&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The QL follows \(1\)-step bootstrapping, so in theory, if the \(n\)-step is used, the QL also needs the IS. &lt;a href=&quot;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&quot;&gt;Sutton and Barto&lt;/a&gt; introduced the \(n\)-step tree backup algorithm for the \(n\)-step QL, but it seems like small \(n\) without the IS still works in practice, i.e. 3-step in &lt;a href=&quot;https://arxiv.org/abs/1710.02298?context=cs&quot;&gt;RAINBOW&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The QL pseudocode is:&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 600px&quot;&gt;
    &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/3_1_ql_pseudocode.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        QL pseudocode from
        &lt;a href=&quot;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&quot;&gt;
            Reinforcement Learning: An Introduction
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The QL obtains the optimal value directly, so it is more efficient than the SARSA. However, this introduces additional maximization bias. Since \(\max\) is convex, under the Jensenâ€™s inequality:&lt;/p&gt;

\[\begin{align}
    \mathbb{E}_{ \tau \sim \pi } \left[ \max_{a'} q^{(i)} ( s_{t+1} , a' ) \right]
    \geq
    \max_{a'} \mathbb{E}_{ \tau \sim \pi } \left[ q^{(i)} ( s_{t+1} , a' ) \right]
\end{align}\]

&lt;p&gt;Thus, using the bias definition:&lt;/p&gt;

\[\begin{align}
    b ( \max_{a'} q^{(i)} ( s_{t+1} , a' ) )
    &amp;amp; = \mathbb{E}_{ \tau \sim \pi } \left[ \max_{a'} q^{(i)} ( s_{t+1} , a' ) \right] - \max_{a'} q^{(i)} ( s_{t+1} , a' )
    \\
    &amp;amp; = \mathbb{E}_{ \tau \sim \pi } \left[ \max_{a'} q^{(i)} ( s_{t+1} , a' ) \right] - \max_{a'} \mathbb{E}_{ \tau \sim \pi } \left[ q^{(i)} ( s_{t+1} , a' ) \right]
    \\
    &amp;amp; \geq 0
\end{align}\]

&lt;p&gt;This can be easily viewed in the cliff walking problem:&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 800px&quot;&gt;
    &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/3_2_cliff_walking.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Cliff walking problem with SARSA and QL from
        &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf&quot;&gt;
            David Silver's lecture
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;The Cliff&lt;/code&gt; is the terminal state with the reward of \(-100\), otherwise the reward of \(-1\). The QL always takes the shortest but relatively risky path while the SARSA takes the safe path.&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;
&lt;p&gt;In this post, the value-based RL algorithms are covered. Their methods are divided into:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Policy evaluation: A process of evaluating the policy by obtaining the Bellman equation.
    &lt;ul&gt;
      &lt;li&gt;Dynamic Programming: Full &amp;amp; shallow backups, known as model-based. It is not preferred in the real world scenario since it requires to access the environment dynamics.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      v_{\pi}^{(i+1)} ( s ) \leftarrow \sum_{a \in A} \pi (a \vert s)
      \left(
      \sum_{s' \in S} p (s' \vert s , a) \left( r' + \gamma \cdot v_{\pi}^{(i)} (s') \right)
      \right)
  \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;Temporal Difference: Sample &amp;amp; shallow backups, known as model-free. It is generally preferred due to its flexibility and simplicity. It is known to have high bias and low variance, but bias and variance can be balanced using n-step bootstrapping.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} )
      + \alpha \left( \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right) - v_{\pi}^{(i)} ( s_{t} ) \right)
  \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;Monte Carlo: Sample &amp;amp; deep backups, known as model-free. It is generally less preferred than the temporal difference due to its inflexibility and complexity. It is known to have low bias and high variance.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} )
      + \alpha \left( \left( \sum_{k = 0}^{\infty} \gamma^{k} r_{t + k + 1} \right) - v_{\pi}^{(i)} ( s_{t} ) \right)
  \end{equation}\]
  &lt;/li&gt;
  &lt;li&gt;Policy improvement: A process of obtaining the improved policy from the Bellman equation.
    &lt;ul&gt;
      &lt;li&gt;Greedy policy: A type of deterministic policy.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      \pi_{\text{greedy}} ( a \vert s )
      = \begin{cases}
        1 &amp;amp; \arg \max_{a} q_{\pi} ( s , a ) \\
        0 &amp;amp; \text{otherwise}
      \end{cases}
  \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;Softmax policy: A type of stochastic policy.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      \pi_{\text{softmax}} ( a \vert s )
      = \frac{ e^{ q_{\pi} ( s , a ) } }
      { \sum_{a' \in \mathcal{A}} e^{ q_{\pi} ( s , a' ) } }
  \end{equation}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on the policy evaluation, the algorithms are divided into:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Model-based: The optimal policy is obtained using the environment dynamics, such as \(p ( s' \vert s , a )\). DP is known as model-based.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Model-free: The optimal policy is obtained using sampled trajectories \(\tau\). TD and MC are known as model-free.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;The update of model-free algorithms can further be generalized as:&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} ) + \alpha \delta
      \quad
      \delta = \hat{v}_{\pi}^{(i)} ( s_{t} ) - v_{\pi}^{(i)} ( s_{t} )
      \\
      \hat{v}_{\pi}^{(i)} ( s_{t} ) =
      \begin{cases}
          r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) &amp;amp; \text{for TD} \\
          R_{t} = \sum_{k = 0}^{\infty} \gamma^{k} r_{t+k+1} &amp;amp; \text{for MC} \\
      \end{cases}
  \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;Model-free algorithms are known to suffer from bias and variance problems.
        &lt;ul&gt;
          &lt;li&gt;TD is known to have high bias and low variance&lt;/li&gt;
          &lt;li&gt;MC is known to have low bias and high variance.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;To balance bias and variance, the \(n\)-step bootstrapping is often employed.&lt;/li&gt;
    &lt;/ul&gt;

\[\require{color}
  \begin{equation}
      \hat{v}_{\pi}^{(i)} ( s_{t} ) = R_{t}^{( n )} = \sum_{k = 0}^{n - 1} \gamma^{k} r_{t + k + 1} + \gamma^{ n } v_{\pi}^{(i)} ( s_{t + n} )
  \end{equation}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The value-based RL algorithms are generally divided into:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Policy Iteration: The policy evaluation with the Bellman expectation equation and the policy improvement.&lt;/li&gt;
  &lt;li&gt;Value Iteration: The policy evaluation with the Bellman optimality equation, which encapsulates the policy improvement with the greedy policy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DP and TD algorithms are:&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 500px&quot;&gt;
    &lt;img src=&quot;/assets/machine_learning/decision_1_value_based_reinforcement_learning/4_1_dp_td.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        A comparison between DP and TD from
        &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf&quot;&gt;
            David Silver's lecture
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Where the first and second algorithms are of the policy iteration and the last algorithm is of the value iteration.&lt;/p&gt;

&lt;p&gt;In TD, there are two ways to sample \(\tau\).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;On-policy: How the agent explores is the same as what it learns, or \(a \sim \pi ( a \vert s )\) where \(\pi ( a \vert s ) = \arg \max_{a} q_{\pi} ( s , a )\).&lt;/li&gt;
  &lt;li&gt;Off-policy: How the agent explores is different from what it learns, or \(a \sim \beta ( a \vert s )\) and \(\pi ( a \vert s ) = \arg \max_{a} q_{\pi} ( s , a )\), where \(\beta ( a \vert s )\) is referred to as the behaviour policy and \(\pi ( a \vert s )\) is referred to as the target policy such that \(\beta ( a \vert s ) \neq \pi ( a \vert s )\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\(\beta\) is used to act on the environment and collect trajectories while \(\pi\) is the policy updated with the collected trajectories. In general, \(\beta\) uses \(\pi\) partially, either with probability or smoothing.&lt;/p&gt;

&lt;p&gt;The off-policy method introduces the distribution shift, where the distribution we are following, \(\beta\), is not the same as the distribution we are updating, \(\pi\). To diminish this effect, we use the IS.&lt;/p&gt;

\[\require{color}
\begin{align}
    \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} )
    &amp;amp; = \prod_{l=0}^{\infty}
    \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }
    { \beta ( a_{t+l} \vert s_{t+l} ) }
    \left( \sum_{k=0}^{\infty} \gamma^{k} r_{t+k+1}^{[\beta]} \right)
    = \prod_{l=0}^{\infty}
    \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }
    { \beta ( a_{t+l} \vert s_{t+l} ) }
    R_{t}^{[\beta]}
    = R_{t}^{[\pi]}
    \\
    &amp;amp; \equiv \prod_{l=0}^{n-1}
    \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }
    { \beta ( a_{t+l} \vert s_{t+l} ) }
    \left( \sum_{k=0}^{n-1} \gamma^{k} r_{t+k+1}^{[\beta]}
    + \gamma^{n} q_{\pi}^{(i)} ( s_{t+n} , a_{t+n} ) \right)
\end{align}\]

&lt;p&gt;The QL is the most popular value-based RL algorithm. In summary, the QL:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;uses sampling, so it is robust to complex unknown environments. (Pros over DP)&lt;/li&gt;
  &lt;li&gt;has low variance due to bootstrapping, so it converges fast. (Pros over MC)&lt;/li&gt;
  &lt;li&gt;is off-policy, so it has better exploration than on-policy. (Pros over on-policy)&lt;/li&gt;
  &lt;li&gt;avoids importance sampling correction since it uses the Bellman optimality equation. (Pros over TD learning and SARSA)&lt;/li&gt;
  &lt;li&gt;has initialization bias, so the initialization influences the convergence. (Cons over MC)&lt;/li&gt;
  &lt;li&gt;has overestimation bias, so it takes a risky path. (Cons over SARSA)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Lecture 3 to 5 of &lt;a href=&quot;https://www.davidsilver.uk/teaching/&quot;&gt;David Silverâ€™s lecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Chapter 4 to 7 in &lt;a href=&quot;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>D. K. Ryu</name></author><category term="Machine Learning&amp;#x003a; Decision" /><summary type="html">Value-based reinforcement learning is one of two fundamental classes of reinforcement learning algorithms. It implicitly constructs the policy based on estimated value functions, so its objective is to correctly estimate value functions. This post aims to provide a tutorial on value-based reinforcement learning algorithms.</summary></entry><entry><title type="html">Policy-based Reinforcement Learning</title><link href="http://localhost:4000/posts/machine_learning/decision_2_policy_based_reinforcement_learning/" rel="alternate" type="text/html" title="Policy-based Reinforcement Learning" /><published>2022-09-15T00:00:00+09:00</published><updated>2023-04-15T00:00:00+09:00</updated><id>http://localhost:4000/posts/machine_learning/decision_2_policy_based_reinforcement_learning</id><content type="html" xml:base="http://localhost:4000/posts/machine_learning/decision_2_policy_based_reinforcement_learning/">&lt;div class=&quot;notice--primary&quot;&gt;
  
&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/posts/mathematics/markov_model_1_markov_chain&quot;&gt;Markov Chain&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/posts/mathematics/markov_model_2_markov_decision_process&quot;&gt;Markov Decision Process&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning&quot;&gt;Value-based Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;

&lt;blockquote&gt;
    Machine intelligence is the last invention that humanity will ever need to make.
    &lt;br /&gt;
    &lt;cite&gt;Nick Bostrom&lt;/cite&gt;
&lt;/blockquote&gt;

&lt;p&gt;If the universe is constructed with patterns and humans can interpret this, then so may machines. If machines become intelligent over humans, &lt;em&gt;humans need not apply&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Machine learning&lt;/strong&gt; is the study of algorithms that involve training a computer to learn patterns and relationships in data without being explicitly programmed. It can be divided into three types:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Supervised Learning&lt;/strong&gt;: The algorithm, or model, is trained on labeled data, where the goal is to map each input to the correct output.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unsupervised Learning&lt;/strong&gt;: The algorithm, or model, is trained on unlabeled data, where the goal is to identify patterns or structure in the data without being told what to look for.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reinforcement Learning&lt;/strong&gt;: The algorithm, or agent, is trained on the interactive worlds, where the goal is to learn a policy that maximizes rewards and minimize penalties.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The problems in ML can be largely divided by four types:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Computer Vision&lt;/strong&gt;: The problems require machines to interpret and understand visual data, i.e. images and videos. They includes image classification, object localization, video analysis, image captioning and visual question answering.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Natural Language Processing&lt;/strong&gt;: The problems require machines to interpret and understand human language, i.e. words and documents. They includes text classification, translation, summarization, speech analysis, question answering and dialogue system.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Graph Analysis&lt;/strong&gt;: The problems require machines to interpret and understand graph data, i.e. nodes and edges in graphs. They includes graph classification, link prediction, knowledge graph, social network analysis, recommendation system, combinatorial optimization and path finding.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Decision Making&lt;/strong&gt;: The problems require machines to interact with the world and choose an appropriate action based on data and objectives. They include game playing, autonomous vehicles and robotics.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Reinforcement learning&lt;/strong&gt; is a subfield of machine learning, popular in decision making problems. The algorithms are generally grouped by:&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot; colspan=&quot;2&quot;&gt; &lt;b&gt;Value-based&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot; rowspan=&quot;2&quot;&gt; &lt;b&gt;Policy-based&lt;/b&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt; &lt;b&gt;Model-based&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt; &lt;b&gt;Model-free&lt;/b&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt;
            &lt;b&gt;Classic&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning/#policy-iteration&quot;&gt;Policy Iteration&lt;/a&gt;,
            &lt;br /&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning/#value-iteration&quot;&gt;Value Iteration&lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning/#policy-evaluation&quot;&gt;Monte Carlo&lt;/a&gt;,
            &lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning/#sarsa&quot;&gt;SARSA&lt;/a&gt;,
            &lt;br /&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_1_value_based_reinforcement_learning/#q-learning&quot;&gt;Q Learning&lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_2_policy_based_reinforcement_learning/#policy-gradient&quot;&gt;
                Policy Gradient
            &lt;/a&gt;,
            &lt;a href=&quot;/posts/machine_learning/decision_2_policy_based_reinforcement_learning/#deterministic-policy-gradient&quot;&gt;
                Deterministic Policy Gradient
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt;
            &lt;b&gt;Deep&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot; colspan=&quot;2&quot;&gt;
            Deep Q Network, Double Deep Q Network,
            &lt;br /&gt;
            Duel Deep Q Network, Rainbow
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            Vanilla Policy Gradient, Natural Policy Gradient,
            &lt;br /&gt;
            Trust Region Policy Optimization,
            &lt;br /&gt;
            Proximal Policy Optimization
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt;
            &lt;b&gt;Classic&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 1000px&quot; colspan=&quot;4&quot;&gt;
            &lt;a href=&quot;/posts/machine_learning/decision_2_policy_based_reinforcement_learning/#actor-critic&quot;&gt;Actor Critic&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 100px&quot;&gt;
            &lt;b&gt;Deep&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 1000px&quot; colspan=&quot;4&quot;&gt;
            Deep Deterministic Policy Gradient, Twin Delayed Deep Deterministic Policy Gradient,
            &lt;br /&gt;
            Soft Q Learning, Soft Actor Critic
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Policy-based reinforcement learning&lt;/strong&gt; is one of two fundamental classes of reinforcement learning algorithms. It explicitly constructs the policy, directly mapping from the state to the action, such that maximizes the return. Unlike value-based reinforcement learning, policy-based reinforcement learning attempts directly find the optimal policy by optimizing:&lt;/p&gt;

\[\begin{equation}
    \mathcal{J} ( \theta )
    = \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} } [ R ( s , a ) ]
\end{equation}\]

&lt;p&gt;Where maximizing the objective is to obtain \(\pi_{\theta}\) such that maximizes \(R\) for every \(s\) and \(a\). This post aims to provide a tutorial on policy-based reinforcement learning algorithms and introduce a combination of value-based and policy-based reinforcement learning algorithm.&lt;/p&gt;

&lt;h1 id=&quot;policy-gradient&quot;&gt;Policy Gradient&lt;/h1&gt;

&lt;p&gt;The &lt;strong&gt;policy gradient (PG)&lt;/strong&gt; is an on-policy reinforcement learning (RL) algorithm that finds the optimal policy using the Monte-Carlo (MC) method. Unlike value-based methods, the PG parameterizes the policy with a function approximator such that maximizes the return. This has two advantages over value-based RL algorithms.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The policy is naturally stochastic.&lt;/li&gt;
  &lt;li&gt;The action space can be either discrete or continuous.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Consider the standard RL framework with an agent with the parameterized policy, \(\pi_{\theta}\), where \(\theta\) is the parameter. The optimal policy, \(\pi^{\ast}\), is:&lt;/p&gt;

\[\begin{equation}
    \pi^{\ast} ( a \vert s ) = \arg \max_{\pi} \mathbb{E}_{ s \sim d^{\pi}, a \sim \pi } [ R ( s , a ) ]
\end{equation}\]

&lt;p&gt;Then, the objective function can be expressed as:&lt;/p&gt;

\[\begin{equation}
    \mathcal{J} ( \theta )
    = \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} } [ R ( s , a ) ]
\end{equation}\]

&lt;p&gt;Maximizing the objective through gradient ascent obtains \(\theta\) that results in the policy which drives the agent to the high expected return at &lt;em&gt;any&lt;/em&gt; state-action pair.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The PG objective is different from \(Q^{\pi_{\theta}} ( s , a ) = \mathbb{E}_{ \tau \sim \pi_{\theta} } [ R_{t} \vert s_{t} = s , a_{t} = a ]\).
    &lt;ul&gt;
      &lt;li&gt;The Q value is the expectation of the return for a particular state-action pair.&lt;/li&gt;
      &lt;li&gt;The PG objective is the expectation of the return for &lt;em&gt;any&lt;/em&gt; state-action pair.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If the MDP is average-reward continuous and ergodic, maximizing \(Q^{\pi_{\theta}}\) is actually the same as the PG objective.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The PG objective function has many equivalent expressions.&lt;/p&gt;

\[\begin{align}
    \mathcal{J} ( \theta )
    &amp;amp; = \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} } [ R ( s , a ) ]
    = \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) R ( s , a )
    \\
    &amp;amp; \equiv \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} } \left[ Q^{\pi_{\theta}} (s, a) \right]
    = \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) Q^{\pi_{\theta}} (s, a)
    \\
    &amp;amp; \equiv \mathbb{E}_{ s \sim d^{\pi_{\theta}} } \left[ V^{\pi_{\theta}} (s) \right]
    = \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) V^{\pi_{\theta}} (s)
    \\
    &amp;amp; \equiv \mathbb{E}_{ s_{0} \sim p_{0} } \left[ V^{\pi_{\theta}} (s_{0}) \right]
    = \sum_{s_{0} \in \mathcal{S}} p_{0} ( s_{0} ) V^{\pi_{\theta}} (s_{0})
\end{align}\]

&lt;p&gt;Maximizing any of the expressions leads to the optimal policy.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Since \(d^{\pi_{\theta}} ( s ) \in [0, 1]\) and \(\pi_{\theta} (a \vert s) \in [0, 1]\), \(\sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) R ( s , a )\) is the expectation of the \(R\).&lt;/li&gt;
  &lt;li&gt;Even if \(Q^{\pi_{\theta}}\) is not the same as the PG objective, acquiring \(\theta\) that maximizes \(Q^{\pi_{\theta}}\) is equivalent to maximizing \(R\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;policy-gradient-theorem&quot;&gt;Policy Gradient Theorem&lt;/h2&gt;

&lt;p&gt;Computing the gradient of the PG objective, \(\nabla_{\theta} \mathcal{J}\), is very difficult since:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(\nabla_{\theta} \left( \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) R ( s , a ) \right)\) is intractable because \(d^{\pi_{\theta}} ( s )\), \(\pi_{\theta} (a \vert s)\), \(Q^{\pi_{\theta}} (s, a)\), \(V^{\pi_{\theta}} (s)\) and \(R ( s , a )\) are dependent on \(\theta\).&lt;/li&gt;
  &lt;li&gt;\(\nabla_{\theta} R ( s , a )\) where \(s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta}\) is doable because \(Q^{\pi_{\theta}} (s, a)\), \(V^{\pi_{\theta}} (s)\) and \(R ( s , a )\) are not explicit functions of \(\theta\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&quot;https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&quot;&gt;&lt;strong&gt;Theorem 1 (Policy Gradient)&lt;/strong&gt;&lt;/a&gt; allows us to approximate \(\nabla_{\theta} \mathcal{J}\) into tractable form.&lt;/p&gt;

\[\begin{align}
    \nabla_{\theta} \mathcal{J} ( \theta )
    &amp;amp; = \nabla_{\theta}
    \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s )
    \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) Q^{\pi_{\theta}} (s, a)
    = \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} }
    \left[ \nabla_{\theta} Q^{\pi_{\theta}} (s, a) \right]
    \\
    &amp;amp; \propto
    \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s )
    \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) Q^{\pi_{\theta}} (s, a) \nabla_{\theta} \pi_{\theta} (a \vert s)
    = \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} }
    \left[ Q^{\pi_{\theta}} (s, a) \nabla_{\theta} \ln \pi_{\theta} (a \vert s) \right]
\end{align}\]

&lt;p&gt;In the original paper, the theorem is proved in two different environments, &lt;em&gt;start-state episodic&lt;/em&gt; and &lt;em&gt;average-reward continuous&lt;/em&gt;, but in this post, I will stick with the episodic environment without a discount factor since it is more intuitive and blends well with previous posts.&lt;/p&gt;

&lt;p&gt;The gradient of the PG objective can be defined as:&lt;/p&gt;

\[\begin{equation}
    \nabla_{\theta} \mathcal{J} ( \theta )
    \equiv \nabla_{\theta} \mathbb{E}_{ s \sim d^{\pi_{\theta}} } \left[ V^{\pi_{\theta}} (s) \right]
    \equiv \nabla_{\theta} V^{\pi_{\theta}} (s)
    \quad
    \forall s \in \mathcal{S}
\end{equation}\]

&lt;p&gt;We can decompose \(\nabla_{\theta} V^{\pi_{\theta}} ( s )\) into:&lt;/p&gt;

\[\require{color}
\begin{align}
    \textcolor{red}{\nabla_{ \theta }} V^{ \pi_{\theta} } (s)
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} \left( \sum_{a \in \mathcal{A} } \pi_\theta (a \vert s) Q^{\pi_{\theta}} (s, a) \right)
    \\
    = &amp;amp; \sum_{a \in \mathcal{A}} \left(
    \bigl( \textcolor{red}{\nabla_{ \theta }} \pi_{\theta} (a \vert s) \bigr)
    Q^{\pi_{\theta}} (s, a)
    + \pi_{\theta} (a \vert s)
    \bigl( \textcolor{red}{\nabla_{ \theta }} Q^{\pi_{\theta}} (s, a) \bigr)
    \right)
    \\
    = &amp;amp; \sum_{a \in \mathcal{A}} \left(
    Q^{\pi_{\theta}} (s, a) \textcolor{red}{\nabla_{ \theta }} \pi_{\theta} (a \vert s)
    + \pi_{\theta} (a \vert s)
    \left( \textcolor{red}{\nabla_{ \theta }} \sum_{s' \in \mathcal{S}} p(s' \vert s, a) \textcolor{blue}{(r (s, a) + V^{\pi_{\theta}} (s'))} \right)
    \right)
    \\
    = &amp;amp; \sum_{a \in \mathcal{A}} \left(
    Q^{\pi_{\theta}} (s, a) \textcolor{red}{\nabla_{ \theta }} \pi_{\theta} (a \vert s)
    + \pi_{\theta} (a \vert s)
    \sum_{s' \in \mathcal{S}} p(s' \vert s,a) \textcolor{blue}{} \textcolor{red}{\nabla_{ \theta }} \textcolor{blue}{V^{\pi_{\theta}}(s')}
    \right)
    \\
    = &amp;amp; \sum_{a \in \mathcal{A}}
    Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \pi_{\theta} (a \vert s)
    + \sum_{a \in \mathcal{A}}
    \left( \pi_{\theta} (a \vert s) \sum_{s' \in \mathcal{S}} p(s' \vert s,a) \nabla_{ \theta} V^{\pi_{\theta}}(s') \right)
    \\
    = &amp;amp; \sum_{a \in \mathcal{A}}
    Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \pi_{\theta} (a \vert s)
    + \sum_{s' \in \mathcal{S}}
    \left( \textcolor{red}{\sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) p(s' \vert s,a)} \right)
    \nabla_{ \theta} V^{\pi_{\theta}}(s')
    \\
    = &amp;amp; \sum_{a \in \mathcal{A}}
    Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \pi_{\theta} (a \vert s)
    + \sum_{s' \in \mathcal{S}}
    \textcolor{blue}{\Pr (s \rightarrow s' \vert 1, \pi_{\theta})}
    \nabla_{ \theta} V^{\pi_{\theta}}(s')
\end{align}\]

&lt;p&gt;Where the state visitation probability is \(\textcolor{blue}{\Pr (s \rightarrow s' \vert 1, \pi_{\theta})} = \textcolor{red}{\sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) p(s' \vert s,a)}\).&lt;/p&gt;

&lt;p&gt;Then, recursively:&lt;/p&gt;

\[\require{color}
\begin{align}
    \nabla_{ \theta } V^{ \pi_{\theta} } (\textcolor{red}{s})
    = &amp;amp; \sum_{a \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{red}{s}, a) \nabla_{ \theta } \pi_{\theta} (a \vert \textcolor{red}{s})
    \\
    &amp;amp; + \sum_{\textcolor{blue}{s'} \in \mathcal{S}} \Pr (\textcolor{red}{s} \rightarrow \textcolor{blue}{s'} \vert 1, \pi_{\theta})
    \nabla_{ \theta} V^{\pi_{\theta}}(\textcolor{blue}{s'})
    \\
    = &amp;amp; \sum_{a \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{red}{s}, a) \nabla_{ \theta } \pi_{\theta} (a \vert \textcolor{red}{s})
    \\
    &amp;amp; + \sum_{\textcolor{blue}{s'} \in \mathcal{S}} \Pr (\textcolor{red}{s} \rightarrow \textcolor{blue}{s'} \vert 1, \pi_{\theta})
    \Biggl( \sum_{aâ€™ \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{blue}{s'}, aâ€™) \nabla_{ \theta } \pi_{\theta} (aâ€™ \vert \textcolor{blue}{s'})
    + \sum_{\textcolor{green}{s''} \in \mathcal{S}} \Pr (\textcolor{blue}{s'} \rightarrow \textcolor{green}{s''} \vert 1, \pi_{\theta})
    \nabla_{ \theta} V^{\pi_{\theta}}(\textcolor{green}{s''}) \Biggr)
    \\
    = &amp;amp; \sum_{a \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{red}{s}, a) \nabla_{ \theta } \pi_{\theta} (a \vert \textcolor{red}{s})
    \\
    &amp;amp; + \sum_{\textcolor{blue}{s'} \in \mathcal{S}} \Pr (\textcolor{red}{s} \rightarrow \textcolor{blue}{s'} \vert 1, \pi_{\theta})
    \sum_{aâ€™ \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{blue}{s'}, aâ€™) \nabla_{ \theta } \pi_{\theta} (aâ€™ \vert \textcolor{blue}{s'})
    \\
    &amp;amp; + \sum_{\textcolor{blue}{s'} \in \mathcal{S}} \Pr (\textcolor{red}{s} \rightarrow \textcolor{blue}{s'} \vert 1, \pi_{\theta})
    \sum_{\textcolor{green}{s''} \in \mathcal{S}} \Pr (\textcolor{blue}{s'} \rightarrow \textcolor{green}{s''} \vert 1, \pi_{\theta})
    \nabla_{ \theta} V^{\pi_{\theta}}(\textcolor{green}{s''})
    \\
    = &amp;amp; \sum_{a \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{red}{s}, a) \nabla_{ \theta } \pi_{\theta} (a \vert \textcolor{red}{s})
    \\
    &amp;amp; + \sum_{\textcolor{blue}{s'} \in \mathcal{S}} \Pr (\textcolor{red}{s} \rightarrow \textcolor{blue}{s'} \vert 1, \pi_{\theta})
    \sum_{aâ€™ \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{blue}{s'}, aâ€™) \nabla_{ \theta } \pi_{\theta} (aâ€™ \vert \textcolor{blue}{s'})
    \\
    &amp;amp; + \sum_{\textcolor{green}{s''} \in \mathcal{S}} \Pr (\textcolor{red}{s} \rightarrow \textcolor{green}{s''} \vert 2, \pi_{\theta})
    \nabla_{ \theta} V^{\pi_{\theta}}(\textcolor{green}{s''})
    \\
    = &amp;amp; \sum_{\textcolor{red}{s} \in \mathcal{S}} \Pr (\textcolor{red}{s} \rightarrow \textcolor{red}{s} \vert 0, \pi_{\theta})
    \sum_{a \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{red}{s}, a) \nabla_{ \theta } \pi_{\theta} (a \vert \textcolor{red}{s})
    \\
    &amp;amp; + \sum_{\textcolor{blue}{s'} \in \mathcal{S}} \Pr (\textcolor{red}{s} \rightarrow \textcolor{blue}{s'} \vert 1, \pi_{\theta})
    \sum_{aâ€™ \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{blue}{s'}, aâ€™) \nabla_{ \theta } \pi_{\theta} (aâ€™ \vert \textcolor{blue}{s'})
    \\
    &amp;amp; + \sum_{\textcolor{green}{s''} \in \mathcal{S}} \Pr (\textcolor{red}{s} \rightarrow \textcolor{green}{s''} \vert 2, \pi_{\theta})
    \sum_{aâ€™â€™ \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{green}{s''}, a'') \nabla_{ \theta } \pi_{\theta} (a'' \vert \textcolor{green}{s''})
    + \cdots
    \\
    = &amp;amp; \sum_{\textcolor{violet}{x} \in \mathcal{S}} \sum_{\textcolor{orange}{k} = 0}^{\infty}
    \Pr (\textcolor{red}{s} \rightarrow \textcolor{violet}{x} \vert \textcolor{orange}{k}, \pi_{\theta})
    \sum_{a \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{violet}{x}, a) \nabla_{ \theta } \pi_{\theta} (a \vert \textcolor{violet}{x})
\end{align}\]

&lt;p&gt;Finally, the update is:&lt;/p&gt;

\[\begin{align}
    \nabla_{ \theta } \mathcal{J} (\theta)
    \equiv &amp;amp; \sum_{\textcolor{red}{s_{0}} \in \mathcal{S}} p ( \textcolor{red}{s_{0}} ) \nabla_{ \theta } V^{ \pi_{\theta} } (\textcolor{red}{s_{0}})
    \\
    = &amp;amp; \sum_{\textcolor{red}{s_{0}} \in \mathcal{S}} p ( \textcolor{red}{s_{0}} )
    \sum_{\textcolor{blue}{s} \in \mathcal{S}} \sum_{\textcolor{orange}{k} = 0}^{\infty}
    \Pr (\textcolor{red}{s_{0}} \rightarrow \textcolor{blue}{s} \vert \textcolor{orange}{k}, \pi_{\theta})
    \sum_{a \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{blue}{s}, a) \nabla_{ \theta } \pi_{\theta} (a \vert \textcolor{blue}{s})
    \\
    = &amp;amp; \sum_{\textcolor{blue}{s} \in \mathcal{S}}
    \left(
    \sum_{\textcolor{red}{s_{0}} \in \mathcal{S}}
    \sum_{\textcolor{orange}{k} = 0}^{\infty}
    p ( \textcolor{red}{s_{0}} )
    \Pr (\textcolor{red}{s_{0}} \rightarrow \textcolor{blue}{s} \vert \textcolor{orange}{k}, \pi_{\theta}) \right)
    \sum_{a \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{blue}{s}, a) \nabla_{ \theta } \pi_{\theta} (a \vert \textcolor{blue}{s})
    \\
    = &amp;amp; \sum_{\textcolor{blue}{s} \in \mathcal{S}} d^{\pi_{\theta}} ( \textcolor{blue}{s} )
    \sum_{a \in \mathcal{A}} Q^{\pi_{\theta}} (\textcolor{blue}{s}, a) \nabla_{ \theta} \pi_{\theta} (a \vert \textcolor{blue}{s})
    \\
    = &amp;amp; \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \frac{ \pi_{\theta} (a \vert s) }{\pi_{\theta} (a \vert s)} Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \pi_{\theta} (a \vert s)
    \\
    = &amp;amp; \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) Q^{\pi_{\theta}} (s, a) \frac{ \nabla_{ \theta} \pi_{\theta} (a \vert s) }{\pi_{\theta} (a \vert s)}
    \\
    = &amp;amp; \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    = \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} } \left[ Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s) \right]
\end{align}\]

&lt;p&gt;Another way to view the PG objective is from the log-likelihood. The PG objective without the value function is actually the same as the objective used in the &lt;em&gt;behavioral cloning&lt;/em&gt; in imitation learning and more generally the log-likelihood in supervised learning. In the behavioral cloning, there are two factors that determine the policy:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The representation of the demonstrations in the dataset&lt;/li&gt;
  &lt;li&gt;The sampling distribution of the demonstrations from the dataset&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If the sampling distribution is skewed for the update, the agent will learn the representations of the demonstration data that are more frequently sampled. On the other hand, the PG contains the value function in its objective. This enforces the agent to learn from:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The representation of the experiences&lt;/li&gt;
  &lt;li&gt;The sampling distribution of the experiences which is determined by the exploration&lt;/li&gt;
  &lt;li&gt;The value function as the magnitude of the acquired experiences&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thus, what the value function does is that it acts as the magnitude on how important the experience is. This will be discussed in later posts.&lt;/p&gt;

&lt;h2 id=&quot;policy-gradient-with-function-approximation&quot;&gt;Policy Gradient with Function Approximation&lt;/h2&gt;

&lt;p&gt;The use of the real Q value results in high variance since it uses MC to construct the Q value. To reduce this variance, we can use another function approximator for Q value with parameter \(\phi\), where \(Q_{\phi} : \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\), under the &lt;a href=&quot;https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&quot;&gt;&lt;strong&gt;Theorem 2 (Policy Gradient with Function Approximation)&lt;/strong&gt;&lt;/a&gt;, which deals with two conditions:&lt;/p&gt;

\[\begin{equation}
    \nabla_{ \phi } Q_{ \phi } (s, a) = \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    \qquad
    \nabla_{ \phi } \mathcal{J} ( \phi )
    = \mathbb{E}_{ s \sim d^{\pi_{\theta}} , a \sim \pi_{\theta} } \left[
    \left( Q_{\phi} (s, a) - Q^{\pi_{\theta}} (s, a) \right)^{2}
    \right]
    \leq
    \varepsilon
\end{equation}\]

&lt;p&gt;Where \(\varepsilon\) is a small real number.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The first condition is that the gradient of the estimated Q value, \(\nabla_{ \phi } Q_{ \phi } (s, a)\), must be equal to \(\nabla_{ \theta} \ln \pi_{\theta} (a \vert s)\).&lt;/li&gt;
  &lt;li&gt;The second is that \(Q_{\phi} (s, a)\) must be optimized with the mean squared error with respect to \(Q^{\pi_{\theta}} (s, a)\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is actually outside of the scope of the PG since this introduces the value-based method into the policy-based method. However, since the original paper proves this, here it goes. The objective of the Q value can be expressed as:&lt;/p&gt;

\[\begin{align}
    \nabla_{ \phi } \mathcal{J} ( \phi )
    = &amp;amp; \nabla_{ \phi } \mathbb{E}_{ s \sim d^{\pi_{\theta}} , a \sim \pi_{\theta} } \left[
    \left( Q_{ \phi } (s, a) - Q^{\pi_{\theta}} (s, a) \right)^{2}
    \right]
    = \nabla_{ \phi } \varepsilon= 0
    \\
    = &amp;amp; \nabla_{ \phi } \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s)
    \left( Q_{ \phi } (s, a) - Q^{\pi_{\theta}} (s, a) \right)^{2}
    \\
    = &amp;amp; \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s)
    \cdot 2 \left( Q_{ \phi } (s, a) - Q^{\pi_{\theta}} (s, a) \right) \nabla_{ \phi } Q_{ \phi } (s, a)
    \\
    \propto &amp;amp; \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s)
    \left( Q_{ \phi } (s, a) - Q^{\pi_{\theta}} (s, a) \right) \nabla_{ \phi } Q_{ \phi } (s, a)
    \quad
    \nabla_{ \phi } Q_{ \phi } (s, a) = \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    \\
    = &amp;amp; \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s)
    \left( Q_{ \phi } (s, a) - Q^{\pi_{\theta}} (s, a) \right) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    \\
    = &amp;amp; \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s)
    Q_{ \phi } (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    - \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s)
    Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s) = 0
\end{align}\]

&lt;p&gt;This means:&lt;/p&gt;

\[\begin{align}
    \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s)
    Q_{ \phi } (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    &amp;amp; = \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s)
    Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    \\
    \mathbb{E}_{ s \sim d^{\pi_{\theta}} , a \sim \pi_{\theta} } \left[
    Q_{ \phi } (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    \right]
    &amp;amp; = \mathbb{E}_{ s \sim d^{\pi_{\theta}} , a \sim \pi_{\theta} } \left[
    Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    \right]
\end{align}\]

&lt;p&gt;Therefore, 1) if \(\nabla_{ \phi } Q_{ \phi } (s, a) = \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)\) holds and 2) if the function approximation of \(Q_{ \phi }\) is optimized with the mean squared error with \(Q^{\pi_{\theta}}\), we can set the objective function as:&lt;/p&gt;

\[\begin{equation}
    \nabla_{ \theta } \mathcal{J} (\theta)
    \propto \mathbb{E}_{ s \sim d^{\pi_{\theta}} , a \sim \pi_{\theta} } \left[
    Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    \right]
    \equiv \mathbb{E}_{ s \sim d^{\pi_{\theta}} , a \sim \pi_{\theta} } \left[
    Q_{ \phi } (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    \right]
\end{equation}\]

&lt;p&gt;In traditional PG, the policy distribution is constructed with parameter, \(\theta\), and a feature vector of state-action pair, \(\mathbf{x} ( s , a )\), where \(\theta, \mathbf{x} ( s , a ) \in \mathbb{R}^{n}\).&lt;/p&gt;

&lt;p&gt;Consider a cartpole problem with two state features, \(( x_{p} , x_{v} )\) as (Position, Velocity), and two actions, \(( a_{l} , a_{r} )\) as (Left, Right). The simplest form of \(\mathbf{x} ( s , a )\) can be expressed as:&lt;/p&gt;

\[\begin{equation}
    \mathbf{x} ( s , a = a_{l} ) = \begin{bmatrix} x_{p} &amp;amp; x_{v} &amp;amp; 0 &amp;amp; 0 \end{bmatrix}
    \qquad
    \mathbf{x} ( s , a = a_{r} ) = \begin{bmatrix} 0 &amp;amp; 0 &amp;amp; x_{p} &amp;amp; x_{v} \end{bmatrix}
    \qquad
    \theta = \begin{bmatrix} \theta_{pl} &amp;amp; \theta_{vl} &amp;amp; \theta_{pr} &amp;amp; \theta_{vr} \end{bmatrix}^{\intercal}
\end{equation}\]

&lt;p&gt;For a given \(\theta\), parameterized policy will be:&lt;/p&gt;

\[\begin{equation}
    \pi_{\theta} (a_{l} \vert s) = \mathbf{x} ( s , a = a_{l} ) \cdot \theta
    \qquad
    \pi_{\theta} (a_{r} \vert s) = \mathbf{x} ( s , a = a_{r} ) \cdot \theta
\end{equation}\]

&lt;p&gt;Thus, the first two values of \(\theta\) learn to map features to \(a_{l}\) and the last two map features to \(a_{r}\). However, \(\theta\) and \(\mathbf{x}\) are the design choice, so based on the problems and algorithms, they vary.&lt;/p&gt;

&lt;p&gt;There are two types of policy distribution that meet the two conditions in the &lt;a href=&quot;https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&quot;&gt;&lt;strong&gt;Theorem 2 (Policy Gradient with Function Approximation)&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Softmax policy distribution&lt;/strong&gt;: A Gibbs distribution in a linear combination of features, a.k.a. softmax policy distribution, to produce action probability for discrete action space.&lt;/p&gt;

\[\begin{equation}
      \pi_{\theta} (a \vert s)
      = \text{Softmax} ( e^{ \mathbf{x} ( s , a ) \cdot \theta } )
      = \frac{ e^{ \mathbf{x} ( s , a ) \cdot \theta } }{ \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } }
  \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;Where softmax signifies the normalization of \(e^{ \mathbf{x} ( s , a ) \cdot \theta }\) across all the actions. To meet the condition in the &lt;a href=&quot;https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&quot;&gt;&lt;strong&gt;Theorem 2 (Policy Gradient with Function Approximation)&lt;/strong&gt;&lt;/a&gt;:&lt;/li&gt;
    &lt;/ul&gt;

\[\require{color}
  \begin{align}
      \nabla_{ \phi } Q_{ \phi } (s, a)
      &amp;amp; = \nabla_{ \theta } \ln \pi_{\theta} (a \vert s)
      = \frac{ \nabla_{ \theta } \pi_{\theta} (a \vert s) }{\pi_{\theta} (a \vert s)}
      \\
      &amp;amp; = \frac{ 1 }{ \pi_{\theta} (a \vert s) }
      \nabla_{ \theta }
      \textcolor{green}{ \frac{ e^{ \mathbf{x} ( s , a ) \cdot \theta } }
      { \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } } }
      \\
      &amp;amp; = \frac{ 1 }{\pi_{\theta} (a \vert s)}
      \frac{ \left( \nabla_{ \theta } \textcolor{blue}{ e^{ \mathbf{x} ( s , a ) \cdot \theta } } \right)
      \textcolor{red}{ \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } }
      - \textcolor{blue}{ e^{ \mathbf{x} ( s , a ) \cdot \theta } }
      \left( \nabla_{ \theta } \textcolor{red}{ \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } } \right) }
      { \left( \textcolor{red}{ \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } } \right)^{2} }
      \\
      &amp;amp; = \frac{ 1 }{\pi_{\theta} (a \vert s)}
      \left(
      \frac{ \mathbf{x} ( s , a ) \textcolor{blue}{ e^{ \mathbf{x} ( s , a ) \cdot \theta } }
      \textcolor{red}{ \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } } }
      { \left( \textcolor{red}{ \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } } \right)^{2} }
      - \frac{ \textcolor{blue}{ e^{ \mathbf{x} ( s , a ) \cdot \theta } }
      \sum_{ a' \in \mathcal{A} } \mathbf{x} ( s , a' ) e^{ \mathbf{x} ( s , a' ) \cdot \theta } }
      { \left( \textcolor{red}{ \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } } \right)^{2} }
      \right)
      \\
      &amp;amp; = \frac{ 1 }{\pi_{\theta} (a \vert s)}
      \left(
      \mathbf{x} ( s , a )
      \textcolor{green}{ \frac{ e^{ \mathbf{x} ( s , a ) \cdot \theta } }
      { \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } } }
      - \textcolor{green}{ \frac{ e^{ \mathbf{x} ( s , a ) \cdot \theta } }
      { \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } } }
      \cdot \frac{ \sum_{ a' \in \mathcal{A} } \mathbf{x} ( s , a' ) e^{ \mathbf{x} ( s , a' ) \cdot \theta } }
      { \textcolor{red}{ \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } } }
      \right)
      \\
      &amp;amp; = \frac{ 1 }{ \pi_{\theta} (a \vert s) }
      \left( \mathbf{x} ( s , a ) \cdot \pi_{\theta} (a \vert s)
      - \pi_{\theta} (a \vert s) \sum_{ a'' \in \mathcal{A} } \mathbf{x} ( s , a'' )
      \textcolor{green}{ \frac{ e^{ \theta \cdot \mathbf{x} ( s , a'' ) } }
      { \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } } }
      \right)
      \\
      &amp;amp; = \mathbf{x} ( s , a ) - \sum_{ a' \in \mathcal{A} } \mathbf{x} ( s , a' ) \pi_{\theta} (a' \vert s)
  \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;Where \(\theta, \mathbf{x} \in \mathbb{R}^{n}\). Thus, Q value is parameterized with \(\phi \in \mathbb{R}^{n}\).&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      Q_{ \phi } (s, a)
      = \left( \mathbf{x} ( s , a ) - \sum_{ a' \in \mathcal{A} } \mathbf{x} ( s , a' ) \pi_{\theta} (a' \vert s) \right) \cdot \phi
  \end{equation}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Gaussian policy distribution&lt;/strong&gt;: A Gaussian distribution with a parameterized mean and a fixed standard deviation for continuous action space.&lt;/p&gt;

\[\begin{equation}
      \pi_{\theta} (a \vert s) = \mathcal{N} ( \mu ( s , a ) , \sigma^{2} )
      \quad
      \mu ( s , a ) = \mathbf{x} ( s , a ) \cdot \theta
  \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;Where \(\mu\) is the parameterized mean of Gaussian distribution and \(\sigma\) is the fixed standard deviation. To meet the condition in the &lt;a href=&quot;https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&quot;&gt;&lt;strong&gt;Theorem 2 (Policy Gradient with Function Approximation)&lt;/strong&gt;&lt;/a&gt;:&lt;/li&gt;
    &lt;/ul&gt;

\[\require{color}
  \begin{align}
      \nabla_{ \phi } Q_{ \phi } (s, a)
      &amp;amp; = \nabla_{ \theta } \ln \pi_{\theta} (a \vert s)
      = \frac{ \nabla_{ \theta } \pi_{\theta} (a \vert s) }{\pi_{\theta} (a \vert s)}
      \\
      &amp;amp; = \frac{ 1 }{\pi_{\theta} (a \vert s)} \nabla_{ \theta } \mathcal{N} ( \mu ( s , a ) , \sigma^{2} )
      = \frac{ 1 }{\pi_{\theta} (a \vert s)}
      \left( \nabla_{ \theta } \frac{1}{ \sigma \sqrt{ 2 \pi } }
      e^{ - \frac{ ( a - \mu ( s , a ) )^{2} }{ 2 \sigma^{2} } } \right)
      \\
      &amp;amp; = \frac{ 1 }{\pi_{\theta} (a \vert s)}
      \left( \frac{1}{ \sigma \sqrt{ 2 \pi } }
      e^{ - \frac{ ( a - \mu ( s , a ) )^{2} }{ 2 \sigma^{2} } } \right)
      - \frac{ \nabla_{ \theta } ( a - \mu ( s , a ) )^{2} }{ 2 \sigma^{2} }
      \\
      &amp;amp; = \frac{ 1 }{\pi_{\theta} (a \vert s)} \mathcal{N} ( \mu ( s , a ) , \sigma^{2} )
      - \frac{ -2 ( a - \mu ( s , a ) ) \nabla_{ \theta } \mu ( s , a ) }{ 2 \sigma^{2} }
      \\
      &amp;amp; = \frac{ 1 }{\pi_{\theta} (a \vert s)} \pi_{\theta} (a \vert s)
      + \frac{ ( a - \mu ( s , a ) ) \mathbf{x} ( s , a ) }{ \sigma^{2} }
      \\
      &amp;amp; = \mathbf{x} ( s , a ) \frac{ a - \mu ( s , a ) }{ \sigma^{2} }
  \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;Where \(\theta, \mathbf{x} \in \mathbb{R}^{n}\). Thus, Q value is parameterized with \(\phi \in \mathbb{R}^{n}\).&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      Q_{ \phi } (s, a)
      = \left( \mathbf{x} ( s , a ) \frac{ a - \mu ( s , a ) }{ \sigma^{2} } \right) \cdot \phi
  \end{equation}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In both cases, the agent samples action from the policy distribution \(a \sim \pi_{\theta} (a \vert s)\).&lt;/p&gt;

&lt;h1 id=&quot;deterministic-policy-gradient&quot;&gt;Deterministic Policy Gradient&lt;/h1&gt;

&lt;p&gt;The standard stochastic PG with the continuous action space requires integrating over both state and action spaces.&lt;/p&gt;

\[\begin{equation}
    \nabla_{ \theta } \mathcal{J} (\theta)
    \propto \int_{\mathcal{S}} d^{\pi_{\theta}} ( s )
    \int_{\mathcal{A}} \pi_{\theta} (a \vert s) Q^{\pi_{\theta}} (s, a)
    \nabla_{ \theta} \ln \pi_{\theta} (a \vert s) \text{ d}a \text{ d}s
    = \mathbb{E}_{ s \sim d^{\pi_{\theta}} , a \sim \pi_{\theta} } \left[
    Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
    \right]
\end{equation}\]

&lt;p&gt;To increase the computational efficiency, the &lt;strong&gt;deterministic policy gradient (DPG)&lt;/strong&gt; is proposed, where the policy is no longer distribution, but just value in the continuous action space. Thus, instead of forming the Gaussian policy to sample actions, \(a \sim \mathcal{N} ( \mu ( s ) , \sigma^{2} )\), we treat the policy as the action, \(a = \mu ( s )\), or the greedy policy in the continuous action space. This allows the objective to be formalized to only integrate over the state space:&lt;/p&gt;

\[\begin{align}
    \mathcal{J} ( \theta )
    &amp;amp; = \mathbb{E}_{ s \sim d^{\mu_{\theta}} } [ R ( s , \mu_{\theta} ( s ) ) ]
    = \int_{\mathcal{S}} d^{\mu_{\theta}} ( s ) R ( s , \mu_{\theta} ( s ) ) \text{ d}s
    \\
    &amp;amp; \equiv \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[ Q^{\mu_{\theta}} (s, \mu_{\theta} ( s )) \right]
    = \int_{\mathcal{S}} d^{\mu_{\theta}} ( s ) Q^{\mu_{\theta}} (s, \mu_{\theta} ( s )) \text{ d}s
    \\
    &amp;amp; \equiv \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[ V^{\mu_{\theta}} (s) \right]
    = \int_{\mathcal{S}} d^{\mu_{\theta}} ( s ) V^{\mu_{\theta}} (s) \text{ d}s
    \\
    &amp;amp; \equiv \mathbb{E}_{ s_{0} \sim p_{0} } \left[ V^{\mu_{\theta}} (s_{0}) \right]
    = \int_{\mathcal{S}} p_{0} ( s_{0} ) V^{\mu_{\theta}} (s_{0}) \text{ d}s_{0}
\end{align}\]

&lt;p&gt;Few things to note are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The reason why the DPG is more computationally efficient is &lt;em&gt;because the computation is independent of the action space&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;To avoid deficient exploration, the DPG is often implemented as off-policy with the behaviour policy, \(a \sim \beta ( a \vert s ) = \mathcal{N} ( \mu ( s ) , \sigma^{2} )\).
    &lt;ul&gt;
      &lt;li&gt;In other words, the DPG updates the deterministic policy with the trajectories collected by the stochastic policy.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The original paper used \(\mu ( s )\) notation instead of \(\mu ( s , a )\) to generalize its applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;deterministic-policy-gradient-theorem&quot;&gt;Deterministic Policy Gradient Theorem&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;http://proceedings.mlr.press/v32/silver14.pdf&quot;&gt;&lt;strong&gt;Theorem 1 (Deterministic Policy Gradient Theorem)&lt;/strong&gt;&lt;/a&gt; allows us to approximate \(\nabla_{\theta} \mathcal{J}\) into tractable form.&lt;/p&gt;

\[\begin{align}
    \nabla_{\theta} \mathcal{J} ( \theta )
    &amp;amp; = \nabla_{\theta}
    \int_{\mathcal{S}} d^{\mu_{\theta}} ( s ) Q^{\mu_{\theta}} (s, \mu_{\theta} ( s )) \text{ d}s
    = \mathbb{E}_{ s \sim d^{\mu_{\theta}} }
    \left[ \nabla_{\theta} Q^{\mu_{\theta}} (s, \mu_{\theta} ( s )) \right]
    \\
    &amp;amp; \propto \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
    \nabla_{ \theta} \mu_{\theta} ( s )
    \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )} \text{ d}s
    = \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[
    \nabla_{ \theta} \mu_{\theta} ( s )
    \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}
    \right]
\end{align}\]

&lt;p&gt;Of course, you can simply apply the chain rule, \(\nabla_{\theta} Q^{\mu_{\theta}} (s, \mu_{\theta} ( s )) = \nabla_{ \theta} \mu_{\theta} ( s ) \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}\), but the original paper proves this in a similar manner to the PG paper.&lt;/p&gt;

&lt;p&gt;The gradient of the DPG objective can be defined as:&lt;/p&gt;

\[\begin{equation}
    \nabla_{\theta} \mathcal{J} ( \theta )
    \equiv \nabla_{\theta} \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[ V^{\mu_{\theta}} (s) \right]
    \equiv \nabla_{\theta} V^{\mu_{\theta}} (s)
    \quad
    \forall s \in \mathcal{S}
\end{equation}\]

&lt;p&gt;We can decompose \(\nabla_{\theta} V^{\mu_{\theta}} ( s )\) into:&lt;/p&gt;

\[\begin{align}
    \textcolor{red}{\nabla_{ \theta }} V^{ \mu_{\theta} } (s)
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} Q^{\mu_{\theta}} (s, \mu_{\theta} (s))
    \\
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} \left(
    \int_{\mathcal{S}} p(s' \vert s, \mu_{\theta} ( s )) \left(
    r ( s , \mu_{\theta} ( s ) ) + V^{\mu_{\theta}} (s')
    \right) \text{ d}s'
    \right)
    \\
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} r ( s , \mu_{\theta} ( s ) )
    + \textcolor{red}{\nabla_{ \theta }}
    \int_{\mathcal{S}} p(s' \vert s, \mu_{\theta} ( s )) V^{\mu_{\theta}} (s') \text{ d}s'
    \\
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s ) \textcolor{blue}{\nabla_{ a }} r ( s , a ) \vert_{a = \mu_{\theta} ( s )}
    \\
    &amp;amp; + \int_{\mathcal{S}}
    \left( \textcolor{red}{\nabla_{ \theta }} p(s' \vert s, \mu_{\theta} ( s )) \right) V^{\mu_{\theta}} (s')
    + p(s' \vert s, \mu_{\theta} ( s )) \left( \textcolor{red}{\nabla_{ \theta }} V^{\mu_{\theta}} (s') \right)
    \text{ d}s'
    \\
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s ) \textcolor{blue}{\nabla_{ a }} r ( s , a ) \vert_{a = \mu_{\theta} ( s )}
    \\
    &amp;amp; + \int_{\mathcal{S}}
    V^{\mu_{\theta}} (s') \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s ) \textcolor{blue}{\nabla_{ a }} p(s' \vert s, a) \vert_{a = \mu_{\theta} ( s )}
    + p(s' \vert s, \mu_{\theta} ( s )) \textcolor{red}{\nabla_{ \theta }} V^{\mu_{\theta}} (s')
    \text{ d}s'
    \\
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s ) \textcolor{blue}{\nabla_{ a }}
    \left. \left(
    r ( s , a )
    + \int_{\mathcal{S}}
    V^{\mu_{\theta}} (s') p(s' \vert s, a)
    \text{ d}s'
    \right) \right\vert_{a = \mu_{\theta} ( s )}
    + \int_{\mathcal{S}}
    p(s' \vert s, \mu_{\theta} ( s )) \textcolor{red}{\nabla_{ \theta }} V^{\mu_{\theta}} (s')
    \text{ d}s'
    \\
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s ) \textcolor{blue}{\nabla_{ a }}
    Q^{ \mu_{\theta} } ( s , a ) \vert_{a = \mu_{\theta} ( s )}
    + \int_{\mathcal{S}}
    \Pr (s \rightarrow s' \vert 1 , \mu_{\theta} ( s )) \textcolor{red}{\nabla_{ \theta }} V^{\mu_{\theta}} (s')
    \text{ d}s'
\end{align}\]

&lt;p&gt;Then, recursively:&lt;/p&gt;

\[\begin{align}
    \textcolor{red}{\nabla_{ \theta }} V^{ \mu_{\theta} } (s)
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s )
    \textcolor{blue}{\nabla_{ a }} Q^{ \mu_{\theta} } ( s , a ) \vert_{a = \mu_{\theta} ( s )}
    \\
    &amp;amp; + \int_{\mathcal{S}}
    \Pr (s \rightarrow s' \vert 1 , \mu_{\theta})
    \textcolor{red}{\nabla_{ \theta }} V^{\mu_{\theta}} (s')
    \text{ d}s'
    \\
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s )
    \textcolor{blue}{\nabla_{ a }} Q^{ \mu_{\theta} } ( s , a ) \vert_{a = \mu_{\theta} ( s )}
    \\
    &amp;amp; + \int_{\mathcal{S}}
    \Pr (s \rightarrow s' \vert 1 , \mu_{\theta})
    \left(
    \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s' )
    \textcolor{blue}{\nabla_{ a' }} Q^{ \mu_{\theta} } ( s' , a' ) \vert_{a' = \mu_{\theta} ( s' )}
    + \int_{\mathcal{S}} \Pr (s' \rightarrow s'' \vert 1 , \mu_{\theta})
    \textcolor{red}{\nabla_{ \theta }} V^{\mu_{\theta}} (s'')
    \text{ d}s''
    \right) \text{ d}s'
    \\
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s )
    \textcolor{blue}{\nabla_{ a }} Q^{ \mu_{\theta} } ( s , a ) \vert_{a = \mu_{\theta} ( s )}
    \\
    &amp;amp; + \int_{\mathcal{S}}
    \Pr (s \rightarrow s' \vert 1 , \mu_{\theta})
    \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s' )
    \textcolor{blue}{\nabla_{ a' }} Q^{ \mu_{\theta} } ( s' , a' ) \vert_{a' = \mu_{\theta} ( s' )} \text{ d}s'
    \\
    &amp;amp; + \int_{\mathcal{S}}
    \Pr (s \rightarrow s' \vert 1 , \mu_{\theta}) \int_{\mathcal{S}}
    \Pr (s' \rightarrow s'' \vert 1 , \mu_{\theta})
    \textcolor{red}{\nabla_{ \theta }} V^{\mu_{\theta}} (s'')
    \text{ d}s'' \text{ d}s'
    \\
    = &amp;amp; \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s )
    \textcolor{blue}{\nabla_{ a }} Q^{ \mu_{\theta} } ( s , a ) \vert_{a = \mu_{\theta} ( s )}
    \\
    &amp;amp; + \int_{\mathcal{S}}
    \Pr (s \rightarrow s' \vert 1 , \mu_{\theta})
    \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s' )
    \textcolor{blue}{\nabla_{ a' }} Q^{ \mu_{\theta} } ( s' , a' ) \vert_{a' = \mu_{\theta} ( s' )} \text{ d}s'
    \\
    &amp;amp; + \int_{\mathcal{S}}
    \Pr (s \rightarrow s'' \vert 2 , \mu_{\theta})
    \textcolor{red}{\nabla_{ \theta }} V^{\mu_{\theta}} (s'')
    \text{ d}s''
    \\
    = &amp;amp; \int_{\mathcal{S}}
    \Pr (s \rightarrow s \vert 0 , \mu_{\theta})
    \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s )
    \textcolor{blue}{\nabla_{ a }} Q^{ \mu_{\theta} } ( s , a ) \vert_{a = \mu_{\theta} ( s )} \text{ d}s
    \\
    &amp;amp; + \int_{\mathcal{S}}
    \Pr (s \rightarrow s' \vert 1 , \mu_{\theta})
    \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s' )
    \textcolor{blue}{\nabla_{ a' }} Q^{ \mu_{\theta} } ( s' , a' ) \vert_{a' = \mu_{\theta} ( s' )} \text{ d}s'
    \\
    &amp;amp; + \int_{\mathcal{S}}
    \Pr (s \rightarrow s'' \vert 2 , \mu_{\theta})
    \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( s'' )
    \textcolor{blue}{\nabla_{ a'' }} Q^{ \mu_{\theta} } ( s'' , a'' ) \vert_{a'' = \mu_{\theta} ( s'' )} \text{ d}s'' + \cdots
    \\
    = &amp;amp; \int_{\mathcal{S}}
    \sum_{k=0}^{\infty} \Pr (s \rightarrow x \vert k , \mu_{\theta})
    \textcolor{red}{\nabla_{ \theta }} \mu_{\theta} ( x ) \textcolor{blue}{\nabla_{ a }} Q^{ \mu_{\theta} } ( x , a ) \vert_{a = \mu_{\theta} ( x )} \text{ d}x
\end{align}\]

&lt;p&gt;Finally, the update is:&lt;/p&gt;

\[\begin{align}
    \nabla_{ \theta } \mathcal{J} ( \theta )
    = &amp;amp; \int_{\mathcal{S}} p ( s_{0} ) \nabla_{ \theta } V^{ \mu_{\theta} } ( s_{0} ) \text{ d}s_{0}
    \\
    = &amp;amp; \int_{\mathcal{S}} p ( s_{0} )
    \int_{\mathcal{S}}
    \sum_{k=0}^{\infty} \Pr (s_{0} \rightarrow s \vert k , \mu_{\theta})
    \nabla_{ \theta } \mu_{\theta} ( s ) \nabla_{ a } Q^{ \mu_{\theta} } ( s , a ) \vert_{a = \mu_{\theta} ( s )} \text{ d}s
    \text{ d}s_{0}
    \\
    = &amp;amp; \int_{\mathcal{S}}
    \left( \int_{\mathcal{S}}
    \sum_{k=0}^{\infty} p ( s_{0} ) \Pr (s_{0} \rightarrow s \vert k , \mu_{\theta}) \text{ d}s_{0} \right)
    \nabla_{ \theta } \mu_{\theta} ( s ) \nabla_{ a } Q^{ \mu_{\theta} } ( s , a ) \vert_{a = \mu_{\theta} ( s )} \text{ d}s
    \\
    = &amp;amp; \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
    \nabla_{ \theta } \mu_{\theta} ( s ) \nabla_{ a } Q^{ \mu_{\theta} } ( s , a ) \vert_{a = \mu_{\theta} ( s )} \text{ d}s
    = \mathbb{E}_{ s \sim d^{\mu_{\theta}} ( s ) } \left[
    \nabla_{ \theta} \mu_{\theta} ( s )
    \nabla_{ a } Q^{ \mu_{\theta} } (s, a) \vert_{a = \mu_{\theta} ( s )}
    \right]
\end{align}\]

&lt;p&gt;The DPG can be seen as the PG with zero standard deviation.&lt;/p&gt;

\[\begin{equation}
    \lim_{\sigma \rightarrow 0}
    \nabla_{\theta} \mathcal{J} ( \pi_{ \mu_{\theta}, \sigma } ) = \nabla_{\theta} \mathcal{J} ( \mu_{\theta} )
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;This is the &lt;a href=&quot;http://proceedings.mlr.press/v32/silver14.pdf&quot;&gt;&lt;strong&gt;Theorem 2 (Limit of the Stochastic Policy Gradient)&lt;/strong&gt;&lt;/a&gt;, which is not proven in this post.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;testing&lt;/p&gt;

&lt;h2 id=&quot;compatible-function-approximation&quot;&gt;Compatible Function Approximation&lt;/h2&gt;

&lt;p&gt;Similar to the PG, the DPG also provides a proof of the compatibility of a function approximator for the Q value under the &lt;a href=&quot;http://proceedings.mlr.press/v32/silver14.pdf&quot;&gt;&lt;strong&gt;Theorem 3 (Compatible Function Approximation)&lt;/strong&gt;&lt;/a&gt;. In the DPG, we use function approximator for \(\nabla_{ a } Q_{ \phi } (s, a) \vert_{a = \mu_{\theta} ( s )}\) instead of \(Q_{\phi} (s, a)\). The two conditions are:&lt;/p&gt;

\[\begin{equation}
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} = \nabla_{ \theta} \mu_{\theta} ( s ) \cdot \phi
    \qquad
    \mathcal{J} ( \phi )
    = \mathbb{E}_{ s \sim d^{\mu_{\theta}}} \left[ \left(
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )}
    - \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}
    \right)^{2} \right]
    \leq
    \varepsilon
\end{equation}\]

&lt;p&gt;Where \(\varepsilon\) is a small real number.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The first condition is that the gradient of the estimated Q value for the greedy policy, \(\nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )}\), must be equal to \(\nabla_{ \theta} \mu_{\theta} ( s ) \cdot \phi\).&lt;/li&gt;
  &lt;li&gt;The second is that \(\nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )}\) must be optimized with the mean squared error with respect to \(\nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since a function approximator does not estimate the Q value, this is not really the value-based method, but rather a unique method that the DPG has. The objective of the gradient of the Q value can be expressed as:&lt;/p&gt;

\[\begin{align}
    \nabla_{ \phi } \mathcal{J} ( \phi )
    = &amp;amp; \nabla_{ \phi } \mathbb{E}_{ s \sim d^{\mu_{\theta}}} \left[ \left(
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )}
    - \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}
    \right)^{2} \right]
    = \nabla_{ \phi } \varepsilon = 0
    \\
    = &amp;amp; \nabla_{ \phi } \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
    \left(
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )}
    - \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}
    \right)^{2} \text{ d}s
    \\
    = &amp;amp; \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
    \cdot 2 \left(
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )}
    - \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}
    \right) \nabla_{ \phi } \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} \text{ d}s
    \\
    \propto &amp;amp; \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
    \left(
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )}
    - \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}
    \right) \nabla_{ \phi } \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} \text{ d}s
    \\
    &amp;amp; \begin{aligned}
        \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad
        \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} = \nabla_{ \theta} \mu_{\theta} ( s ) \cdot \phi
    \end{aligned}
    \\
    = &amp;amp; \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
    \left(
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )}
    - \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}
    \right) \nabla_{ \theta} \mu_{\theta} ( s ) \text{ d}s
    \\
    = &amp;amp; \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} \nabla_{ \theta} \mu_{\theta} ( s ) \text{ d}s
    - \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
    \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )} \nabla_{ \theta} \mu_{\theta} ( s ) \text{ d}s
\end{align}\]

&lt;p&gt;This means,&lt;/p&gt;

\[\begin{align}
    \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} \nabla_{ \theta} \mu_{\theta} ( s ) \text{ d}s
    &amp;amp; = \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
    \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )} \nabla_{ \theta} \mu_{\theta} ( s ) \text{ d}s
    \\
    \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} \nabla_{ \theta} \mu_{\theta} ( s )
    \right]
    &amp;amp; = \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[
    \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )} \nabla_{ \theta} \mu_{\theta} ( s )
    \right]
\end{align}\]

&lt;p&gt;Therefore, 1) if \(\nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} = \nabla_{ \theta} \mu_{\theta} ( s ) \cdot \phi\) holds and 2) if the function approximation of \(\nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )}\) is optimized with the mean squared error with \(\nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}\), we can set the objective function as:&lt;/p&gt;

\[\begin{equation}
    \nabla_{ \theta } \mathcal{J} (\theta)
    \propto \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[
    \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )} \nabla_{ \theta} \mu_{\theta} ( s )
    \right]
    \equiv \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[
    \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} \nabla_{ \theta} \mu_{\theta} ( s )
    \right]
\end{equation}\]

&lt;h1 id=&quot;practical-algorithm&quot;&gt;Practical Algorithm&lt;/h1&gt;

&lt;p&gt;Although the PG can be practically implemented, it is often that we divide it from practical algorithms.&lt;/p&gt;

&lt;h2 id=&quot;reinforce&quot;&gt;REINFORCE&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;REINFORCE&lt;/strong&gt; is the earliest practical PG algorithm proposed from &lt;a href=&quot;https://link.springer.com/article/10.1007/BF00992696&quot;&gt;Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning&lt;/a&gt;. It is an on-policy stochastic PG algorithm that uses a raw return for the update per every episode.&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 650px&quot;&gt;
    &lt;img src=&quot;/assets/machine_learning/decision_2_policy_based_reinforcement_learning/1_reinforce_pseudocode.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        REINFORCE pseudocode from
        &lt;a href=&quot;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&quot;&gt;
            Reinforcement Learning: An Introduction
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Where \(G_{t}\) is the return at \(t\), so for every \(t\), compute \(G_{t}\) for the update.&lt;/p&gt;

&lt;h2 id=&quot;actor-critic&quot;&gt;Actor Critic&lt;/h2&gt;

&lt;p&gt;Since the PG uses MC, it suffers from high variance. To remedy this, the &lt;strong&gt;actot critic (AC)&lt;/strong&gt; is introduced, which employs a bootstrapping strategy in the PG. The AC parameterizes both the value function and policy, and 1) update the value functions with a bootstrapping to estimate the true value functions and 2) update the policy in respect to the estimated Q value to acquire higher value functions. The name is originated from the fact that the value function critizes (&lt;em&gt;critic&lt;/em&gt;) the action selection from the policy (&lt;em&gt;actor&lt;/em&gt;).&lt;/p&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 650px&quot;&gt;
    &lt;img src=&quot;/assets/machine_learning/decision_2_policy_based_reinforcement_learning/2_ac_pseudocode.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Actor critic pseudocode from
        &lt;a href=&quot;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&quot;&gt;
            Reinforcement Learning: An Introduction
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Where the &lt;em&gt;temporal difference (TD) residual&lt;/em&gt; is used to compute \(\delta\). Unlike REINFORCE, one-step AC updates the parameters every time step.&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;
&lt;p&gt;The PG is an on-policy RL algorithm that finds the optimal policy using the MC method. Unlike value-based methods, the PG parameterizes the policy with a function approximator such that maximizes the return. The PG objective function has many equivalent expressions.&lt;/p&gt;

\[\begin{align}
    \mathcal{J} ( \theta )
    &amp;amp; = \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} } [ R ( s , a ) ]
    = \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) R ( s , a )
    \\
    &amp;amp; \equiv \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} } \left[ Q^{\pi_{\theta}} (s, a) \right]
    = \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) Q^{\pi_{\theta}} (s, a)
    \\
    &amp;amp; \equiv \mathbb{E}_{ s \sim d^{\pi_{\theta}} } \left[ V^{\pi_{\theta}} (s) \right]
    = \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s ) V^{\pi_{\theta}} (s)
    \\
    &amp;amp; \equiv \mathbb{E}_{ s_{0} \sim p_{0} } \left[ V^{\pi_{\theta}} (s_{0}) \right]
    = \sum_{s_{0} \in \mathcal{S}} p_{0} ( s_{0} ) V^{\pi_{\theta}} (s_{0})
\end{align}\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&quot;&gt;&lt;strong&gt;Theorem 1 (Policy Gradient)&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;

\[\begin{equation}
      \nabla_{\theta} \mathcal{J} ( \theta )
      \propto
      \sum_{s \in \mathcal{S}} d^{\pi_{\theta}} ( s )
      \sum_{a \in \mathcal{A}} \pi_{\theta} (a \vert s) Q^{\pi_{\theta}} (s, a) \nabla_{\theta} \pi_{\theta} (a \vert s)
      = \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} }
      \left[ Q^{\pi_{\theta}} (s, a) \nabla_{\theta} \ln \pi_{\theta} (a \vert s) \right]
  \end{equation}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&quot;&gt;&lt;strong&gt;Theorem 2 (Policy Gradient with Function Approximation)&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;

\[\begin{equation}
      \nabla_{ \theta } \mathcal{J} (\theta)
      \propto \mathbb{E}_{ s \sim d^{\pi_{\theta}} , a \sim \pi_{\theta} } \left[
      Q^{\pi_{\theta}} (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
      \right]
      \equiv \mathbb{E}_{ s \sim d^{\pi_{\theta}} , a \sim \pi_{\theta} } \left[
      Q_{ \phi } (s, a) \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
      \right]
  \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;Where two conditions must meet.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      \nabla_{ \phi } Q_{ \phi } (s, a) = \nabla_{ \theta} \ln \pi_{\theta} (a \vert s)
      \qquad
      \mathcal{J} ( \phi )
      = \mathbb{E}_{ s \sim d^{\pi_{\theta}}, a \sim \pi_{\theta} } \left[
      \left( Q_{\phi} (s, a) - Q^{\pi_{\theta}} (s, a) \right)^{2}
      \right]
      \leq
      \varepsilon
  \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;There are two types of policy distribution that meet the two conditions.
        &lt;ul&gt;
          &lt;li&gt;Softmax policy distribution for discrete action space.&lt;/li&gt;
        &lt;/ul&gt;

\[\begin{equation}
      \pi_{\theta} (a \vert s)
      = \text{Softmax} ( e^{ \mathbf{x} ( s , a ) \cdot \theta } )
      = \frac{ e^{ \mathbf{x} ( s , a ) \cdot \theta } }{ \sum_{ a' \in \mathcal{A} } e^{ \mathbf{x} ( s , a' ) \cdot \theta } }
      \qquad
      Q_{ \phi } (s, a)
      = \left( \mathbf{x} ( s , a ) - \sum_{ a' \in \mathcal{A} } \mathbf{x} ( s , a' ) \pi_{\theta} (a' \vert s) \right) \cdot \phi
  \end{equation}\]

        &lt;ul&gt;
          &lt;li&gt;Gaussian policy distribution for continuous action space.&lt;/li&gt;
        &lt;/ul&gt;

\[\begin{equation}
      \pi_{\theta} (a \vert s) = \mathcal{N} ( \mu ( s , a ) , \sigma^{2} )
      \quad
      \mu ( s , a ) = \mathbf{x} ( s , a ) \cdot \theta
      \qquad
      Q_{ \phi } (s, a)
      = \left( \mathbf{x} ( s , a ) \frac{ a - \mu ( s , a ) }{ \sigma^{2} } \right) \cdot \phi
  \end{equation}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The DPG is just the PG with the greedy policy, developed to improve computational efficiency in continuous state and action space.&lt;/p&gt;

\[\begin{align}
    \mathcal{J} ( \theta )
    &amp;amp; = \mathbb{E}_{ s \sim d^{\mu_{\theta}} } [ R ( s , \mu_{\theta} ( s ) ) ]
    = \int_{\mathcal{S}} d^{\mu_{\theta}} ( s ) R ( s , \mu_{\theta} ( s ) ) \text{ d}s
    \\
    &amp;amp; \equiv \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[ Q^{\mu_{\theta}} (s, \mu_{\theta} ( s )) \right]
    = \int_{\mathcal{S}} d^{\mu_{\theta}} ( s ) Q^{\mu_{\theta}} (s, \mu_{\theta} ( s )) \text{ d}s
    \\
    &amp;amp; \equiv \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[ V^{\mu_{\theta}} (s) \right]
    = \int_{\mathcal{S}} d^{\mu_{\theta}} ( s ) V^{\mu_{\theta}} (s) \text{ d}s
    \\
    &amp;amp; \equiv \mathbb{E}_{ s_{0} \sim p_{0} } \left[ V^{\mu_{\theta}} (s_{0}) \right]
    = \int_{\mathcal{S}} p_{0} ( s_{0} ) V^{\mu_{\theta}} (s_{0}) \text{ d}s_{0}
\end{align}\]

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v32/silver14.pdf&quot;&gt;&lt;strong&gt;Theorem 1 (Deterministic Policy Gradient Theorem)&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;

\[\begin{align}
      \nabla_{\theta} \mathcal{J} ( \theta )
      &amp;amp; = \nabla_{\theta}
      \int_{\mathcal{S}} d^{\mu_{\theta}} ( s ) Q^{\mu_{\theta}} (s, \mu_{\theta} ( s )) \text{ d}s
      = \mathbb{E}_{ s \sim d^{\mu_{\theta}} }
      \left[ \nabla_{\theta} Q^{\mu_{\theta}} (s, \mu_{\theta} ( s )) \right]
      \\
      &amp;amp; \propto \int_{\mathcal{S}} d^{\mu_{\theta}} ( s )
      \nabla_{ \theta} \mu_{\theta} ( s )
      \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )} \text{ d}s
      = \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[
      \nabla_{ \theta} \mu_{\theta} ( s )
      \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}
      \right]
  \end{align}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v32/silver14.pdf&quot;&gt;&lt;strong&gt;Theorem 2 (Limit of the Stochastic Policy Gradient)&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;

\[\begin{equation}
      \lim_{\sigma \rightarrow 0}
      \nabla_{\theta} \mathcal{J} ( \pi_{ \mu_{\theta}, \sigma } ) = \nabla_{\theta} \mathcal{J} ( \mu_{\theta} )
  \end{equation}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v32/silver14.pdf&quot;&gt;&lt;strong&gt;Theorem 3 (Compatible Function Approximation)&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt;

\[\begin{equation}
      \nabla_{ \theta } \mathcal{J} (\theta)
      \propto \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[
      \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )} \nabla_{ \theta} \mu_{\theta} ( s )
      \right]
      \equiv \mathbb{E}_{ s \sim d^{\mu_{\theta}} } \left[
      \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} \nabla_{ \theta} \mu_{\theta} ( s )
      \right]
  \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;Where two conditions must meet.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )} = \nabla_{ \theta} \mu_{\theta} ( s ) \cdot \phi
      \qquad
      \mathcal{J} ( \phi )
      = \mathbb{E}_{ s \sim d^{\mu_{\theta}}} \left[ \left(
      \nabla_{ a } Q_{\phi} (s, a) \vert_{a = \mu_{\theta} ( s )}
      - \nabla_{ a } Q^{\mu_{\theta}} (s, a) \vert_{a = \mu_{\theta} ( s )}
      \right)^{2} \right]
      \leq
      \varepsilon
  \end{equation}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Three popular model-based algorithms are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The QL:
    &lt;ol&gt;
      &lt;li&gt;parameterizes the value function.&lt;/li&gt;
      &lt;li&gt;naturally encapsulates the deterministic policy.&lt;/li&gt;
      &lt;li&gt;handles the MDP with only the discrete state/action space.&lt;/li&gt;
      &lt;li&gt;updates per step using a bootstrapping.&lt;/li&gt;
      &lt;li&gt;has relatively low variance, so it converges fast.&lt;/li&gt;
      &lt;li&gt;has relatively high bias, so it may be more likely to fall into local optima.
        &lt;ul&gt;
          &lt;li&gt;has initialization bias, so the initialization influences the final performance.&lt;/li&gt;
          &lt;li&gt;has overestimation bias, so it takes a risky path.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;is naturally off-policy, so it has better exploration.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;The PG:
    &lt;ol&gt;
      &lt;li&gt;parameterizes the policy.&lt;/li&gt;
      &lt;li&gt;naturally encapsulates the stochastic policy.&lt;/li&gt;
      &lt;li&gt;handles the MDP with both the discrete and continuous state/action space.&lt;/li&gt;
      &lt;li&gt;updates per episode.&lt;/li&gt;
      &lt;li&gt;has relatively high variance, so it converges slow.&lt;/li&gt;
      &lt;li&gt;has relatively low bias, so it may be less likely to fall into local optima.&lt;/li&gt;
      &lt;li&gt;is naturally on-policy, but off-policy can be employed with the importance sampling.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;The AC:
    &lt;ol&gt;
      &lt;li&gt;parameterizes the value function and the policy.&lt;/li&gt;
      &lt;li&gt;naturally encapsulates the stochastic policy.&lt;/li&gt;
      &lt;li&gt;handles the MDP with both the discrete and continuous state/action space.&lt;/li&gt;
      &lt;li&gt;updates per step using a bootstrapping.&lt;/li&gt;
      &lt;li&gt;has relatively low variance, so it converges fast.&lt;/li&gt;
      &lt;li&gt;has relatively low bias, so it may be less likely to fall into local optima.&lt;/li&gt;
      &lt;li&gt;is naturally on-policy, but off-policy can be employed with the importance sampling.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html&quot;&gt;Policy Gradient Methods for Reinforcement Learning with Function Approximation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://proceedings.mlr.press/v32/silver14&quot;&gt;Deterministic Policy Gradient Algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Chapter 13 in
&lt;a href=&quot;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other helpful resources are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/posts/2018-04-08-policy-gradient/&quot;&gt;Policy Gradient Algorithms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/&quot;&gt;Going Deeper Into Reinforcement Learning: Fundamentals of Policy Gradients&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>D. K. Ryu</name></author><category term="Machine Learning&amp;#x003a; Decision" /><summary type="html">Policy-based reinforcement learning is one of two fundamental classes of reinforcement learning algorithms. It explicitly constructs the policy, directly mapping from the state to the action, such that maximizes the return. This post aims to provide a tutorial on policy-based reinforcement learning algorithms and introduce a combination of value-based and policy-based reinforcement learning algorithm.</summary></entry><entry><title type="html">Markov Chain</title><link href="http://localhost:4000/posts/mathematics/markov_model_1_markov_chain/" rel="alternate" type="text/html" title="Markov Chain" /><published>2022-09-15T00:00:00+09:00</published><updated>2023-04-15T00:00:00+09:00</updated><id>http://localhost:4000/posts/mathematics/markov_model_1_markov_chain</id><content type="html" xml:base="http://localhost:4000/posts/mathematics/markov_model_1_markov_chain/">&lt;div class=&quot;notice--primary&quot;&gt;
  
&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Basic probability&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;

&lt;blockquote&gt;
    All is number.
    &lt;br /&gt;
    &lt;cite&gt;Pythagoras&lt;/cite&gt;
&lt;/blockquote&gt;

&lt;p&gt;Humans have invented and developed languages to symbolize entities, constructing &lt;em&gt;Matrix&lt;/em&gt; that captures the essence of reality. On the other hand, any form of entities can be modelled as &lt;em&gt;number&lt;/em&gt;, from natural to complex numbers and from points to objects, which existed long before human civilization. Thereby, the universe itself is inherently number, or in short, &lt;em&gt;all is number&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mathematics&lt;/strong&gt; is the study of numbers, so if an entity exists, there exists mathematics to describe it, even if we cannot perceive. This is why we (arguably) refer mathematics as &lt;em&gt;discovery&lt;/em&gt; instead of &lt;em&gt;invention&lt;/em&gt; and often considered the most fundamental and precise way of describing reality. Mathematics is a vast, diverse and abstract field such that it requires books of worth. Here, a few branches of mathematics are focused on.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;Markov model&lt;/strong&gt; is a stochastic process that describes pseudo-randomly changing systems under the &lt;strong&gt;Markov property&lt;/strong&gt;, or the Markov assumption. The Markov property stipulates the future state depends only on the current state, not the history of states. There are four common Markov models:&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;b&gt;Fully Observable&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;b&gt;Partially Observable&lt;/b&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;b&gt;Autonomous System&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;a href=&quot;/posts/mathematics/markov_model_1_markov_chain/#markov-chain&quot;&gt;Markov Chain&lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;a href=&quot;/posts/mathematics/markov_model_1_markov_chain/#hidden-markov-model&quot;&gt;Hidden Markov Model&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt; &lt;b&gt;Controllable System&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;a href=&quot;/posts/mathematics/markov_model_2_markov_decision_process/#markov-decision-process&quot;&gt;Markov Decision Process&lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;a href=&quot;/posts/mathematics/markov_model_2_markov_decision_process/#partial-observabiliby&quot;&gt;Partially Observable Markov Decision Process&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;A &lt;strong&gt;Markov chain&lt;/strong&gt; is the simplest form of a Markov model, which describes the world with an autonomous system. It is used to model simulations, speech recognition, time series forecasting, bioinformatics, game theories and many more. This post aims to provide a tutorial on a Markov chain and introduce a commonly used variant, a hidden Markov model.&lt;/p&gt;

&lt;h1 id=&quot;markov-chain&quot;&gt;Markov Chain&lt;/h1&gt;

&lt;p&gt;A &lt;strong&gt;Markov chain (MC)&lt;/strong&gt;, or Markov process, is the simplest form of a Markov model. In an MC, there exists a system that starts at a particular state, and every time step, the system travels to neighbouring states within the MC.&lt;/p&gt;

&lt;p&gt;The Markov property is formally defined as following. There exists a &lt;strong&gt;state&lt;/strong&gt;, \(s \in \mathcal{S}\), and a &lt;strong&gt;transition probability&lt;/strong&gt;, \(p \in \mathcal{P}\), such that:&lt;/p&gt;

\[\begin{equation}
    \Pr ( S_{t+1} = s_{t+1} \vert S_{0:t} = s_{0:t} )
    \equiv p ( S_{t+1} = s_{t+1} \vert S_{t} = s_{t} )
    = p ( s_{t+1} \vert s_{t} )
\end{equation}\]

&lt;p&gt;Where \(S_{0:t} = s_{0:t}\) signifies a sequence of states that the system visits from \(0\) to \(t\) time, \(S_{0} = s_{0} , \cdots , S_{t} = s_{t}\). A collection of states that the system visited for a given maximum time, \(T\), is referred to as a &lt;strong&gt;path&lt;/strong&gt;, \(\{ S_{0} = s_{0} , \cdots S_{T} = s_{T} \}\). Unlike computer science, people from mathematics often refer to a state as an event or a process with \(\{ X_{t} \}_{t=0}^{T}\) notation.&lt;/p&gt;

&lt;p&gt;The Markov property assumes that the probability of transitioning between two states is only dependent on the one-step previously visited states, not on the history of all the previously visited states. This allows us to formalize the transition probabilities as a fixed transition matrix, \(\mathbf{P}\):&lt;/p&gt;

\[\begin{equation}
    \mathbf{P}
    = \begin{bmatrix}
        p ( s_{1} \vert s_{1} ) &amp;amp; p ( s_{2} \vert s_{1} ) &amp;amp; \cdots &amp;amp; p ( s_{n} \vert s_{1} ) \\
        p ( s_{1} \vert s_{2} ) &amp;amp; p ( s_{2} \vert s_{2} ) &amp;amp; \cdots &amp;amp; p ( s_{n} \vert s_{2} ) \\
        \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
        p ( s_{1} \vert s_{n} ) &amp;amp; p ( s_{2} \vert s_{n} ) &amp;amp; \cdots &amp;amp; p ( s_{n} \vert s_{n} ) \\
    \end{bmatrix}
\end{equation}\]

&lt;p&gt;Consider the following MC.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 350px&quot; colspan=&quot;2&quot;&gt;
            Example Markov Chain from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Markovkate_01.svg&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;img src=&quot;/assets/mathematics/markov_model_1_markov_chain/1_mc.png&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 150px&quot;&gt;
            $$
            \begin{align}
                \mathbf{P}
                &amp;amp; = \begin{bmatrix}
                    p ( E \vert E ) &amp;amp; p ( A \vert E ) \\
                    p ( E \vert A ) &amp;amp; p ( A \vert A ) \\
                \end{bmatrix}
                \\
                &amp;amp; = \begin{bmatrix}
                    0.3 &amp;amp; 0.7 \\
                    0.4 &amp;amp; 0.6 \\
                \end{bmatrix}
            \end{align}
            $$
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;\(\mathbf{P}\) is what determines the property of a state in an MC, and moreover, the property of an MC. However, \(\mathbf{P}\) is not really intuitive, so instead, we usually analyze the accessibility of states from each other, referred to as &lt;strong&gt;communication&lt;/strong&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If \(s_{i}\) communicates with itself, \(s_{i} \leftrightarrow s_{i}\).&lt;/li&gt;
  &lt;li&gt;If \(s_{i}\) communicates with \(s_{j}\), \(s_{i} \leftrightarrow s_{j}\).&lt;/li&gt;
  &lt;li&gt;If \(s_{i}\) communicates with \(s_{k}\) through \(s_{j}\), \(s_{i} \leftrightarrow s_{j}\) and \(s_{j} \leftrightarrow s_{k}\), then \(s_{i} \leftrightarrow s_{k}\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these, we can determine the property of an MC. Few additional important concepts in an MC are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;closed set&lt;/strong&gt; is a set of states that the system cannot entre or leave.
    &lt;ul&gt;
      &lt;li&gt;A set includes a single element as well, so a state alone can be a closed set.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;An &lt;strong&gt;initial distribution&lt;/strong&gt; is a probability distribution over states that the system starts. It is a row vector, \(\pi^{(0)} \in \mathbb{R}^{n}\).
    &lt;ul&gt;
      &lt;li&gt;The proability of the system visiting states at \(k\) step for a given \(\pi^{(0)}\) is \(\pi^{(0)} \mathbf{P}^{k}\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;stationary distribution&lt;/strong&gt; is an equilibrium probability distribution over states that do not change over time. It is a row vector, \(\pi \in \mathbb{R}^{n}\).
    &lt;ul&gt;
      &lt;li&gt;\(\pi\) is the left eigenvector for \(\mathbf{P}\), or \(\pi = \pi \mathbf{P}\).&lt;/li&gt;
      &lt;li&gt;There may exist more than one \(\pi\) depending on the perperty of an MC.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;limiting distribution&lt;/strong&gt; is a unique non-zero equilibrium probability distribution over states that the system will end up if it travels infinitely. It is a row vector, \(\pi^{(\infty)} \in \mathbb{R}^{n}\).
    &lt;ul&gt;
      &lt;li&gt;\(\pi^{(\infty)}\) is the proability of the system visiting states at an infinite step for a given \(\pi^{(0)}\), or \(\pi^{(\infty)} = \pi^{(0)} \mathbf{P}^{\infty}\).&lt;/li&gt;
      &lt;li&gt;Similar to \(\pi\), \(\pi^{(\infty)}\) is the left eigenvector for \(\mathbf{P}\), or \(\pi^{(\infty)} = \pi^{(\infty)} \mathbf{P}\).&lt;/li&gt;
      &lt;li&gt;Unlike \(\pi^{(0)}\) and \(\pi\), \(\pi^{(\infty)}\) may not exist depending on the property of an MC. If \(\pi^{(\infty)}\) exists, it is unique and equivalent to \(\pi\), or \(\pi^{(\infty)} = \pi\).
        &lt;ul&gt;
          &lt;li&gt;In other words, if \(\pi^{(\infty)}\) exists, there only exists a single \(\pi\) such that \(\pi = \pi^{(\infty)}\).&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usually, \(\mathbf{P}\) is used to determine:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Whether a particular state communicates with other states.&lt;/li&gt;
  &lt;li&gt;The characteristics of the stationary distribution.&lt;/li&gt;
  &lt;li&gt;The existence of the limiting distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, I will go through some typical properties of states and MC, particularly about countably finite discrete-time MC.&lt;/p&gt;

&lt;h2 id=&quot;absorption&quot;&gt;Absorption&lt;/h2&gt;

&lt;p&gt;A state is referred to as an &lt;strong&gt;absorbing state&lt;/strong&gt; and a chain of states is referred to as an &lt;strong&gt;absorbing chain&lt;/strong&gt; if the system cannot leave once entered, or &lt;strong&gt;absorption&lt;/strong&gt; occurs. An MC with an absorbing state and/or an absorbing chain is referred to as an &lt;strong&gt;absorbing MC&lt;/strong&gt;. In an absorbing MC, a non-absorbing state is referred to as a &lt;strong&gt;transient state&lt;/strong&gt;. Thus, two types of states exist, absorbing and transient in an absorbing MC.&lt;/p&gt;

&lt;p&gt;In an absorbing MC, \(\mathbf{P} \in \mathbb{R}^{n \times n}\) is usually expressed in canonical form. Let \(t\) be transient states and \(r\) be absorbing states, where \(n = t + r\).&lt;/p&gt;

\[\begin{equation}
    \mathbf{P}
    = \begin{bmatrix}
        \mathbf{Q} &amp;amp; \mathbf{R} \\
        \mathbf{0} &amp;amp; \mathbf{I} \\
    \end{bmatrix}
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;\(\mathbf{Q} \in \mathbb{R}^{t \times t}\) is \(p\) between transient states.&lt;/li&gt;
  &lt;li&gt;\(\mathbf{R} \in \mathbb{R}^{t \times r}\) is \(p\) from transient states to absorbing states.&lt;/li&gt;
  &lt;li&gt;\(\mathbf{I} \in \mathbb{R}^{r \times r}\) is \(p\) between absorbing states.&lt;/li&gt;
  &lt;li&gt;\(\mathbf{0} \in \mathbb{R}^{r \times t}\) is \(p\) from absorbing states to transient states, which is \(0\)-matrix.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A &lt;strong&gt;fundamental matrix&lt;/strong&gt;, \(\mathbf{N} \in \mathbb{R}^{t \times t}\), is the expected number of the visitations between every transient state before absorption, \(\mathbf{N} = \sum_{k=0}^{\infty} \mathbf{Q}^{k} = ( \mathbf{I} - \mathbf{Q} )^{-1}\).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Each \(( i , j )\) element represents the expected number of the visitations from \(s_{i}\) to \(s_{j}\) before absorption.&lt;/li&gt;
  &lt;li&gt;\(\mathbf{Q}^{k}\) is the proability of the system travelling between transient states at \(k\) step.&lt;/li&gt;
  &lt;li&gt;\(\mathbf{Q}^{\infty}\) approaches zero over time, \(\lim_{ k \rightarrow \infty } \mathbf{Q}^{k} = \mathbf{0} \in \mathbb{R}^{t \times t}\).&lt;/li&gt;
  &lt;li&gt;Under the Taylor series, \(\frac{1}{1-x} = \sum_{k=0}^{\infty} x^{k}\), and thus, \(\frac{1}{\mathbf{I} - \mathbf{Q}} = ( \mathbf{I} - \mathbf{Q} )^{-1} = \sum_{k=0}^{\infty} \mathbf{Q}^{k} = \mathbf{N}\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An &lt;strong&gt;irreduciblility&lt;/strong&gt; is another commonly seen term in an absorbing MC. It is often confused with absorption since it can be a synonym or antonym based on the context. A set of states that the system cannot leave once entered is referred to as a &lt;strong&gt;closed irreducible set&lt;/strong&gt;. An MC with a closed irreducible set is referred to as a &lt;strong&gt;reducible MC&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Thus, &lt;em&gt;a closed irreducible set&lt;/em&gt; is &lt;em&gt;a synonym for an absorbing state and absorbing chain&lt;/em&gt;, but &lt;em&gt;an absorbing MC is antonym for an irreducible MC&lt;/em&gt;. More formally:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;An MC is an absorbing MC if there exist one or more closed irreducible sets, in which the system can reach from any state in a finite number of steps.&lt;/li&gt;
  &lt;li&gt;An MC is an irreducible MC if the system can reach any state from any other state in a finite number of time step.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If an absorbing MC is &lt;em&gt;reduced&lt;/em&gt; into multiple disjoint subsets based on reduciblility, that are an absorbing state, an absorbing chain and a chain of transient states, &lt;em&gt;states in each subset has the property that all states can communicate with each other&lt;/em&gt;, i.e. absorbing or transient.&lt;/p&gt;

&lt;h2 id=&quot;recurrence&quot;&gt;Recurrence&lt;/h2&gt;

&lt;p&gt;A state is referred to as a &lt;strong&gt;recurrent state&lt;/strong&gt; if the system is guaranteed to return to the starting state within a finite step. Note that the term &lt;em&gt;recurrent&lt;/em&gt; is not often used to refer to an MC. More formally, a state is a recurrent state if:&lt;/p&gt;

\[\begin{equation}
    \sum_{t = 1}^{\infty} \Pr ( S_{t} = s_{i} \vert S_{0} = s_{i} ) = 1
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;\(\Pr ( S_{t} = s_{i} \vert S_{0} = s_{i} )\): For a given starting state, \(s_{i}\), the probability of the system returning to \(s_{i}\) at \(t\) step.&lt;/li&gt;
  &lt;li&gt;\(\sum_{t = 1}^{\infty} \Pr ( S_{t} = s_{i} \vert S_{0} = s_{i} ) = 1\): The sum of the probabilities that the system will return to \(s_{i}\) is \(1\) in finite step. This means that the system guarantees that it will re-visit the starting state.&lt;/li&gt;
  &lt;li&gt;The notation omits \(S_{1} \neq s_{i} , \cdots , S_{t-1} \neq s_{i}\), so what it really signifies is \(\Pr ( S_{t} = s_{i} \vert S_{0} = s_{i} , S_{1} \neq s_{i} , \cdots , S_{t-1} \neq s_{i} )\). This means that, for a given \(S_{0} = s_{i}\), the probability of the system returning to \(s_{i}\) at \(t\) step without visiting \(s_{i}\) in between.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This definition is equivalent to all the following statements:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(\sum_{t=1}^{\infty} \mathbb{1} ( S_{t} = s_{i} \vert S_{0} = s_{i} ) = \infty\): For a given \(S_{0} = s_{i}\), the summation of the booleans that the system will return to \(s_{i}\) is infinite.&lt;/li&gt;
  &lt;li&gt;\(\mathbb{1} ( S_{t} = s_{i} \text{ for infinitely many } t \vert S_{0} = s ) = 1\): For a given \(S_{0} = s_{i}\), the boolean that the system will return to \(s_{i}\) for infinitely many \(t\) is 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If the system is not guaranteed to return, the state is referred to as a &lt;strong&gt;transient state&lt;/strong&gt;, which is equivalent to the transient in an absorbing MC. More formally:&lt;/p&gt;

\[\begin{equation}
    \sum_{t=1}^{\infty} \Pr ( S_{t} = s_{i} \vert S_{0} = s_{i} ) &amp;lt; 1
    \\
    \sum_{t=1}^{\infty} \mathbb{1} ( S_{t} = s_{i} \vert S_{0} = s_{i} ) &amp;lt; \infty
    \\
    \mathbb{1} ( S_{t} = s_{i} \text{ for infinitely many } t \vert S_{0} = s_{i} ) = 0
\end{equation}\]

&lt;p&gt;An absorbing MC is the common MC that contains both recurrent and transient states.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;An absorbing state is the recurrent state. If \(s_{i}\) is absorbing, \(\Pr ( S_{1} = s_{i} \vert S_{0} = s_{i} ) = 1\) and \(\Pr ( S_{n} = s_{i} \vert S_{0} = s_{i} ) = 0\) for \(n &amp;gt; 1\).&lt;/li&gt;
  &lt;li&gt;All the states in an absorbing chain is the recurrent states. If \(s_{i}\) is within the absorbing chain, \(\sum_{t = 1}^{\infty} \Pr ( S_{t} = s_{i} \vert S_{0} = s_{i} ) = 1\) within the absorbing chain.&lt;/li&gt;
  &lt;li&gt;A non-absorbing state is the transient state. If \(s_{i}\) is not absorbing, the system will fall into an absorbing state or chain within a finite step, \(\sum_{t=1}^{\infty} \Pr ( S_{t} = s_{i} \vert S_{0} = s_{i} ) &amp;lt; 1\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A recurrence is to claim that the system will return to the starting state. However, there is a further division, &lt;em&gt;positive&lt;/em&gt; or &lt;em&gt;null&lt;/em&gt;. Both of them guarantee that the system will return to the starting state, but what matters is when. More formally, let a &lt;strong&gt;hitting time&lt;/strong&gt; to be \(\tau_{s} := \inf \{ t \geq 0 : s_{i} \in \mathcal{S} \}\), which is the infimum of the number of steps that the system returns to the starting state, \(s_{i}\).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Positive Recurrent&lt;/strong&gt;: \(\tau_{s}\) is finite and finite expectation, or \(\tau_{s} &amp;lt; \infty\) and \(\mathbb{E} \left[ \tau_{s} \right] &amp;lt; \infty\), and thus, guaranteed to return in finite step.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Null Recurrent&lt;/strong&gt;: \(\tau_{s}\) is finite, but infinite expectation, or \(\tau_{s} &amp;lt; \infty\) and \(\mathbb{E} \left[ \tau_{s} \right] = \infty\), and thus, guaranteed to return in infinite step.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transient&lt;/strong&gt;: \(\tau_{s}\) is infinite and infinite expectation, or \(\tau_{s} = \infty\) and \(\mathbb{E} \left[ \tau_{s} \right] = \infty\), and thus, not guaranteed to return.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Due to this, positive recurrent is often described as it will return in a finite step while null recurrent as it will almost certainly return in a finite step.&lt;/p&gt;

&lt;p&gt;A null recurrent state can only be present in a countably infinite MC, so in a countably finite MC, if an MC is said to be recurrent, it is always positive recurrent.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If an MC is countably finite:
    &lt;ul&gt;
      &lt;li&gt;There must be at least one recurrent state.
        &lt;ul&gt;
          &lt;li&gt;If reducible, an absorbing state or chain is recurrent and others are transient.&lt;/li&gt;
          &lt;li&gt;If irreducible, every state is positive recurrent.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;There can be no transient state, but not every state can be transient.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If an MC is countably infinite:
    &lt;ul&gt;
      &lt;li&gt;Every state can be either positive recurrent, null recurrent or transient.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;periodicity&quot;&gt;Periodicity&lt;/h2&gt;
&lt;p&gt;A state is referred to as a &lt;strong&gt;periodic state&lt;/strong&gt; if the system returns to a particular state \(s_{i}\) at some rates \(\{ n , 2n , 3n , \cdots \}\), where \(n &amp;gt; 1\). For instance, for a given \(S_{0} = s_{i}\), if the system returns to \(s_{i}\) at \(t = \{ 4 , 16 , 24 , 28 , \cdots \}\), then the state is periodic with \(n = 4\). If one state is periodic, then all the other states are periodic with the same period \(n\). An MC with periodic states is referred to as a &lt;strong&gt;periodic MC&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;If at least one state has \(p\) to itself, \(p ( s_{i} \vert s_{i} ) &amp;gt; 0\), the state is aperiodic, and thus, the MC is also aperiodic. In general, it is said that an aperiodic MC has \(n = 1\) or \(n\) does not exist.&lt;/p&gt;

&lt;p&gt;To compare with other properties:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If an MC is absorbing, it cannot be periodic since the system will get stuck at an absorbing state or chain in a finite step.&lt;/li&gt;
  &lt;li&gt;If an MC is transient or null recurrent, it cannot be periodic since the system may never return or return in an infinite step.&lt;/li&gt;
  &lt;li&gt;Thus, a periodic MC is always &lt;em&gt;irreducible&lt;/em&gt; and &lt;em&gt;positive recurrent&lt;/em&gt; while aperiodic can hold any property. However, irreducible and/or positive recurrent does not tell us any information about periodicity.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ergodicity&quot;&gt;Ergodicity&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;ergodicity&lt;/strong&gt; is one of the typical assumptions in an MC for machine learning applications. It is derived from ergodic theory, a branch of mathematics that studies statistical properties of deterministic dynamical systems, but the use of ergodicity is fairly simple in machine learning. The regular MC we usually refer to in machine learning actually refers to an ergodic MC.&lt;/p&gt;

&lt;p&gt;An MC is referred to as an &lt;strong&gt;ergodic MC&lt;/strong&gt; if the system can travel from any state to any other state in any period, a.k.a. aperiodic, irreducible and positive recurrent. If an MC is ergodic, there exists a limiting distribution.&lt;/p&gt;

\[\begin{equation}
    \pi_{i}^{(\infty)}
    = \lim_{n \rightarrow \infty} \Pr ( S_{n} = s_{i} \vert S_{0} = s ) &amp;gt; 0
    \quad
    s \in \mathcal{S}
\end{equation}\]

&lt;p&gt;This means, given any starting state, there exists a unique non-zero equilibrium probability that the system lands on a particular state, regardless of where it starts, if it travels infinitely.&lt;/p&gt;

&lt;p&gt;A limiting distribution is often interchangeably used with a stationary distribution, but they are different.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A stationary distribution is an equilibrium probability distribution that does not change over time, which is the left eigenvector for \(\mathbf{P}\), \(\pi = \pi \mathbf{P}\), such that \(\pi_{i} \geq 0 , \sum_{i}^{n} \pi_{i} = 1\), where \(\mathcal{S} = \{ s_{0} , \cdots , s_{n} \}\).
    &lt;ul&gt;
      &lt;li&gt;For an MC to have \(\pi\), it must have at least one positive recurrent state, in other words, if an MC has countably finite states, it has \(\pi\).&lt;/li&gt;
      &lt;li&gt;There is no restriction on how many \(\pi\) to exist, and thus, there may be more than one \(\pi\).&lt;/li&gt;
      &lt;li&gt;For instance, a periodic MC has a single \(\pi\) while an absorbing MC has infinitely many \(\pi\).&lt;/li&gt;
      &lt;li&gt;One way to obtain \(\pi\) is by finding eigenvalues and eigenvectors of the transposed \(\mathbf{P}\).&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
     \pi = \pi \mathbf{P}
     \quad \rightarrow \quad
     \lambda \pi^{\intercal} = \mathbf{P}^{\intercal} \pi^{\intercal}
     \quad
     \text{where, } \lambda = 1 , \pi_{i} \geq 0 , \sum_{i}^{n} \pi_{i} = 1
 \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;However, this method does not obtain every \(\pi\). For instance, in an absorbing MC, there are infinitely many \(\pi\) and this method only acquires a few instances.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Another way to view \(\pi\) is from \(\tau_{s}\).&lt;/p&gt;

\[\begin{equation}
      \pi_{i} = \frac{1}{ \mathbb{E} \left[ \tau_{i} \right] }
  \end{equation}\]

        &lt;ul&gt;
          &lt;li&gt;This also reflects why if \(\mathbb{E} \left[ \tau_{s} \right] = \infty\), i.e. null recurrent or transient, \(\pi\) approaches zero.&lt;/li&gt;
          &lt;li&gt;For an MC with null recurrent states only or transient states only, \(\tau_{s}\) for all the states is infinite, so \(\sum_{i}^{\infty} \pi_{i} = 1\) does not hold, and thus, there is no \(\pi\). Often determining if a \(\pi\) exists in the case of a recurrent MC is a method of proving if an MC is positive recurrent.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;So, if an MC is countably finite, at least one \(\pi\) exists, but if countably infinite, there may be no \(\pi\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A limiting distribution is a unique non-zero equilibrium distribution that is not dependent on an initial distribution, \(\pi_{i}^{(\infty)} = \lim_{n \rightarrow \infty} \Pr ( S_{n} = s_{i} \vert S_{0} = s ) &amp;gt; 0\).
    &lt;ul&gt;
      &lt;li&gt;Since \(\pi^{(\infty)}\) is a unique distribution, the multiplication of \(\mathbf{P}\) to any \(\pi^{(0)}\) converges to \(\pi^{(\infty)}\).&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{align}
     \mathbf{P}^{\ast}
     = \lim_{k \rightarrow \infty} \mathbf{P}^{k}
     &amp;amp; = \lim_{k \rightarrow \infty}
     \begin{bmatrix}
         p ( s_{1} \vert s_{1} ) &amp;amp; p ( s_{2} \vert s_{1} ) &amp;amp; \cdots &amp;amp; p ( s_{n} \vert s_{1} ) \\
         p ( s_{1} \vert s_{2} ) &amp;amp; p ( s_{2} \vert s_{2} ) &amp;amp; \cdots &amp;amp; p ( s_{n} \vert s_{2} ) \\
         \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
         p ( s_{1} \vert s_{n} ) &amp;amp; p ( s_{2} \vert s_{n} ) &amp;amp; \cdots &amp;amp; p ( s_{n} \vert s_{n} ) \\
     \end{bmatrix}^{k}
     \\
     &amp;amp; = \begin{bmatrix}
         \pi_{1}^{(\infty)} &amp;amp; \pi_{2}^{(\infty)} &amp;amp; \cdots &amp;amp; \pi_{n}^{(\infty)} \\
         \pi_{1}^{(\infty)} &amp;amp; \pi_{2}^{(\infty)} &amp;amp; \cdots &amp;amp; \pi_{n}^{(\infty)} \\
         \vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots \\
         \pi_{1}^{(\infty)} &amp;amp; \pi_{2}^{(\infty)} &amp;amp; \cdots &amp;amp; \pi_{n}^{(\infty)} \\
     \end{bmatrix}
     = \begin{bmatrix}
         \pi^{(\infty)} \\
         \pi^{(\infty)} \\
         \vdots \\
         \pi^{(\infty)} \\
     \end{bmatrix}
     \quad
     \text{where, }
     \pi^{(\infty)}
     = \begin{bmatrix}
         \pi_{1}^{(\infty)} \\ \pi_{2}^{(\infty)} \\ \vdots \\ \pi_{n}^{(\infty)} \\
     \end{bmatrix}^{\intercal}
 \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;A periodic MC has no \(\pi^{(\infty)}\) since it oscillates periodically.&lt;/li&gt;
      &lt;li&gt;An absorbing MC has no \(\pi^{(\infty)}\) since it converges to a distribution dependent to \(\pi^{(0)}\) and the converged distribution contains zero value at transient states.&lt;/li&gt;
      &lt;li&gt;If \(\pi^{(\infty)}\) exists, \(\pi^{(\infty)} = \pi\), and thus, \(\pi\) is non-zero and unique.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The existence of \(\pi^{(\infty)}\) is necessary and sufficient for the ergodicity of an MC.&lt;/li&gt;
  &lt;li&gt;The ergodicity of an MC is necessary and sufficient for the existence of \(\pi^{(\infty)}\).&lt;/li&gt;
  &lt;li&gt;The existence of \(\pi^{(\infty)}\) is only sufficient for a non-zero unique \(\pi\), not necessary.&lt;/li&gt;
  &lt;li&gt;A non-zero unique \(\pi\) is neither necessary nor sufficient for the existence of \(\pi^{(\infty)}\).&lt;/li&gt;
  &lt;li&gt;A non-zero unique \(\pi\) is neither necessary nor sufficient for the ergodicity of an MC.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;markov-chain-example&quot;&gt;Markov Chain Example&lt;/h1&gt;

&lt;p&gt;To give better insights, I will go through some examples.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt; Absorbing Markov chain &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt; Periodic Markov chain &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt; Ergodic Markov chain &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            &lt;img src=&quot;/assets/mathematics/markov_model_1_markov_chain/2_mc_absorbing.png&quot; width=&quot;300px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            &lt;img src=&quot;/assets/mathematics/markov_model_1_markov_chain/2_mc_periodic.png&quot; width=&quot;300px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            &lt;img src=&quot;/assets/mathematics/markov_model_1_markov_chain/2_mc_ergodic.png&quot; width=&quot;300px&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            $ 1 $ and $ 2 $ are transient while $ 3 $ and $ 4 $ are absorbing.
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            All are periodic with $ n = 2 $, $ \{ 2, 4, 6, \cdots \} $.
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            Other than $ p ( 3 \vert 4 ) $ and $ p ( 2 \vert 3 ) $, all the transition probabilities are non-zero.
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            $$
            \begin{equation}
                \mathbf{P}
                = \begin{bmatrix}
                    0.2 &amp;amp; 0.3 &amp;amp; 0.1 &amp;amp; 0.4 \\
                    0.2 &amp;amp; 0.3 &amp;amp; 0.4 &amp;amp; 0.1 \\
                    0.0 &amp;amp; 0.0 &amp;amp; 1.0 &amp;amp; 0.0 \\
                    0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.0 \\
                \end{bmatrix}
            \end{equation}
            $$
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            $$
            \begin{equation}
                \mathbf{P}
                = \begin{bmatrix}
                    0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
                    0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.0 \\
                    1.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
                    0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
                \end{bmatrix}
            \end{equation}
            $$
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            $$
            \begin{equation}
                \mathbf{P}
                = \begin{bmatrix}
                    0.2 &amp;amp; 0.3 &amp;amp; 0.1 &amp;amp; 0.4 \\
                    0.2 &amp;amp; 0.3 &amp;amp; 0.4 &amp;amp; 0.1 \\
                    0.5 &amp;amp; 0.0 &amp;amp; 0.2 &amp;amp; 0.3 \\
                    0.1 &amp;amp; 0.7 &amp;amp; 0.0 &amp;amp; 0.2 \\
                \end{bmatrix}
            \end{equation}
            $$
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Markov chain examples. The original code for visualization and calculation can be found in
        &lt;a href=&quot;/assets/mathematics/markov_model_1_markov_chain/figure.ipynb&quot;&gt;
            here
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/table&gt;

&lt;h2 id=&quot;absorbing-markov-chain&quot;&gt;Absorbing Markov Chain&lt;/h2&gt;

&lt;p&gt;An absorbing MC has infinitely many stationary distributions, but no limiting distribution. The eigenvalues and eigenvectors for \(\mathbf{P}^{\intercal}\) are:&lt;/p&gt;

\[\begin{equation}
    \lambda \pi^{\intercal} = \mathbf{P}^{\intercal} \pi^{\intercal}
    \quad \rightarrow \quad
    \pi
    = \begin{bmatrix}
        0.0 &amp;amp; 0.0 &amp;amp; 1.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad \text{or} \quad
    \begin{bmatrix}
        0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.0 \\
    \end{bmatrix}
\end{equation}\]

&lt;p&gt;However, they are not all. Consider the transition matrix.&lt;/p&gt;

\[\begin{equation}
    \mathbf{P}
    = \begin{bmatrix}
        0.2 &amp;amp; 0.3 &amp;amp; 0.1 &amp;amp; 0.4 \\
        0.2 &amp;amp; 0.3 &amp;amp; 0.4 &amp;amp; 0.1 \\
        0.0 &amp;amp; 0.0 &amp;amp; 1.0 &amp;amp; 0.0 \\
        0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.0 \\
    \end{bmatrix}
    \quad \rightarrow \quad
    \mathbf{P}^{\infty}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.0 &amp;amp; 0.38 &amp;amp; 0.62 \\
        0.0 &amp;amp; 0.0 &amp;amp; 0.68 &amp;amp; 0.32 \\
        0.0 &amp;amp; 0.0 &amp;amp; 1.0  &amp;amp; 0.0  \\
        0.0 &amp;amp; 0.0 &amp;amp; 1.0  &amp;amp; 0.0  \\
    \end{bmatrix}
\end{equation}\]

&lt;p&gt;Under different initial distributions:&lt;/p&gt;

\[\begin{align}
    \pi^{(0)}
    = \begin{bmatrix}
        1.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad \rightarrow \quad &amp;amp;
    \pi^{(\infty)}
    = \pi^{(0)} \mathbf{P}^{\infty}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.0 &amp;amp; 0.38 &amp;amp; 0.62 \\
    \end{bmatrix}
    \\
    \pi^{(0)}
    = \begin{bmatrix}
        0.2 &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad \rightarrow \quad &amp;amp;
    \pi^{(\infty)}
    = \pi^{(0)} \mathbf{P}^{\infty}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.0 &amp;amp; 0.62 &amp;amp; 0.38 \\
    \end{bmatrix}
    \\
    \pi^{(0)}
    = \begin{bmatrix}
        0.0 &amp;amp; 1.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad \rightarrow \quad &amp;amp;
    \pi^{(\infty)}
    = \pi^{(0)} \mathbf{P}^{\infty}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.0 &amp;amp; 0.68 &amp;amp; 0.32 \\
    \end{bmatrix}
    \\
    \pi^{(0)}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.0 &amp;amp; 1.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad \rightarrow \quad &amp;amp;
    \pi^{(\infty)}
    = \pi^{(0)} \mathbf{P}^{\infty}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.0 &amp;amp; 1.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \\
    \pi^{(0)}
    = \begin{bmatrix}
        0.3 &amp;amp; 0.3 &amp;amp; 0.4 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad \rightarrow \quad &amp;amp;
    \pi^{(\infty)}
    = \pi^{(0)} \mathbf{P}^{\infty}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.0 &amp;amp; 0.718 &amp;amp; 0.282 \\
    \end{bmatrix}
\end{align}\]

&lt;ul&gt;
  &lt;li&gt;A stationary distribution depends on the initial distribution, so there is no unique equilibrium, a.k.a. no limiting distribution.&lt;/li&gt;
  &lt;li&gt;Transient states always have zero stationary distribution, since the system falls into absorbing states.&lt;/li&gt;
  &lt;li&gt;A fundamental matrix and an absorption time are:&lt;/li&gt;
&lt;/ul&gt;

\[\begin{equation}
    \mathbf{P}
    = \begin{bmatrix}
        0.2 &amp;amp; 0.3 &amp;amp; 0.1 &amp;amp; 0.4 \\
        0.2 &amp;amp; 0.3 &amp;amp; 0.4 &amp;amp; 0.1 \\
        0.0 &amp;amp; 0.0 &amp;amp; 1.0 &amp;amp; 0.0 \\
        0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.0 \\
    \end{bmatrix}
    = \begin{bmatrix}
        \mathbf{Q} &amp;amp; \mathbf{R} \\
        \mathbf{0} &amp;amp; \mathbf{I} \\
    \end{bmatrix}
    \\
    \mathbf{Q}
    = \begin{bmatrix}
        0.2 &amp;amp; 0.3 \\
        0.2 &amp;amp; 0.3 \\
    \end{bmatrix}
    \quad
    \mathbf{Q}^{2}
    = \begin{bmatrix}
        0.1 &amp;amp; 0.15 \\
        0.1 &amp;amp; 0.15 \\
    \end{bmatrix}
    \quad
    \mathbf{Q}^{\infty}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.0 \\
        0.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \\
    \mathbf{N} = \sum_{k=0}^{\infty} \mathbf{Q}^{k} = ( \mathbf{I} - \mathbf{Q} )^{-1}
    = \begin{bmatrix}
        1.4 &amp;amp; 0.6 \\
        0.4 &amp;amp; 1.6 \\
    \end{bmatrix}
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;\(( i , j )\) element in \(\mathbf{Q}^{2}\) is the probability of visiting \(s_{j}\) from \(s_{i}\) at \(t = 2\), \(p ( S_{2} = s_{j} \vert S_{0} = s_{i}\).&lt;/li&gt;
  &lt;li&gt;Every element in \(\mathbf{Q}^{\infty}\) reaches zero since the system falls into absorbing states.&lt;/li&gt;
  &lt;li&gt;\(( i , j )\) element in \(\mathbf{N}\) is the number of times that the system will visit \(s_{j}\) from \(s_{i}\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;periodic-markov-chain&quot;&gt;Periodic Markov Chain&lt;/h2&gt;

&lt;p&gt;A periodic MC has a single stationary distribution, but no limiting distribution since \(\lim_{k \rightarrow \infty} \mathbf{P}^{k}\) never converges.&lt;/p&gt;

\[\begin{equation}
    \lambda \pi^{\intercal} = \mathbf{P}^{\intercal} \pi^{\intercal}
    \quad \rightarrow \quad
    \pi
    = \begin{bmatrix}
        0.25 &amp;amp; 0.25 &amp;amp; 0.25 &amp;amp; 0.25 \\
    \end{bmatrix}
\end{equation}\]

&lt;p&gt;For \(\mathbf{P}^{k}\), as \(k \rightarrow \infty\):&lt;/p&gt;

\[\begin{equation}
    \mathbf{P}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
        0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.0 \\
        1.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
        0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad
    \mathbf{P}^{2}
    = \begin{bmatrix}
        0.5 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.5 \\
        0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
        0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
        0.5 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.5 \\
    \end{bmatrix}
    \\
    \mathbf{P}^{3}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
        0.5 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.5 \\
        0.5 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.5 \\
        0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad
    \mathbf{P}^{4}
    = \begin{bmatrix}
        0.5 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.5 \\
        0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
        0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
        0.5 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.5 \\
    \end{bmatrix}
\end{equation}\]

&lt;p&gt;\(\mathbf{P}^{k}\) oscillates between \(\mathbf{P}^{2}\) and \(\mathbf{P}^{3}\), and thus,&lt;/p&gt;

\[\begin{equation}
    \pi^{(0)}
    = \begin{bmatrix}
        1.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad
    \pi^{(0)} \mathbf{P}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
    \end{bmatrix}
    \\
    \pi^{(0)} \mathbf{P}^{2}
    = \begin{bmatrix}
        0.5 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.5 \\
    \end{bmatrix}
    \quad
    \pi^{(0)} \mathbf{P}^{3}
    = \begin{bmatrix}
        0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \\
    \end{bmatrix}
\end{equation}\]

&lt;p&gt;So, it repeats between \(\begin{bmatrix} 0.5 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.5 \end{bmatrix}\) and \(\begin{bmatrix} 0.0 &amp;amp; 0.5 &amp;amp; 0.5 &amp;amp; 0.0 \end{bmatrix}\).&lt;/p&gt;

&lt;h2 id=&quot;ergodic-markov-chain&quot;&gt;Ergodic Markov Chain&lt;/h2&gt;

&lt;p&gt;An ergodic MC has a limiting distribution, and so, a stationary distribution is unique.&lt;/p&gt;

\[\begin{align}
    \lambda \pi^{\intercal} = \mathbf{P}^{\intercal} \pi^{\intercal}
    \quad \rightarrow \quad &amp;amp;
    \pi
    = \begin{bmatrix}
        0.236 &amp;amp; 0.334 &amp;amp; 0.197 &amp;amp; 0.233 \\
    \end{bmatrix}
    \\
    \mathbf{P}
    = \begin{bmatrix}
        0.2 &amp;amp; 0.3 &amp;amp; 0.1 &amp;amp; 0.4 \\
        0.2 &amp;amp; 0.3 &amp;amp; 0.4 &amp;amp; 0.1 \\
        0.5 &amp;amp; 0.0 &amp;amp; 0.2 &amp;amp; 0.3 \\
        0.1 &amp;amp; 0.7 &amp;amp; 0.0 &amp;amp; 0.2 \\
    \end{bmatrix}
    \quad \rightarrow \quad &amp;amp;
    \mathbf{P}^{\infty}
    = \begin{bmatrix}
        0.236 &amp;amp; 0.334 &amp;amp; 0.197 &amp;amp; 0.233 \\
        0.236 &amp;amp; 0.334 &amp;amp; 0.197 &amp;amp; 0.233 \\
        0.236 &amp;amp; 0.334 &amp;amp; 0.197 &amp;amp; 0.233 \\
        0.236 &amp;amp; 0.334 &amp;amp; 0.197 &amp;amp; 0.233 \\
    \end{bmatrix}
    = \begin{bmatrix}
        \pi \\
        \pi \\
        \pi \\
        \pi \\
    \end{bmatrix}
\end{align}\]

&lt;p&gt;Under different initial distributions:&lt;/p&gt;

\[\begin{align}
    \pi^{(0)}
    = \begin{bmatrix}
        1.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad \rightarrow \quad &amp;amp;
    \pi^{(\infty)}
    = \pi^{(0)} \mathbf{P}^{\infty}
    = \begin{bmatrix}
        0.236 &amp;amp; 0.334 &amp;amp; 0.197 &amp;amp; 0.233 \\
    \end{bmatrix}
    = \pi
    \\
    \pi^{(0)}
    = \begin{bmatrix}
        0.2 &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad \rightarrow \quad &amp;amp;
    \pi^{(\infty)}
    = \pi^{(0)} \mathbf{P}^{\infty}
    = \begin{bmatrix}
        0.236 &amp;amp; 0.334 &amp;amp; 0.197 &amp;amp; 0.233 \\
    \end{bmatrix}
    = \pi
    \\
    \pi^{(0)}
    = \begin{bmatrix}
        0.0 &amp;amp; 1.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
    \end{bmatrix}
    \quad \rightarrow \quad &amp;amp;
    \pi^{(\infty)}
    = \pi^{(0)} \mathbf{P}^{\infty}
    = \begin{bmatrix}
        0.236 &amp;amp; 0.334 &amp;amp; 0.197 &amp;amp; 0.233 \\
    \end{bmatrix}
    = \pi
\end{align}\]

&lt;p&gt;So, the stationary distribution converges the same regardless of the initial distribution, and hence, it is the limiting distribution.&lt;/p&gt;

&lt;h1 id=&quot;hidden-markov-model&quot;&gt;Hidden Markov Model&lt;/h1&gt;

&lt;p&gt;A &lt;strong&gt;hidden Markov model (HMM)&lt;/strong&gt; is an MC with partial observability. The idea behind an HMM is that the state is hidden from the system, or a &lt;strong&gt;hidden state&lt;/strong&gt; or unobserved event or latent variable. So, instead of observing a state, the system needs to infer the state from an additional variable, referred to as an &lt;strong&gt;observation&lt;/strong&gt;, \(\mathcal{O}\). There also exists a probability of emitting different observations for a given state, referred to as an &lt;strong&gt;emission probability&lt;/strong&gt;, \(\mathcal{P}_{o}\).&lt;/p&gt;

&lt;p&gt;An emission probability also holds the Markov property. There exists an observation, \(o \in \mathcal{O}\), and an emission probability, \(p_{o} \in \mathcal{P}_{o}\), such that,&lt;/p&gt;

\[\begin{equation}
    \Pr ( O_{t} = o_{t} \vert S_{1:t} = s_{1:t} , O_{1:t-1} = o_{1:t-1} )
    \equiv p_{o} ( O_{t} = o_{t} \vert S_{t} = s_{t} )
    = p_{o} ( o_{t} \vert s_{t} )
\end{equation}\]

&lt;p&gt;So, in an HMM, \(p\) and \(p_{o}\) are only dependent on the previous state and the current state, respectively.&lt;/p&gt;

&lt;p&gt;Consider the following HMM.&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 650px&quot; colspan=&quot;2&quot;&gt;
            Hidden Markov model from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:HiddenMarkovModel.svg&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            &lt;img src=&quot;/assets/mathematics/markov_model_1_markov_chain/3_hmm.png&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 350px&quot;&gt;
            $$
            \begin{equation}
                \mathbf{P}
                = \begin{bmatrix}
                    p ( X1 \vert X1 ) &amp;amp; p ( X2 \vert X1 ) &amp;amp; p ( X3 \vert X1 ) \\
                    p ( X1 \vert X2 ) &amp;amp; p ( X2 \vert X2 ) &amp;amp; p ( X3 \vert X2 ) \\
                    p ( X1 \vert X3 ) &amp;amp; p ( X2 \vert X3 ) &amp;amp; p ( X3 \vert X3 ) \\
                \end{bmatrix}
                \\
                \mathbf{P}_{o}
                = \begin{bmatrix}
                    p ( y1 \vert X1 ) &amp;amp; p ( y2 \vert X1 ) &amp;amp; p ( y3 \vert X1 ) &amp;amp; p ( y4 \vert X1 ) \\
                    p ( y1 \vert X2 ) &amp;amp; p ( y2 \vert X2 ) &amp;amp; p ( y3 \vert X2 ) &amp;amp; p ( y4 \vert X2 ) \\
                    p ( y1 \vert X3 ) &amp;amp; p ( y2 \vert X3 ) &amp;amp; p ( y3 \vert X3 ) &amp;amp; p ( y4 \vert X3 ) \\
                \end{bmatrix}
            \end{equation}
            $$
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The joint probability of all the states and observations can be expressed as:&lt;/p&gt;

\[\begin{align}
    \Pr ( S_{1:T} = s_{1:T} , O_{1:T} = o_{1:T} )
    = &amp;amp; \Pr ( s_{1} , \cdots , s_{T} , o_{1} , \cdots , o_{T})
    \\
    = &amp;amp; p_{0} ( s_{1} ) p_{o} ( o_{T} \vert s_{T} ) \Pi_{t=1}^{T-1} p ( s_{t+1} \vert s_{t} ) p_{o} ( o_{t} \vert s_{t} )
\end{align}\]

&lt;p&gt;Where:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(p_{0} ( s_{1} )\) is the initial distribution for \(s_{1}\).&lt;/li&gt;
  &lt;li&gt;\(p ( s_{t+1} \vert s_{t} )\) is the transition probability from \(s_{t}\) to \(s_{t+1}\).&lt;/li&gt;
  &lt;li&gt;\(p_{o} ( o_{t} \vert s_{t} )\) is the emission probability of \(o_{t}\) at \(s_{t}\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In an HMM, the property is not really what we concern about, instead, the inference of hidden states is what we concern about.&lt;/p&gt;

&lt;h2 id=&quot;inference&quot;&gt;Inference&lt;/h2&gt;

&lt;p&gt;There are two core inferences in an HMM.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Forward inference&lt;/strong&gt;: The joint probability of \(s_{t}\) and \(o_{1:t}\).&lt;/li&gt;
&lt;/ul&gt;

\[\require{color}
\begin{align}
    \alpha_{1} ( s_{1} )
    &amp;amp; = \Pr ( S_{1} = s_{1} , O_{1} = o_{1} )
    \\
    &amp;amp; = \textcolor{red}{ p_{o} ( o_{1} \vert s_{1} ) p_{0} ( s_{1} ) }
    \\
    \alpha_{2} ( s_{2} )
    &amp;amp; = \Pr ( S_{2} = s_{2} , O_{1} = o_{1} , O_{2} = o_{2} )
    \\
    &amp;amp; = \textcolor{blue}{ \sum_{s_{1} \in \mathcal{S}}
    \textcolor{red}{ p_{o} ( o_{1} \vert s_{1} ) p_{0} ( s_{1} ) }
    p_{o} ( o_{2} \vert s_{2} ) p ( s_{2} \vert s_{1} ) }
    = \sum_{s_{1} \in \mathcal{S}} \alpha_{1} ( s_{1} ) p_{o} ( o_{2} \vert s_{2} ) p ( s_{2} \vert s_{1} )
    \\
    \alpha_{3} ( s_{3} )
    &amp;amp; = \Pr ( S_{3} = s_{3} , O_{1} = o_{1} , O_{2} = o_{2} , O_{3} = o_{3} )
    \\
    &amp;amp; = \sum_{s_{2} \in \mathcal{S}}
    \textcolor{blue}{ \sum_{s_{1} \in \mathcal{S}}
    \textcolor{red}{ p_{o} ( o_{1} \vert s_{1} ) p_{0} ( s_{1} ) }
    p_{o} ( o_{2} \vert s_{2} ) p ( s_{2} \vert s_{1} ) }
    p_{o} ( o_{3} \vert s_{3} ) p ( s_{3} \vert s_{2} )
    = \sum_{s_{2} \in \mathcal{S}} \alpha_{2} ( s_{2} ) p_{o} ( o_{3} \vert s_{3} ) p ( s_{3} \vert s_{2} )
    \\
    &amp;amp; \; \vdots
    \\
    \alpha_{t} ( s_{t} )
    &amp;amp; = \Pr ( S_{t} = s_{t} , O_{1:t} = o_{1:t} )
    = \sum_{s_{t-1} \in \mathcal{S}} \alpha_{t-1} ( s_{t-1} )
    p_{o} ( o_{t} \vert s_{t} ) p ( s_{t} \vert s_{t-1} )
\end{align}\]

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Backward inference&lt;/strong&gt;: The conditional probability of \(o_{t+1:T}\) for a given \(s_{t}\).&lt;/li&gt;
&lt;/ul&gt;

\[\require{color}
\begin{align}
    \beta_{T} ( s_{T} )
    &amp;amp; = \Pr ( \cdot \vert S_{T} = s_{T} )
    \\
    &amp;amp; = \textcolor{red}{ p_{o} ( \cdot \vert \cdot ) p ( \cdot \vert s_{T} ) } = 1
    \\
    \beta_{T-1} ( s_{T-1} )
    &amp;amp; = \Pr ( O_{T} = o_{T} \vert S_{T-1} = s_{T-1} )
    \\
    &amp;amp; = \textcolor{blue}{ \sum_{s_{T} \in \mathcal{S}}
    \textcolor{red}{ p_{o} ( \cdot \vert \cdot ) p ( \cdot \vert s_{T} ) }
    p_{o} ( o_{T} \vert s_{T} ) p ( s_{T} \vert s_{T-1} ) }
    = \sum_{s_{T} \in \mathcal{S}} \beta_{T} ( s_{T} ) p_{o} ( o_{T} \vert s_{T} ) p ( s_{T} \vert s_{T-1} )
    \\
    \beta_{T-2} ( s_{T-2} )
    &amp;amp; = \Pr ( O_{T-1} = o_{T-1} , O_{T} = o_{T} \vert S_{T-2} = s_{T-2} )
    \\
    &amp;amp; = \sum_{s_{T-1} \in \mathcal{S}}
    \textcolor{blue}{ \sum_{s_{T} \in \mathcal{S}}
    \textcolor{red}{ p_{o} ( \cdot \vert \cdot ) p ( \cdot \vert s_{T} ) }
    p_{o} ( o_{T} \vert s_{T} ) p ( s_{T} \vert s_{T-1} ) }
    p_{o} ( o_{T-1} \vert s_{T-1} ) p ( s_{T-1} \vert s_{T-2} )
    \\
    &amp;amp; = \sum_{s_{T-1} \in \mathcal{S}} \beta_{T-1} ( s_{T-1} ) p_{o} ( o_{T-1} \vert s_{T-1} ) p ( s_{T-1} \vert s_{T-2} )
    \\
    &amp;amp; \; \vdots
    \\
    \beta_{t} ( s_{t} )
    &amp;amp; = \Pr ( O_{t+1:T} = o_{t+1:T} \vert S_{t} = s_{t} )
    = \sum_{s_{t+1} \in \mathcal{S}}
    \beta_{t+1} ( s_{t+1} ) p_{o} ( o_{t+1} \vert s_{t+1} ) p ( s_{t+1} \vert s_{t} )
\end{align}\]

&lt;p&gt;Together, we can derive two important inferences.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The probability of the system visiting \(s_{i}\) at \(t\) for a given \(o_{1:T}\).&lt;/p&gt;

\[\begin{align}
      \gamma_{t} ( s_{i} )
      &amp;amp; = \Pr ( S_{t} = s_{i} \vert O_{1:T} = o_{1:T} )
      \\
      &amp;amp; = \frac{\Pr ( S_{t} = s_{i} , O_{1:T} = o_{1:T} )}
      {\Pr ( O_{1:T} = o_{1:T} )}
      \\
      &amp;amp; = \frac{ \Pr ( S_{t} = s_{i} , O_{1:t} = o_{1:t} ) \Pr ( O_{t+1:T} = o_{t+1:T} \vert S_{t} = s_{i} ) }
      { \sum_{i'} \Pr ( S_{t} = s_{i'} , O_{1:t} = o_{1:t} ) \Pr ( O_{t+1:T} = o_{t+1:T} \vert S_{t} = s_{i'} ) }
      \\
      &amp;amp; = \frac{ \alpha_{t} ( s_{i} ) \beta_{t} ( s_{i} ) }
      { \sum_{s_{j} \in \mathcal{S}} \alpha_{t} ( s_{j} ) \beta_{t} ( s_{j} ) }
  \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;\(\sum_{t=1}^{T-1} \gamma_{t} ( s_{i} )\) is the expected number of the visitations to \(s_{i}\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The probability of \(s_{i}\) at \(t\) and \(s_{j}\) at \(t+1\) for a given \(o_{1:T}\).&lt;/p&gt;

\[\begin{align}
      \xi_{t} ( s_{i} , s_{j} )
      &amp;amp; = \Pr ( S_{t} = s_{i} , S_{t+1} = s_{j} \vert O_{1:T} = o_{1:T} )
      \\
      &amp;amp; = \frac{\Pr ( S_{t} = s_{i} , S_{t+1} = s_{j} , O_{1:T} = o_{1:T} )}
      {\Pr ( O_{1:T} = o_{1:T} )}
      \\
      &amp;amp; = \frac{ p_{o} ( o_{t+1} \vert s_{j} ) p ( s_{j} \vert s_{i} ) \alpha_{t} ( s_{i} ) \beta_{t+1} ( s_{j} ) }
      { \sum_{s_{l} \in \mathcal{S}} \sum_{s_{k} \in \mathcal{S}} p_{o} ( o_{t+1} \vert s_{l} ) p ( s_{l} \vert s_{k} ) \alpha_{t} ( s_{k} ) \beta_{t+1} ( s_{l} ) }
  \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;\(\sum_{t=1}^{T-1} \xi_{t} ( s_{i} , s_{j} )\) is the expected number of the visitations to \(s_{i}\) at \(t\) and \(s_{j}\) at \(t+1\).&lt;/li&gt;
      &lt;li&gt;\(\sum_{s_{j} \in \mathcal{S}} \xi_{t} ( s_{i} , s_{j} ) = \gamma_{t} ( s_{i} )\) since summing all the visitations to \(s_{j}\) at \(t+1\) in \(\xi_{t} ( s_{i} , s_{j} )\) only leaves the expected number of the visitations to \(s_{i}\) at \(t\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using these four basic inferences, we can infer hidden states in an HMM. Note that literatures often use notations, \(\pi_{i} = p_{0} ( S_{1} = s_{i} )\), \(a_{ij} = p ( X_{t} = s_{j} \vert X_{t-1} = s_{i} )\), and \(b_{j} ( o_{i} ) = p_{o} ( O_{t} = o_{i} \vert S_{t} = s_{j} )\).&lt;/p&gt;

&lt;h2 id=&quot;baum-welch-algorithm&quot;&gt;Baum-Welch Algorithm&lt;/h2&gt;

&lt;p&gt;Most real-world problems do not provide environment dynamics, so \(p_{0}\), \(p\) and \(p_{o}\) are unknown. The &lt;strong&gt;Baum-Welch algorithm&lt;/strong&gt;, a type of expectation maximization algorithm, allows us to obtain these environment dynamics.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Expectation step&lt;/p&gt;

\[\begin{align}
     \gamma_{t} ( s_{i} )
     = \frac{ \alpha_{t} ( s_{i} ) \beta_{t} ( s_{i} ) }
     { \sum_{s_{j} \in \mathcal{S}} \alpha_{t} ( s_{j} ) \beta_{t} ( s_{j} ) }
     \quad
     \xi_{t} ( s_{i} , s_{j} )
     = \frac{ p_{o} ( o_{t+1} \vert s_{j} ) p ( s_{j} \vert s_{i} ) \alpha_{t} ( s_{i} ) \beta_{t+1} ( s_{j} ) }
     { \sum_{s_{l} \in \mathcal{S}} \sum_{s_{k} \in \mathcal{S}} p_{o} ( o_{t+1} \vert s_{l} ) p ( s_{l} \vert s_{k} ) \alpha_{t} ( s_{k} ) \beta_{t+1} ( s_{l} ) }
 \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;\(\gamma_{t} ( s_{i} )\) is the probability of \(s_{i}\) at \(t\) for a given \(o_{1:T}\).&lt;/li&gt;
      &lt;li&gt;\(\xi_{t} ( s_{i} , s_{j} )\) is the probability of \(s_{i}\) at \(t\) and \(s_{j}\) at \(t+1\) for a given \(o_{1:T}\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maximization step&lt;/p&gt;

\[\begin{align}
     p_{0} ( s_{i} ) = \gamma_{1} ( s_{i} )
     \quad
     p ( s_{j} \vert s_{i} ) = \frac{ \sum_{t=1}^{T-1} \xi_{t} ( s_{i} , s_{j} ) }
     { \sum_{t=1}^{T-1} \gamma_{t} ( s_{i} ) }
     \quad
     p_{o} ( o_{i} \vert s_{i} ) = \frac{ \sum_{t=1}^{T-1} \gamma_{t} ( s_{i} ) \mathbb{1} ( o_{t} = o_{i} ) }
     { \sum_{t=1}^{T-1} \gamma_{t} ( s_{i} ) }
 \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;\(\gamma_{1} ( s_{i} )\) is the probability of \(s_{i}\) at \(t = 1\) for a given \(o_{1:T}\).&lt;/li&gt;
      &lt;li&gt;\(\frac{ \sum_{t=1}^{T-1} \xi_{t} ( s_{i} , s_{j} ) }{ \sum_{t=1}^{T-1} \gamma_{t} ( s_{i} ) }\) is the probability of \(s_{j}\) at \(t+1\) for a given \(s_{i}\) at \(t\) and \(o_{1:T}\).&lt;/li&gt;
      &lt;li&gt;\(\frac{ \sum_{t=1}^{T-1} \gamma_{t} ( s_{i} ) \mathbb{1} ( o_{t} = o_{i} ) }{ \sum_{t=1}^{T-1} \gamma_{t} ( s_{i} ) }\) is the probability of \(o_{i}\) at \(t\) for a given \(s_{i}\) at \(t\) and \(o_{1:T}\).&lt;/li&gt;
      &lt;li&gt;\(p_{0} ( s_{i} )\), \(p ( s_{j} \vert s_{i} )\) and \(p_{o} ( o_{i} \vert s_{i} )\) are normalized at each maximization step.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Since the probability is a fraction, multiplying them continuously will approach zero due to the precision. To avoid this underflow, often normalization is applied to \(\alpha\) and \(\beta\).&lt;/p&gt;

\[\begin{align}
    \hat{\alpha} ( s_{t} )
    = \frac{ \alpha ( s_{t} ) }{ \sum_{s_{tâ€™}} \alpha ( s_{tâ€™} ) }
    \quad
    \hat{\beta} ( s_{t} )
    = \frac{ \beta ( s_{t} ) }{ \sum_{s_{tâ€™}} \beta ( s_{tâ€™} ) }
\end{align}\]

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this post, Markov models with an autonomous system in a discrete-time setting are covered.&lt;/p&gt;

&lt;p&gt;A transition probability in an MC follows the Markov property.&lt;/p&gt;

\[\begin{align}
    \Pr ( S_{t+1} = s_{t+1} \vert S_{0:t} = s_{0:t} )
    \equiv p ( S_{t+1} = s_{t+1} \vert S_{t} = s_{t} )
    = p ( s_{t+1} \vert s_{t} )
\end{align}\]

&lt;p&gt;An MC is all about the properties, which can be determined by:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Is an MC countably finite or infinite?
    &lt;ul&gt;
      &lt;li&gt;If countably infinite MC, it can be any of transient, null recurrent or positive recurrent, more theories behind.
        &lt;ul&gt;
          &lt;li&gt;A limiting distribution and stationary distribution may or may not exist.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;If countably finite MC, it has one or more stationary distributions. Next question.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Is an MC absorbing or irreducible?
    &lt;ul&gt;
      &lt;li&gt;If absorbing MC, it is composed of positive recurrent absorbing states/chains and transient states.
        &lt;ul&gt;
          &lt;li&gt;A limiting distribution does not exist, but more than one stationary distribution exist.&lt;/li&gt;
          &lt;li&gt;Fundamental matrix, \(\mathbf{N}\):&lt;/li&gt;
        &lt;/ul&gt;

\[\begin{equation}
      \mathbf{N}
      = \sum_{k=0}^{\infty} \mathbf{Q}^{k}
      = ( \mathbf{I} - \mathbf{Q} )^{-1}
      \quad
      \text{where, }
      \mathbf{P}
      = \begin{bmatrix}
          \mathbf{Q} &amp;amp; \mathbf{R} \\
          \mathbf{0} &amp;amp; \mathbf{I} \\
      \end{bmatrix}
      ,
      \lim_{ k \rightarrow \infty } \mathbf{Q}^{k} = \mathbf{0}
  \end{equation}\]
      &lt;/li&gt;
      &lt;li&gt;If irreducible MC, it is composed of only positive recurrent states with no transient state. Next question.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Is an MC periodic or aperiodic?
    &lt;ul&gt;
      &lt;li&gt;If periodic MC, it returns to a particular state at some rates \(\{ n , 2n , 3n , \cdots \}\), where \(n &amp;gt; 1\).
        &lt;ul&gt;
          &lt;li&gt;A limiting distribution does not exist, but a single stationary distribution exists.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;If aperiodic MC, an MC is said to be an ergodic MC.
        &lt;ul&gt;
          &lt;li&gt;A limiting distribution exists and is equivalent to a stationary distribution.&lt;/li&gt;
        &lt;/ul&gt;

\[\begin{equation}
      \pi_{i}^{(\infty)} = \lim_{n \rightarrow \infty} \Pr ( S_{n} = s_{i} \vert S_{0} = s ) &amp;gt; 0
      \\
      \pi = \pi \mathbf{P}
      \equiv
      \pi^{(\infty)} = \pi^{(\infty)} \mathbf{P}
      \equiv
      \pi^{(0)} \mathbf{P}^{\infty}
  \end{equation}\]
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An emission probability in an HMM follows the Markov property.&lt;/p&gt;

\[\begin{align}
    \Pr ( O_{t} = o_{t} \vert S_{1:t} = s_{1:t} , O_{1:t-1} = o_{1:t-1} )
    \equiv p_{o} ( O_{t} = o_{t} \vert S_{t} = s_{t} )
    = p_{o} ( o_{t} \vert s_{t} )
\end{align}\]

&lt;p&gt;An HMM is all about the inferences.&lt;/p&gt;

\[\begin{align}
    \alpha_{1} ( s_{1} )
    &amp;amp; = \Pr ( S_{1} = s_{1} , O_{1} = o_{1} )
    = p_{o} ( o_{1} \vert s_{1} ) p_{0} ( s_{1} )
    \\
    \alpha_{t} ( s_{t} )
    &amp;amp; = \Pr ( S_{t} = s_{t} , O_{1:t} = o_{1:t} )
    = \sum_{s_{t-1} \in \mathcal{S}} \alpha_{t-1} ( s_{t-1} )
    p_{o} ( o_{t} \vert s_{t} ) p ( s_{t} \vert s_{t-1} )
    \\
    \beta_{t} ( s_{t} )
    &amp;amp; = \Pr ( O_{t+1:T} = o_{t+1:T} \vert S_{t} = s_{t} )
    = \sum_{s_{t+1} \in \mathcal{S}}
    \beta_{t+1} ( s_{t+1} ) p_{o} ( o_{t+1} \vert s_{t+1} ) p ( s_{t+1} \vert s_{t} )
    \\
    \beta_{T} ( s_{T} )
    &amp;amp; = \Pr ( \cdot \vert S_{T} = s_{T} )
    = p_{o} ( \cdot \vert \cdot ) p ( \cdot \vert s_{T} ) = 1
    \\
    \gamma_{t} ( s_{i} )
    &amp;amp; = \Pr ( S_{t} = s_{i} \vert O_{1:T} = o_{1:T} )
    = \frac{ \alpha_{t} ( s_{i} ) \beta_{t} ( s_{i} ) }
    { \sum_{s_{j} \in \mathcal{S}} \alpha_{t} ( s_{j} ) \beta_{t} ( s_{j} ) }
    \\
    \xi_{t} ( s_{i} , s_{j} )
    &amp;amp; = \Pr ( S_{t} = s_{i} , S_{t+1} = s_{j} \vert O_{1:T} = o_{1:T} )
    = \frac{ p_{o} ( o_{t+1} \vert s_{j} ) p ( s_{j} \vert s_{i} ) \alpha_{t} ( s_{i} ) \beta_{t+1} ( s_{j} ) }
    { \sum_{s_{l} \in \mathcal{S}} \sum_{s_{k} \in \mathcal{S}} p_{o} ( o_{t+1} \vert s_{l} ) p ( s_{l} \vert s_{k} ) \alpha_{t} ( s_{k} ) \beta_{t+1} ( s_{l} ) }
\end{align}\]

&lt;p&gt;Since the environment dynamics in an HMM, \(p_{0}\), \(p\) and \(p_{o}\), are usually unknown in practice, the Baum-Welch algorithm is used to estimate them.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Expectation step&lt;/p&gt;

\[\begin{align}
     \gamma_{t} ( s_{i} )
     = \frac{ \alpha_{t} ( s_{i} ) \beta_{t} ( s_{i} ) }
     { \sum_{s_{j} \in \mathcal{S}} \alpha_{t} ( s_{j} ) \beta_{t} ( s_{j} ) }
     \quad
     \xi_{t} ( s_{i} , s_{j} )
     = \frac{ p_{o} ( o_{t+1} \vert s_{j} ) p ( s_{j} \vert s_{i} ) \alpha_{t} ( s_{i} ) \beta_{t+1} ( s_{j} ) }
     { \sum_{s_{l} \in \mathcal{S}} \sum_{s_{k} \in \mathcal{S}} p_{o} ( o_{t+1} \vert s_{l} ) p ( s_{l} \vert s_{k} ) \alpha_{t} ( s_{k} ) \beta_{t+1} ( s_{l} ) }
 \end{align}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Maximization step&lt;/p&gt;

\[\begin{align}
     p_{0} ( s_{i} ) = \gamma_{1} ( s_{i} )
     \quad
     p ( s_{j} \vert s_{i} ) = \frac{ \sum_{t=1}^{T-1} \xi_{t} ( s_{i} , s_{j} ) }
     { \sum_{t=1}^{T-1} \gamma_{t} ( s_{i} ) }
     \quad
     p_{o} ( o_{i} \vert s_{i} ) = \frac{ \sum_{t=1}^{T-1} \gamma_{t} ( s_{i} ) \mathbb{1} ( o_{t} = o_{i} ) }
     { \sum_{t=1}^{T-1} \gamma_{t} ( s_{i} ) }
 \end{align}\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.columbia.edu/~ks20/stochastic-I/stochastic-I.html&quot;&gt;Lecture Notes on Stochastic Modeling I&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.stat.yale.edu/~jtc5/251/readings/Basics%20of%20Applied%20Stochastic%20Processes_Serfozo.pdf&quot;&gt;Basics of Applied Stochastic Processes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>D. K. Ryu</name></author><category term="Mathematics&amp;#x003a; Markov Model" /><summary type="html">A Markov chain is the simplest form of a Markov model, which describes the world with an autonomous system. It is used to model simulations, speech recognition, time series forecasting, bioinformatics, game theories and many more. This post aims to provide a tutorial on a Markov chain and introduce a commonly used variant, a hidden Markov model.</summary></entry><entry><title type="html">Markov Decision Process</title><link href="http://localhost:4000/posts/mathematics/markov_model_2_markov_decision_process/" rel="alternate" type="text/html" title="Markov Decision Process" /><published>2022-09-15T00:00:00+09:00</published><updated>2023-04-15T00:00:00+09:00</updated><id>http://localhost:4000/posts/mathematics/markov_model_2_markov_decision_process</id><content type="html" xml:base="http://localhost:4000/posts/mathematics/markov_model_2_markov_decision_process/">&lt;div class=&quot;notice--primary&quot;&gt;
  
&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;posts/mathematics/markov_model_1_markov_chain&quot;&gt;Markov Chain&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;

&lt;blockquote&gt;
    All is number.
    &lt;br /&gt;
    &lt;cite&gt;Pythagoras&lt;/cite&gt;
&lt;/blockquote&gt;

&lt;p&gt;Humans have invented and developed languages to symbolize entities, constructing &lt;em&gt;Matrix&lt;/em&gt; that captures the essence of reality. On the other hand, any form of entities can be modelled as &lt;em&gt;number&lt;/em&gt;, from natural to complex numbers and from points to objects, which existed long before human civilization. Thereby, the universe itself is inherently number, or in short, &lt;em&gt;all is number&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mathematics&lt;/strong&gt; is the study of numbers, so if an entity exists, there exists mathematics to describe it, even if we cannot perceive. This is why we (arguably) refer mathematics as &lt;em&gt;discovery&lt;/em&gt; instead of &lt;em&gt;invention&lt;/em&gt; and often considered the most fundamental and precise way of describing reality. Mathematics is a vast, diverse and abstract field such that it requires books of worth. Here, a few branches of mathematics are focused on.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;Markov model&lt;/strong&gt; is a stochastic process that describes pseudo-randomly changing systems under the &lt;strong&gt;Markov property&lt;/strong&gt;, or the Markov assumption. The Markov property stipulates the future state depends only on the current state, not the history of states. There are four common Markov models:&lt;/p&gt;

&lt;table class=&quot;table-centering&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;b&gt;Fully Observable&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;b&gt;Partially Observable&lt;/b&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt;
            &lt;b&gt;Autonomous System&lt;/b&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;a href=&quot;/posts/mathematics/markov_model_1_markov_chain/#markov-chain&quot;&gt;Markov Chain&lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;a href=&quot;/posts/mathematics/markov_model_1_markov_chain/#hidden-markov-model&quot;&gt;Hidden Markov Model&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 200px&quot;&gt; &lt;b&gt;Controllable System&lt;/b&gt; &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;a href=&quot;/posts/mathematics/markov_model_2_markov_decision_process/#markov-decision-process&quot;&gt;Markov Decision Process&lt;/a&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;a href=&quot;/posts/mathematics/markov_model_2_markov_decision_process/#partial-observabiliby&quot;&gt;Partially Observable Markov Decision Process&lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;A &lt;strong&gt;Markov decision process&lt;/strong&gt; is a type of Markov models used in decision making problems, which describes the world with a controllable system. It is used to model board games, robot locomotions, and more recently language models. This post aims to provide a tutorial on a Markov decision process and its variants, including a partially observable Markov decision process.&lt;/p&gt;

&lt;h1 id=&quot;decision-making&quot;&gt;Decision Making&lt;/h1&gt;

&lt;p&gt;Let the agent navigate the world and execute an action. To model decision making problems, we need to have the following three concepts.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A &lt;strong&gt;state&lt;/strong&gt;, \(s \in \mathcal{S}\), is a computational model of the world that the agent observes, i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hungry&lt;/code&gt; state, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is in front of me&lt;/code&gt; state, or an instance of a board game.&lt;/li&gt;
  &lt;li&gt;An &lt;strong&gt;action&lt;/strong&gt;, \(a \in \mathcal{A}\), is an incremental move that the agent takes to change the state, i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eat&lt;/code&gt; action for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hungry&lt;/code&gt; state, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attack&lt;/code&gt; action for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is in front of me&lt;/code&gt; state.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;reward&lt;/strong&gt;, \(r \in \mathcal{R}\), is a quantity that drives the agent to produce an appropriate action for a given state. It can be positive to encourage or negative to suppress a particular action, i.e. positive rewards on reaching &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;full&lt;/code&gt; state by executing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eat&lt;/code&gt; action from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hungry&lt;/code&gt; state while negative on reaching &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hungrier&lt;/code&gt; state by executing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt; action from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hungry&lt;/code&gt; state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, the model that accounts for these three concepts is intractable. Here, I will go through a Markov decision process in a countably finite discrete-time setting. It employs the Markov property to simplify the world into a tractable model. Note that an agent is a system that executes an action, so the term &lt;em&gt;agent&lt;/em&gt; is often used in decision making problems while the term &lt;em&gt;system&lt;/em&gt; is for autonomous MCs.&lt;/p&gt;

&lt;h2 id=&quot;markov-decision-process&quot;&gt;Markov Decision Process&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;Markov Decision Process (MDP)&lt;/strong&gt; is a Markov chain (MC) with &lt;strong&gt;actions for controllability&lt;/strong&gt; and &lt;strong&gt;rewards for motivation&lt;/strong&gt;. It is a tuple of \(( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma )\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action set, \(\mathcal{P}\) is the state transition probability, \(\mathcal{R}\) is the reward function, and \(\gamma\) is the discount factor.&lt;/p&gt;

&lt;p&gt;In an MDP, the agent ought to 1) execute actions, \(a \in \mathcal{A}\); 2) to navigate to the states, \(s \in \mathcal{S}\); 3) that hold positive rewards, \(r \in \mathcal{R}\). Due to the presence of \(\mathcal{A}\), an MDP has two main differences compared to an MC.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;There exists a probability distribution over actions for a given history of states and actions, referred to as a &lt;strong&gt;policy&lt;/strong&gt;, \(\pi ( A_{t} = a_{t} \vert S_{0:t} = s_{0:t} ) \in \Pi\).&lt;/li&gt;
  &lt;li&gt;A transition probability is often referred to as a &lt;strong&gt;state transition probability&lt;/strong&gt; and is a function of a history of states and actions, \(p ( S_{t+1} = s_{t+1} \vert S_{0:t} = s_{0:t} , A_{0:t} = a_{0:t} ) \in \mathcal{P}\).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thus, more precisely, in an MDP, the agent ought to 1) execute actions, \(a \in \mathcal{A}\); 2) following the policy, \(\pi \in \Pi\); 3) to navigate to the states, \(s \in \mathcal{S}\); 4) through state transition probabilities, \(p \in \mathcal{P}\); 5) that hold positive rewards, \(r \in \mathcal{R}\).&lt;/p&gt;

&lt;p&gt;Under the Markov property, \(\Pi\), \(\mathcal{P}\) and \(\mathcal{R}\) can be expressed as:&lt;/p&gt;

\[\begin{equation}
    \Pr ( A_{t} = a_{t} \vert S_{0:t} = s_{0:t} )
    \equiv \pi ( A_{t} = a_{t} \vert S_{t} = s_{t} )
    = \pi ( a_{t} \vert s_{t} , a_{t-1} )
    \\
    \Pr ( S_{t+1} = s_{t+1} \vert S_{0:t} = s_{0:t} , A_{0:t} = a_{0:t} )
    \equiv p ( S_{t+1} = s_{t+1} \vert S_{t} = s_{t} , A_{t} = a_{t} )
    = p ( s_{t+1} \vert s_{t} , a_{t} )
    \\
    r ( S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} )
    \equiv r ( S_{t} = s_{t} , A_{t-1} = a_{t-1} , S_{t-1} = s_{t-1} )
    = r ( s_{t} , a_{t-1} , s_{t-1} )
\end{equation}\]

&lt;p&gt;Note that a reward is expressed as \(\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}\), but this is a design choice, where some define it as \(\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) or \(\mathcal{R}: \mathcal{S} \rightarrow \mathbb{R}\).&lt;/p&gt;

&lt;p&gt;A collection of states and actions that the agent travelled for a given maximum time, \(T\), is referred to as a &lt;strong&gt;trajectory&lt;/strong&gt;, \(\tau = ( S_{0} = s_{0} , A_{0} = a_{0} , \cdots  A_{T-1} = a_{T-1} , S_{T} = s_{T} )\).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If \(T \rightarrow \infty\), an MDP is referred to as an &lt;strong&gt;average-reward continuous MDP&lt;/strong&gt;, where the task may continue forever. The environment usually provides a reward at each time step.&lt;/li&gt;
  &lt;li&gt;If \(T &amp;lt; \infty\), an MDP is referred to as a &lt;strong&gt;start-state episodic MDP&lt;/strong&gt;, where the task has a clear ending. The environment usually provides the sum of rewards at each episode.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, I will focus on an episodic MDP. An episode may finish if a player died in gameplay or a time step reaches the maximum time in robot locomotion.&lt;/p&gt;

&lt;p&gt;The objective of an MDP is to design an agent that produces the optimal policy, \(\pi^{\ast} \in \Pi\), that receives as high total cumulative rewards as possible in the shortest travel distance. Consider the following MDP.&lt;/p&gt;

&lt;table style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 750px&quot; colspan=&quot;2&quot;&gt;
            Markov decision process from
            &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Markov_Decision_Process.svg&quot;&gt;
                Wikimedia Commons
            &lt;/a&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot; rowspan=&quot;2&quot;&gt;
            &lt;img src=&quot;/assets/mathematics/markov_model_2_markov_decision_process/1_mdp.png&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 350px&quot;&gt;
            $$
            \begin{equation}
                \mathcal{S} = \{ s_{0} , s_{1} , s_{2} \}
                \qquad
                \mathcal{A} = \{ a_{0} , a_{1} \}
                \\
                r ( s_{0} , a_{1} , s_{2} ) = -1
                \qquad
                r ( s_{0} , a_{0} , s_{1} ) = +5
            \end{equation}
            $$
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 350px&quot;&gt;
            $$
            \begin{align}
                \mathbf{P} ( a_{0} )
                &amp;amp; = \begin{bmatrix}
                    p ( s_{0} \vert s_{0} , a_{0} ) &amp;amp; p ( s_{1} \vert s_{0} , a_{0} ) &amp;amp; p ( s_{2} \vert s_{0} , a_{0} ) \\
                    p ( s_{0} \vert s_{1} , a_{0} ) &amp;amp; p ( s_{1} \vert s_{1} , a_{0} ) &amp;amp; p ( s_{2} \vert s_{1} , a_{0} ) \\
                    p ( s_{0} \vert s_{2} , a_{0} ) &amp;amp; p ( s_{1} \vert s_{2} , a_{0} ) &amp;amp; p ( s_{2} \vert s_{2} , a_{0} ) \\
                \end{bmatrix}
                \\
                &amp;amp; = \begin{bmatrix}
                    0.5 &amp;amp; 0.0 &amp;amp; 0.5 \\
                    0.7 &amp;amp; 0.1 &amp;amp; 0.2 \\
                    0.4 &amp;amp; 0.0 &amp;amp; 0.6 \\
                \end{bmatrix}
                \\
                \mathbf{P} ( a_{1} )
                &amp;amp; = \begin{bmatrix}
                    p ( s_{0} \vert s_{0} , a_{1} ) &amp;amp; p ( s_{1} \vert s_{0} , a_{1} ) &amp;amp; p ( s_{2} \vert s_{0} , a_{1} ) \\
                    p ( s_{0} \vert s_{1} , a_{1} ) &amp;amp; p ( s_{1} \vert s_{1} , a_{1} ) &amp;amp; p ( s_{2} \vert s_{1} , a_{1} ) \\
                    p ( s_{0} \vert s_{2} , a_{1} ) &amp;amp; p ( s_{1} \vert s_{2} , a_{1} ) &amp;amp; p ( s_{2} \vert s_{2} , a_{1} ) \\
                \end{bmatrix}
                \\
                &amp;amp; = \begin{bmatrix}
                    0.0 &amp;amp; 0.0 &amp;amp; 1.0 \\
                    0.0 &amp;amp; 0.95 &amp;amp; 0.05 \\
                    0.3 &amp;amp; 0.3 &amp;amp; 0.4 \\
                \end{bmatrix}
            \end{align}
            $$
        &lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Since the objective is to acquire high rewards, the agent should move towards positive rewards and avoid negative rewards. Therefore, the optimal policy, \(\pi^{\ast} \in \Pi\) is:&lt;/p&gt;

\[\begin{equation}
    \pi^{\ast} ( a \vert s )
    = \begin{bmatrix}
        \pi ( a_{0} \vert s_{0} ) &amp;amp; \pi ( a_{1} \vert s_{0} ) \\
        \pi ( a_{0} \vert s_{1} ) &amp;amp; \pi ( a_{1} \vert s_{1} ) \\
        \pi ( a_{0} \vert s_{2} ) &amp;amp; \pi ( a_{1} \vert s_{2} ) \\
    \end{bmatrix}
    = \begin{bmatrix}
        0.0 &amp;amp; 1.0 \\
        1.0 &amp;amp; 0.0 \\
        0.0 &amp;amp; 1.0 \\
    \end{bmatrix}
\end{equation}\]

&lt;p&gt;A policy can be:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deterministic: The choice of action is non-probabilistic, so the next action is produced for a given state, or \(a = \pi ( a \vert s )\).&lt;/li&gt;
  &lt;li&gt;Stochastic: The choice of action is probabilistic, so the next action is sampled from policy distribution, or \(a \sim \pi ( a \vert s ) \in \mathbb{R}^{m}\), where \(\sum_{a \in \mathcal{A}} \pi ( a \vert s ) = 1\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Any type of policy can be used for any type of MDPs, i.e. a deterministic policy in a stochastic MDP or a stochastic MDP in a deterministic MDP.&lt;/p&gt;

&lt;p&gt;Both have pros and cons. In the optimal world, where you can fully observe how the environment dynamics work, which is the above example, the optimal policy is deterministic, but in the real world, the stochastic policy is usually preferred because:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deterministic policy lacks diversity (exploration-exploitation dilemma).&lt;/li&gt;
  &lt;li&gt;The agent does not fully observe states in many practical scenarios (partial observability).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A good example is &lt;a href=&quot;https://web.stanford.edu/class/cs234/CS234Win2019/slides/lnotes8.pdf&quot;&gt;an aliased gridworld from Stanford CS234 lecture&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ergodicity&quot;&gt;Ergodicity&lt;/h2&gt;

&lt;p&gt;The &lt;strong&gt;ergodicity&lt;/strong&gt; is a good property for an MDP to hold, but this is more complicated than an MC due to the presence of the actions and the policy. Consider that the agent is in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is in front of me&lt;/code&gt; state with two available actions, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attack&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt;. When the agent executes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attack&lt;/code&gt;, the agent may or may not defeat the enemy.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;With \(p (\) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is defeated&lt;/code&gt; \(\vert\) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is in front of me&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attack&lt;/code&gt; \()\), the agent reaches &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is defeated&lt;/code&gt; state.&lt;/li&gt;
  &lt;li&gt;With \(p (\) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy dodged attack&lt;/code&gt; \(\vert\) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is in front of me&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attack&lt;/code&gt; \()\), the agent reaches &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy dodged attack&lt;/code&gt; state.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The agent might require to execute &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attack&lt;/code&gt; until it reaches &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is defeated&lt;/code&gt; state. However, if the agent executes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt;, it will reach &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy killed you&lt;/code&gt; state, or \(p (\) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy killed you&lt;/code&gt; \(\vert\) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is in front of me&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt; \() = 1\). If the agent dies, it can never return to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is in front of me&lt;/code&gt; state again.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If there is even a small chance of selecting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt;, \(\pi (\) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt; \(\vert\) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is in front of me&lt;/code&gt; \() &amp;gt; 0\), an MDP has absorbing states or chains.&lt;/li&gt;
  &lt;li&gt;If there is no chance of selecting &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt;, \(\pi (\) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;wait&lt;/code&gt; \(\vert\) &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;an enemy is in front of me&lt;/code&gt; \() = 0\), an MDP has only positive recurrent states.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, unlike an MC, having absorbing states or chains does not mean that the MDP is absorbing. More formally, define a &lt;strong&gt;state stationary distribution&lt;/strong&gt;, stationary distribution or steady-state distribution, \(d ( s )\):&lt;/p&gt;

\[\begin{align}
    d ( s )
    = &amp;amp; \lim_{t \rightarrow \infty} \Pr (S_{t} = s \vert s_{0} , \pi)
    \\
    \equiv &amp;amp; \sum_{k=0}^{\infty} \Pr ( S_{0} = s_{0} \rightarrow S_{k} = s \vert k , \pi)
    = \sum_{k=0}^{\infty} \Pr (s_{0} \rightarrow s \vert k , \pi)
\end{align}\]

&lt;p&gt;Similar to an MC, it is the probability distribution over states that you will end up if you travel an MDP infinitely under \(\pi\).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The first term signifies that for a given \(S_{0} = s_{0}\), the probability that the agent travels to \(s\) when \(t \rightarrow \infty\) following \(\pi\).&lt;/li&gt;
  &lt;li&gt;The second term signifies the summation of the probability that the agent travels from \(S_{0} = s_{0}\) to \(s\) at \(k\) number of steps following \(\pi\) across \(k = 0\) to \(\infty\). This is often referred to as a &lt;strong&gt;state visitation probability&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

\[\begin{align}
    \Pr (s \rightarrow s \vert 0 , \pi )
    = &amp;amp; 1
    \\
    \Pr (s \rightarrow s' \vert 0 , \pi )
    = &amp;amp; 0
    \\
    \Pr (s \rightarrow s' \vert 1 , \pi )
    = &amp;amp; \sum_{a \in \mathcal{A}} \pi ( a \vert s ) p ( s' \vert s , a )
    \\
    \Pr (s \rightarrow s'' \vert k , \pi )
    = &amp;amp; \sum_{s' \in \mathcal{S}} \Pr (s \rightarrow s' \vert n, \pi ) \cdot \Pr (s' \rightarrow s'' \vert k - n , \pi )
\end{align}\]

&lt;p&gt;Furthermore, \(d\) can be represented recursively.&lt;/p&gt;

\[\begin{equation}
    d ( s' )
    = \sum_{s \in \mathcal{S}} d ( s )
    \sum_{a \in \mathcal{A}} \pi ( a \vert s ) p ( s' \vert s , a )
\end{equation}\]

&lt;p&gt;Before getting into the ergodicity, there is one additional class of an MDP. An &lt;strong&gt;unichain MDP&lt;/strong&gt; is an MDP that guarantees a unique stationary state distribution for every policy, but its visitation is not guaranteed for every state. This means that the unichain MDP has a unique stationary state distribution that contains \(d (s) = 0\) for some \(s\), therefore, not ergodic.&lt;/p&gt;

&lt;p&gt;There seems to be no formal definition on the ergodicity in an MDP, but three different definitions I found from &lt;a href=&quot;https://ai.stackexchange.com/questions/27196/what-is-ergodicity-in-a-markov-decision-process-mdp&quot;&gt;What is ergodicity in a Markov Decision Process (MDP)?&lt;/a&gt; are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;There exists a policy \(\pi\) for a unique stationary distribution, \(d ( s )\), such that \(d ( s ) &amp;gt; 0\) from &lt;a href=&quot;https://dl.acm.org/doi/10.5555/3042573.3042759&quot;&gt;Moldovan&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;For every policy \(\pi\), a unique stationary distribution exists, \(d ( s )\), such that \(d ( s ) &amp;gt; 0\) from &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887&quot;&gt;Puterman&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;For every policy \(\pi\), a unique stationary distribution exists, \(d ( s )\), such that \(d ( s ) \geq 0\) from &lt;a href=&quot;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&quot;&gt;Sutton&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since the policy is usually learned, any of the definitions has a risk of falling into absorbing states. Most reinforcement learning takes the third definition, but it is not truly ergodic since a limiting distribution may reach zero for some \(\pi\).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In many reinforcement learning problems, an ergodicity assumption breaks since closed irreducible sets are present in an MDP, i.e. the permanent damages to robots or the progress of the game stage. There is one interesting perspective on how the ergodicity deals with the safety of the agent, &lt;a href=&quot;https://dl.acm.org/doi/10.5555/3042573.3042759&quot;&gt;Safe Exploration in Markov Decision Processes by Moldovan&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The research towards ergodicity in an MDP appears to be more related to operations research, so if you are interested, take a look at operations research.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;value-function&quot;&gt;Value Function&lt;/h2&gt;

&lt;p&gt;The optimal policy is to guide the agent towards states with &lt;em&gt;the highest total cumulative rewards&lt;/em&gt;.&lt;/p&gt;

\[\begin{equation}
    \pi^{\ast} ( a \vert s ) = \arg \max_{\pi} \mathbb{E} [ \sum_{t = 0}^{\infty} r ( s_{t} ) ]
\end{equation}\]

&lt;p&gt;However, the agent must pursue the highest total cumulative rewards in &lt;em&gt;the shortest travel distance&lt;/em&gt;. \(\sum_{t = 0}^{\infty} r ( s_{t} )\) weights the significance of the rewards in short distance and long distance equal. Then, the agent may potentially:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Fall into an infinite loop, where it repetitively visits the same states with rewards.&lt;/li&gt;
  &lt;li&gt;Aim for only large but distant rewards, where it ignores many small rewards in a short distance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To comprehend the length of the travel distance into the optimal policy, we include a discount factor inside the optimization, often referred to as a &lt;strong&gt;return&lt;/strong&gt; that is the total discounted cumulative rewards that the agent received throughout the trajectory, \(R_{t} = \sum_{k = 0}^{\infty} \gamma^{k} r ( s_{t + k + 1} )\).&lt;/p&gt;

\[\begin{equation}
    \pi^{\ast} ( a \vert s )
    = \arg \max_{\pi} \mathbb{E} [ R_{t} ]
    = \arg \max_{\pi} \mathbb{E} [ \sum_{k = 0}^{\infty} \gamma^{k} r ( s_{t + k + 1} ) ]
\end{equation}\]

&lt;p&gt;Appropriately selecting \(\gamma \in [0,1]\) diminishes the effects of the long-distance rewards when computing the return.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If \(\gamma = 0\), the agent only cares about rewards in a single step, or &lt;em&gt;short-sighted&lt;/em&gt;, which may result in the agent not learning anything if the rewards are sparse.&lt;/li&gt;
  &lt;li&gt;If \(\gamma = 1\), the agent cares about rewards of the infinite horizon, or &lt;em&gt;long-sighted&lt;/em&gt;, which may fall into an infinite loop or care about high but far-to-reach rewards.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can view short-sighted and long-sighted from the &lt;em&gt;Stanford marshmallow experiment&lt;/em&gt;. A discount factor has a mathematical property that is bounded by \(\sum_{k = 0}^{\infty} \gamma^{k} = \frac{ 1 }{ 1 - \gamma }\). There are other ways to balance short- and long-sighted and they have different mathematic properties.&lt;/p&gt;

&lt;p&gt;Additionally, \(d\) also includes a discount factor in its expression.&lt;/p&gt;

\[\begin{align}
    d ( s )
    = &amp;amp; \lim_{t \rightarrow \infty} \gamma^{t} \Pr (S_{t} = s \vert s_{0} , \pi)
\end{align}\]

&lt;p&gt;To make the mathematical expression of the optimization more flexible for reinforcement learning algorithms, we use the expectation of a return, or a &lt;strong&gt;value function&lt;/strong&gt;.&lt;/p&gt;

\[\begin{equation}
    v ( s )
    = \mathbb{E} [ R_{t} \vert S = s ]
    \qquad
    q ( s , a )
    = \mathbb{E} [ R_{t} \vert S = s , A = a ]
\end{equation}\]

&lt;p&gt;Where \(v\) is a state value function or &lt;strong&gt;state value&lt;/strong&gt;, and \(q\) is a state-action value function or &lt;strong&gt;Q value&lt;/strong&gt;. \(v\) quantifies how good a particular state is based on the expected return from the state following the policy while \(q\) deals with a state and action pair.&lt;/p&gt;

&lt;p&gt;Thus, the optimal policy is expressed as:&lt;/p&gt;

\[\begin{align}
    \pi^{\ast} ( a \vert s )
    = &amp;amp; \arg \max_{\pi} \mathbb{E} [ R_{t} ]
    \\
    \equiv &amp;amp; \arg \max_{\pi} \mathbb{E} [ v ( s ) ]
    = \arg \max_{\pi} \sum_{s \in \mathcal{S}} d ( s ) v ( s )
    \\
    \equiv &amp;amp; \arg \max_{\pi} \mathbb{E} [ q ( s , a ) ]
    = \arg \max_{\pi} \sum_{s \in \mathcal{S}} d ( s ) \sum_{a \in \mathcal{A}} \pi ( a \vert s ) q ( s , a )
\end{align}\]

&lt;p&gt;An average-reward continuous MDP formalizes the value functions differently. It represents the &lt;em&gt;expected average reward without a discount factor&lt;/em&gt;.&lt;/p&gt;

\[\begin{equation}
    v ( s ) = \sum_{t = 1}^{\infty} \mathbb{E} \left[ r_{t} - R_{t} \vert S = s \right]
    \qquad
    q ( s , a ) = \sum_{t = 1}^{\infty} \mathbb{E} \left[ r_{t} - R_{t} \vert S = s , A = a \right]
\end{equation}\]

&lt;h2 id=&quot;bellman-equation&quot;&gt;Bellman Equation&lt;/h2&gt;

&lt;p&gt;In an MDP, the &lt;strong&gt;Bellman equation&lt;/strong&gt; is a recursion of value functions. The fundamental idea is more abstract, where it deals with a necessary condition for optimality in dynamic programming, but here, I will just cover the simplest application of the Bellman equation in an MDP.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;Bellman expectation equation&lt;/strong&gt; is a type of Bellman equations that decomposes the value function recursively into a reward and discounted future value function. Consider a white circle as a state and a black dot as an action:&lt;/p&gt;

&lt;table style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            State value
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            Q value
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            &lt;img src=&quot;/assets/mathematics/markov_model_2_markov_decision_process/2_1_be_1.png&quot; width=&quot;200px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            &lt;img src=&quot;/assets/mathematics/markov_model_2_markov_decision_process/2_1_be_2.png&quot; width=&quot;200px&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            $$
            \begin{align}
                v (s) = \sum_{a \in A} \pi (a \vert s) q (s, a)
            \end{align}
            $$
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 300px&quot;&gt;
            $$
            \begin{align}
                q (s, a) = \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v (s') \right)
            \end{align}
            $$
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Bellman expectation equation from
        &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf&quot;&gt;
            David Silver's lecture
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/table&gt;

&lt;p&gt;Two value functions can be combined to become recursive.&lt;/p&gt;

&lt;table style=&quot;margin-left: auto; margin-right: auto;&quot;&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            State value
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            Q value
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;img src=&quot;/assets/mathematics/markov_model_2_markov_decision_process/2_2_be_1.png&quot; width=&quot;250px&quot; /&gt;
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            &lt;img src=&quot;/assets/mathematics/markov_model_2_markov_decision_process/2_2_be_2.png&quot; width=&quot;250px&quot; /&gt;
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            $$
            \begin{align}
                v (s)
                = \sum_{a \in A} \pi (a \vert s)
                \left(
                \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v (s') \right)
                \right)
            \end{align}
            $$
        &lt;/td&gt;
        &lt;td style=&quot;text-align: center; vertical-align: middle; width: 400px&quot;&gt;
            $$
            \begin{align}
                q (s, a)
                = \sum_{s' \in S} p (s' \vert s , a)
                \left(
                r ( s , a , s' ) + \gamma \sum_{a' \in A} \pi (a' \vert s') q (s', a')
                \right)
            \end{align}
            $$
        &lt;/td&gt;
    &lt;/tr&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Bellman expectation equation 2 from
        &lt;a href=&quot;https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf&quot;&gt;
            David Silver's lecture
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/table&gt;

&lt;p&gt;The &lt;strong&gt;Bellman optimality equation&lt;/strong&gt; is another Bellman equation, which is basically the Bellman expectation equation with the greedy policy.&lt;/p&gt;

\[\begin{equation}
    \pi^{\ast} ( a \vert s )
    = \begin{cases}
      1 &amp;amp; \arg \max_{a} q ( s , a ) \\
      0 &amp;amp; \text{otherwise}
    \end{cases}
\end{equation}\]

&lt;p&gt;The greedy policy is a type of deterministic policies, driven by only the maximum \(q ( s , a )\). Then, the two value functions can be formulated as:&lt;/p&gt;

\[\begin{equation}
    v^{\ast} (s)
    = \sum_{a \in A} \pi^{\ast} (a \vert s) q^{\ast} (s, a)
    = \max_{a} q^{\ast} (s, a)
    \\
    q^{\ast} (s, a)
    = \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v^{\ast} (s') \right)
\end{equation}\]

&lt;p&gt;Furthermore, recursively:&lt;/p&gt;

\[\begin{equation}
    v^{\ast} (s)
    = \max_a
    \left(
    \sum_{s' \in S} p (s' \vert s , a) ( r ( s , a , s' ) + \gamma v^{\ast} (s'))
    \right)
    \\
    q^{\ast} (s, a)
    = \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma \max_{a'} q^{\ast} (s', a') \right)
\end{equation}\]

&lt;p&gt;Often notations vary:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(d^{\pi}\): The state stationary distribution following the policy, for instance, \(d^{\pi} ( s ) \neq d^{\pi'} ( s )\).&lt;/li&gt;
  &lt;li&gt;\(v_{\pi}\): The state value following the policy, for instance, \(v_{\pi} ( s ) \neq v_{\pi'} ( s )\).&lt;/li&gt;
  &lt;li&gt;\(R ( s_{t} , a_{t} )\): The return at \(s_{t}\) and \(a_{t}\), for instance, \(R_{t} = R ( s_{t} , a_{t} )\).&lt;/li&gt;
  &lt;li&gt;\(\mathbb{E}_{ \tau \sim \pi }\): The expectation such that the trajectories are sampled following the policy, for instance, \(v_{\pi} (s) = \mathbb{E}_{ \tau \sim \pi } [ R_{t} \vert S = s ]\).&lt;/li&gt;
  &lt;li&gt;\(\mathbb{E}_{ s \sim d^{\pi} , a \sim \pi }\): The expectation such that the state is sampled from the state stationary distribution and the action is from the policy, for instance, \(\mathbb{E}_{ \tau \sim \pi } [ R_{t} \vert S = s ] = \mathbb{E}_{ s \sim d^{\pi} , a \sim \pi } [ R_{t} \vert S = s ]\).&lt;/li&gt;
  &lt;li&gt;The mathematical expressions of the Bellman expectation and optimality equations might be different from other articles or lectures. This is because a reward function takes different inputs, i.e. \(r ( s , a )\) or \(r ( s )\), but there is no practical difference.&lt;/li&gt;
  &lt;li&gt;The exact same notations are applied to the Q value.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;extensions-to-markov-decision-process&quot;&gt;Extensions to Markov Decision Process&lt;/h1&gt;

&lt;p&gt;An MDP is a fully observable fixed environment with only a single player. There are a number of extended MDPs commonly used in reinforcement learning.&lt;/p&gt;

&lt;h2 id=&quot;partial-observability&quot;&gt;Partial Observability&lt;/h2&gt;

&lt;p&gt;A &lt;strong&gt;partially observable Markov decision process (POMDP)&lt;/strong&gt; is an MDP with partial observability. Instead of observing states directly, the agent receives an observation, infers a state and executes an action. A POMDP can be seen as an HMM version of an MDP, a tuple of \(( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{O}, \mathcal{P}_{o}, \mathcal{R}, \gamma )\), where \(\mathcal{O}\) is the observation space and \(\mathcal{P}_{o}\) is the conditional observation probability. \(\mathcal{P}_{o}\) also carries the Markov property.&lt;/p&gt;

\[\begin{align}
    \Pr ( O_{t} = o_{t} \vert S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} )
    \equiv &amp;amp; p_{o} ( O_{t} = o_{t} \vert S_{t} = s_{t} , A_{t-1} = a_{t-1} )
    \\
    = &amp;amp; p_{o} ( o_{t} \vert s_{t} , a_{t} )
\end{align}\]

&lt;p&gt;The conditional observation probability can be written as \(\mathcal{P}_{o} : \mathcal{S} \rightarrow \mathcal{O}\) or \(\mathcal{P}_{o} : \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{O}\), but others share the same with MDP.&lt;/p&gt;

&lt;p&gt;In the classic POMDP, a &lt;strong&gt;belief&lt;/strong&gt;, \(b (s)\), is used to infer the state, which is a probability distribution over states. Then, the state can be inferred from:&lt;/p&gt;

\[\begin{equation}
    b'( s' ) = \eta \cdot p_{o} ( o' \vert s' , a ) \sum_{ s \in \mathcal{S} } p ( s' \vert s , a ) b ( s )
\end{equation}\]

&lt;p&gt;Where \(\eta = \frac{1}{ \Pr ( o \vert b , a ) }\) is a normalizing constant with:&lt;/p&gt;

\[\begin{equation}
    \Pr ( o \vert b , a ) = \sum_{ s' \in \mathcal{S} } p_{o} ( o' \vert s' , a ) \sum_{ s \in \mathcal{S} } p ( s' \vert s , a ) b ( s )
\end{equation}\]

&lt;ul&gt;
  &lt;li&gt;\(\sum_{ s \in \mathcal{S} } p ( s' \vert s , a ) b ( s )\): For an action that we executed, we multiply the state probability in the belief to the state transition probability for the next state, then we sum all across every state. This will give us the probability that we will reach at \(s'\) for a given \(a\), or \(\Pr ( s' \vert a )\).&lt;/li&gt;
  &lt;li&gt;\(p_{o} ( o' \vert s' , a ) \Pr ( s' \vert a )\): For an acquired observation \(o'\), we can multiply the conditional observation probability of \(o'\) for \(s'\) and \(a\) pair to \(\Pr ( s' \vert a )\) to get the unnormalized probability that we are in \(s'\) for a given \(o'\) and \(a\). This is the unnormalized belief.&lt;/li&gt;
  &lt;li&gt;\(b( s )\): A probability distribution over states that determines where the agent would be at the current time step.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;However, in deep reinforcement learning, instead, we often use a neural network as a function that maps from the observation space to the state space, \(f : \mathcal{O} \rightarrow \mathcal{S}\).&lt;/p&gt;

&lt;h2 id=&quot;other-environments&quot;&gt;Other Environments&lt;/h2&gt;

&lt;p&gt;A more general setting to a POMDP is a &lt;strong&gt;decentralized POMDP (Dec-POMDP)&lt;/strong&gt;, where multiple agents are involved in decision making. It considers uncertainty in outcomes, sensors and communications. A Dec-POMDP is a 7-tuple \(( \mathcal{S}, \{ \mathcal{A}_{i} \}, \mathcal{P}, \{ \mathcal{O}_{i} \}, \mathcal{P}_{o}, \mathcal{R}, \gamma )\), where there are \(i\) number of agents, so \(i\) number of acquirable observations and executable actions. Hence, a POMDP is a special case of a Dec-POMDP with a single agent.&lt;/p&gt;

&lt;p&gt;Further generalization on Dec-POMDP is a &lt;strong&gt;partially observable stochastic game (POSG)&lt;/strong&gt;. The agents in a POSG hold different rewards, \(( \mathcal{S}, \{ \mathcal{A}_{i} \}, \mathcal{P}, \{ \mathcal{O}_{i} \}, \mathcal{P}_{o}, \{ \mathcal{R}_{i} \}, \gamma )\), so they either compete or cooperate in pursuing their assigned rewards. The terminology is from a game theory, where a &lt;strong&gt;stochastic game&lt;/strong&gt;, or &lt;strong&gt;Markov game&lt;/strong&gt;, refers to a repeated game with probabilistic transitions played by one or more players while partially observable is equivalent to adding observations and emission probabilities to the stochastic game.&lt;/p&gt;

&lt;p&gt;An MDP can also hold non-stationarity, referred to as a &lt;strong&gt;non-stationary MDP (NSMDP)&lt;/strong&gt;. It is simply a varying MDP over time or epoch, formally \(\mathcal{M}^{(i)} = ( \mathcal{S}^{(i)}, \mathcal{A}^{(i)}, \mathcal{P}^{(i)}, \mathcal{R}^{(i)}, \gamma^{(i)} )\), where there is \(i \in \mathbb{R}\). A special case of an NSMDP is used for multi-task reinforcement learning, meta reinforcement learning, and continual reinforcement learning, i.e. \(\mathcal{M}^{(i)} =  ( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}^{(i)}, \gamma )\) if the environment does not change. Note that this can also be applied to a POMDP, \(\mathcal{M}^{(i)} = ( \mathcal{S}^{(i)}, \mathcal{A}^{(i)}, \mathcal{P}^{(i)}, \mathcal{O}^{(i)}, \mathcal{P}_{o}^{(i)}, \mathcal{R}^{(i)}, \gamma^{(i)} )\).&lt;/p&gt;

&lt;p&gt;If you are interested in further generalizations in an MDP, please read &lt;a href=&quot;http://rbr.cs.umass.edu/camato/decpomdp/overview.html&quot;&gt;this blog&lt;/a&gt;.&lt;/p&gt;

&lt;!-- Papers that formally define this notation are,
- &lt;a href=&quot;https://proceedings.neurips.cc/paper/2019/file/859b00aec8885efc83d1541b52a1220d-Paper.pdf&quot;&gt;Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based Reinforcement Learning&lt;/a&gt;
- &lt;a href=&quot;http://proceedings.mlr.press/v119/chandak20a/chandak20a.pdf&quot;&gt;Optimizing for the Future in Non-Stationary MDPs&lt;/a&gt; --&gt;

&lt;h1 id=&quot;environments&quot;&gt;Environments&lt;/h1&gt;

&lt;p&gt;Environments can be modelled with few properties.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Environment can be:
    &lt;ul&gt;
      &lt;li&gt;Stochastic: Multiple states can be reached via an action for a given state, or there exist two or more states that have \(p ( s' \vert s , a ) \in (0, 1)\).&lt;/li&gt;
      &lt;li&gt;Deterministic: Only a single state can be reached via an action for a given state, or there exists a single state that has \(p ( s' \vert s , a ) = 1\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A state, action and time can be:
    &lt;ul&gt;
      &lt;li&gt;Discrete: State space, action space and time are countable, or there exists \(\mathcal{S} = \{ s_{i} \}_{i = 0}^{N}\), \(\mathcal{A} = \{ a_{i} \}_{i = 0}^{M}\), and \(\mathcal{T} = \{ t_{i} \}_{i = 0}^{K}\), where \(\sum_{s \in \mathcal{S}} \sum_{a \in \mathcal{A}} p (s' \vert s , a) = 1\) and \(\sum_{a \in \mathcal{A}} \pi (s \vert a) = 1\).&lt;/li&gt;
      &lt;li&gt;Continuous: State space, action space and time are measurable, or there exists \(\mathcal{S} = \mathbb{R}^{n}\), \(\mathcal{A} = \mathbb{R}^{m}\), and \(\mathcal{T} = \mathbb{R}\), where \(\int_{S} \int_{A} p (s' \vert s , a) \text{ d}a \text{ d}s = 1\) and \(\int_{A} \pi (s \vert a) \text{ d}a = 1\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that discrete state space is interpreted as there exists \(N\) number of states while in continuous, an infinitely many (uncountable) number of states exist in \(n\) space.&lt;/p&gt;

&lt;h2 id=&quot;fully-observable-environments&quot;&gt;Fully Observable Environments&lt;/h2&gt;

&lt;p&gt;There are two typical fully observable environments.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Gridworld&lt;/strong&gt;: Fully Observable Discrete Deterministic MDP
    &lt;ul&gt;
      &lt;li&gt;The agent ought to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;navigate (action)&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a given location of the player (state)&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;solve the game (reward)&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 300px&quot;&gt;
    &lt;img src=&quot;/assets/mathematics/markov_model_2_markov_decision_process/3_1_gridworld.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Gridworld from
        &lt;a href=&quot;https://www.mathworks.com/help/reinforcement-learning/ug/create-custom-grid-world-environments.html&quot;&gt;
            MathWorks
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Game Go&lt;/strong&gt; in the two-player board game: Fully Observable Discrete Stochastic MDP
    &lt;ul&gt;
      &lt;li&gt;The agent ought to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;place a stone (action)&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a given location of all the stones (state)&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;win the game (reward)&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Many multi-player board games, including game Go and Chess, can be seen as either deterministic with multi-agent or stochastic with single-agent settings. For instance, the opponents can be seen as environment dynamics that induce stochasticity in the environment or other agents acting on the deterministic environment.&lt;/li&gt;
      &lt;li&gt;Many multi-player card games are often considered as discrete stochastic MDP with partial observability, a.k.a. a POMDP, or with no observation until the game finishes, a.k.a. an episodic MDP with a return at the end of the game.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 150px&quot;&gt;
    &lt;img src=&quot;/assets/mathematics/markov_model_2_markov_decision_process/3_1_game_go.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Game Go from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:Gokof.png&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;partially-observable-environments&quot;&gt;Partially Observable Environments&lt;/h2&gt;
&lt;p&gt;If the agent takes the state as an input, the environment becomes fully observable.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Zork I&lt;/strong&gt; in the text-based game: Discrete Stochastic POMDP
    &lt;ul&gt;
      &lt;li&gt;The agent ought to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;execute a textual command (action)&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a given textual description of the world (observation)&lt;/code&gt; generated from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;the world graph (state)&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;solve puzzles in the game (reward)&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Many text-based games are either deterministic or mild stochastic as they are designed to solve a set of specific tasks with limited causal relationships between entities. It is practically impossible to create infinitely many tasks with infinitely many causal relationships between entities.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 500px&quot;&gt;
    &lt;img src=&quot;/assets/mathematics/markov_model_2_markov_decision_process/3_2_zork1.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Zork I from
        &lt;a href=&quot;https://www.oldgames.sk/en/gallery.php?image=7594&quot;&gt;
            Old Games
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Space Invader&lt;/strong&gt; in the arcade learning environment: Continuous state and Discrete action Stochastic POMDP
    &lt;ul&gt;
      &lt;li&gt;The agent ought to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;navigate or attack (action)&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a given image of the world (observation)&lt;/code&gt; generated from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;the location of the player, enemies and items (state)&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;defeat the enemies (reward)&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;Similar to the game Go, every enemy can be seen as another fixed agent, but the environment is still stochastic since where the enemies and items appear is probabilistic.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 300px&quot;&gt;
    &lt;img src=&quot;/assets/mathematics/markov_model_2_markov_decision_process/3_2_space_invader.png&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Space invader from
        &lt;a href=&quot;https://en.wikipedia.org/wiki/File:SpaceInvaders-Gameplay.gif&quot;&gt;
            Wikipedia
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Robot Locomotion&lt;/strong&gt; in the Open AI Gym: Continuous Deterministic POMDP.
    &lt;ul&gt;
      &lt;li&gt;The agent ought to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;move joints (action)&lt;/code&gt; for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;a given image of the world (observation)&lt;/code&gt; generated from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;the coordinates of the joints (state)&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;locomote as intended (reward)&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;If realistic factors are accounted, i.e. winds and terrains, it becomes stochastic.&lt;/li&gt;
      &lt;li&gt;Similar to the space invader, it can be a multi-agent setting if two or more robots are used to interact.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;align-center&quot; style=&quot;width: 300px&quot;&gt;
    &lt;img src=&quot;/assets/mathematics/markov_model_2_markov_decision_process/3_2_robot_locomotion.gif&quot; /&gt;
    &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;
        Robot locomotion from
        &lt;a href=&quot;https://commons.wikimedia.org/wiki/File:F4-motion.gif&quot;&gt;
            Wikimedia Commons
        &lt;/a&gt;
    &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;A time is discretized so that the agent can make a decision per time step. Note that by changing some factors, the environment can hold any property, i.e. gridworld with multi-agent setting or robot locomotion with a discrete action space \(\mathcal{A} = \{ 5 \text{m/s}, 10 \text{m/s} \}\). However, regardless of these properties, the policy and return are applied, so in practice:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;All the environments are formalized as a stochastic setting.&lt;/li&gt;
  &lt;li&gt;In discrete MDP, the state is embedded using a high dimensional vector.&lt;/li&gt;
  &lt;li&gt;In POMDP, the agent infers a high dimensional state, i.e. embedding or vector space, from observation using an encoder.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this post, Markov models with a controllable system in a discrete-time setting are covered.&lt;/p&gt;

&lt;p&gt;An MDP is a tuple of \(( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma )\), where a controllable system, or an agent, ought to 1) execute actions, \(a \in \mathcal{A}\); 2) following the policy, \(\pi \in \Pi\); 3) to navigate to the states, \(s \in \mathcal{S}\); 4) through state transition probabilities, \(p \in \mathcal{P}\); 5) that hold positive rewards, \(r \in \mathcal{R}\).&lt;/p&gt;

&lt;p&gt;A policy, state transition probability and reward in an MDP follow the Markov property.&lt;/p&gt;

\[\begin{equation}
    \Pr ( A_{t} = a_{t} \vert S_{0:t} = s_{0:t} )
    \equiv \pi ( A_{t} = a_{t} \vert S_{t} = s_{t} )
    = \pi ( a_{t} \vert s_{t} , a_{t-1} )
    \\
    \Pr ( S_{t+1} = s_{t+1} \vert S_{0:t} = s_{0:t} , A_{0:t} = a_{0:t} )
    \equiv p ( S_{t+1} = s_{t+1} \vert S_{t} = s_{t} , A_{t} = a_{t} )
    = p ( s_{t+1} \vert s_{t} , a_{t} )
    \\
    r ( S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} )
    \equiv r ( S_{t} = s_{t} , A_{t-1} = a_{t-1} , S_{t-1} = s_{t-1} )
    = r ( s_{t} , a_{t-1} , s_{t-1} )
\end{equation}\]

&lt;p&gt;A collection of states and actions that the agent travelled for a given maximum time, \(T\), is referred to as a &lt;strong&gt;trajectory&lt;/strong&gt;, \(\{ S_{0} = s_{0} , A_{1} = a_{1} , \cdots  A_{T-1} = a_{T-1} , S_{T} = s_{T} \}\).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If \(T \rightarrow \infty\), an MDP is referred to as a &lt;strong&gt;continuous average-reward MDP&lt;/strong&gt;, where the task may continue forever.&lt;/li&gt;
  &lt;li&gt;If \(T &amp;lt; \infty\), an MDP is referred to as an &lt;strong&gt;episodic start-state MDP&lt;/strong&gt;, where the task has a clear ending.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An MDP is all about the policy. A policy can be:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Deterministic: The choice of action is non-probabilistic, so the next action is produced for a given state, or \(a = \pi ( a \vert s )\). Ideal for the optimal world, where environment dynamics are fully observed.&lt;/li&gt;
  &lt;li&gt;Stochastic: The choice of action is probabilistic, so the next action is sampled from policy distribution, or \(a \sim \pi ( a \vert s ) \in \mathbb{R}^{m}\), where \(\sum_{a \in \mathcal{A}} \pi ( a \vert s ) = 1\). Ideal for the real world, where environment dynamics are not fully observed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An MDP also has the concept of ergodicity, but the definition varies by literature.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;There exists a policy \(\pi\) for a unique stationary distribution, \(d ( s )\), such that \(d ( s ) &amp;gt; 0\) from &lt;a href=&quot;https://dl.acm.org/doi/10.5555/3042573.3042759&quot;&gt;Moldovan&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;For every policy \(\pi\), a unique stationary distribution exists, \(d ( s )\), such that \(d ( s ) &amp;gt; 0\) from &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887&quot;&gt;Puterman&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;For every policy \(\pi\), a unique stationary distribution exists, \(d ( s )\), such that \(d ( s ) \geq 0\) from &lt;a href=&quot;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&quot;&gt;Sutton&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Where:
    &lt;ul&gt;
      &lt;li&gt;A state stationary distribution, stationary distribution or steady-state distribution, \(d ( s )\):&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{align}
      d ( s )
      = &amp;amp; \lim_{t \rightarrow \infty} \Pr (S_{t} = s \vert s_{0} , \pi)
      \equiv \sum_{k=0}^{\infty} \Pr (s_{0} \rightarrow s \vert k , \pi)
      \\
      = &amp;amp; \sum_{s' \in \mathcal{S}} d ( s' )
      \sum_{a' \in \mathcal{A}} \pi ( a' \vert s' ) p ( s \vert s' , a' )
  \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;A state visitation probability, \(\Pr (s_{0} \rightarrow s \vert k , \pi )\):&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{align}
      \Pr (s \rightarrow s' \vert 1 , \pi )
      = \sum_{a \in \mathcal{A}} \pi ( a \vert s ) p ( s' \vert s , a )
      \qquad
      \Pr (s \rightarrow s'' \vert k , \pi )
      = \sum_{s' \in \mathcal{S}} \Pr (s \rightarrow s' \vert n, \pi ) \cdot \Pr (s' \rightarrow s'' \vert k - n , \pi )
  \end{align}\]
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A policy is all about the rewards.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The optimal policy is the policy that acquires the highest total cumulative rewards.&lt;/p&gt;

\[\begin{equation}
     \pi^{\ast} ( a \vert s ) = \arg \max_{\pi} \mathbb{E} [ \sum_{t = 0}^{\infty} r ( s_{t} ) ]
 \end{equation}\]
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The optimal policy is the policy that acquires the highest total cumulative rewards in the shortest travel distance. A return is to comprehend both the total cumulative rewards and the travel length into the optimal policy.&lt;/p&gt;

\[\begin{equation}
     \pi^{\ast} ( a \vert s )
     = \arg \max_{\pi} \mathbb{E} [ R_{t} ]
     = \arg \max_{\pi} \mathbb{E} [ \sum_{k = 0}^{\infty} \gamma^{k} r ( s_{t + k + 1} ) ]
 \end{equation}\]

    &lt;ul&gt;
      &lt;li&gt;If \(\gamma = 0\), the agent only cares about rewards in a single step, or &lt;em&gt;short-sighted&lt;/em&gt;, which may result in the agent not learning anything if the rewards are sparse.&lt;/li&gt;
      &lt;li&gt;If \(\gamma = 1\), the agent cares about rewards of the infinite horizon, or &lt;em&gt;long-sighted&lt;/em&gt;, which may fall into an infinite loop or care about high but far-to-reach rewards.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To make the mathematical expression of the optimization more flexible for reinforcement learning algorithms, we use the expectation of a return, or a value function. There are two types of value functions, a state value, \(v ( s ) = \mathbb{E} [ R_{t} \vert S = s ]\), and Q value, \(q ( s , a ) = \mathbb{E} [ R_{t} \vert S = s , A = a ]\). \(v\) quantifies how good a particular state is based on the expected return from the state following the policy while \(q\) deals with a state and action pair.&lt;/p&gt;

\[\begin{align}
     \pi^{\ast} ( a \vert s )
     \equiv &amp;amp; \arg \max_{\pi} \mathbb{E} [ v ( s ) ]
     = \arg \max_{\pi} \sum_{s \in \mathcal{S}} d ( s ) v ( s )
     \\
     \equiv &amp;amp; \arg \max_{\pi} \mathbb{E} [ q ( s , a ) ]
     = \arg \max_{\pi} \sum_{s \in \mathcal{S}} d ( s ) \sum_{a \in \mathcal{A}} \pi ( a \vert s ) q ( s , a )
 \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;The Bellman expectation equation decomposes the value function recursively into a reward and discounted future value function.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{align}
     v (s)
     = &amp;amp; \sum_{a \in A} \pi (a \vert s) q (s, a)
     = \sum_{a \in A} \pi (a \vert s)
     \left(
     \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v (s') \right)
     \right)
     \\
     q (s, a)
     = &amp;amp; \sum_{s' \in S} p (s' \vert s , a)
     \left(
     r ( s , a , s' ) + \gamma v (s')
     \right)
     = \sum_{s' \in S} p (s' \vert s , a)
     \left(
     r ( s , a , s' ) + \gamma \sum_{a' \in A} \pi (a' \vert s') q (s', a')
     \right)
 \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;The Bellman optimality equation is the Bellman expectation equation with the greedy policy.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{align}
     v^{\ast} (s)
     = &amp;amp; \sum_{a \in A} \pi^{\ast} (a \vert s) q^{\ast} (s, a)
     = \max_{a} q^{\ast} (s, a)
     = \max_a
     \left(
     \sum_{s' \in S} p (s' \vert s , a) ( r ( s , a , s' ) + \gamma v^{\ast} (s'))
     \right)
     \\
     q^{\ast} (s, a)
     = &amp;amp; \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v^{\ast} (s') \right)
     = \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma \max_{a'} q^{\ast} (s', a') \right)
 \end{align}\]
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Some MDP variants are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Partially observable Markov decision process (POMDP): A tuple of \(( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{O}, \mathcal{P}_{o}, \mathcal{R}, \gamma )\)&lt;/p&gt;

\[\begin{align}
      \Pr ( O_{t} = o_{t} \vert S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} , O_{0:t-1} = o_{0:t-1} )
      \equiv p_{o} ( O_{t} = o_{t} \vert S_{t} = s_{t} , A_{t-1} = a_{t-1} )
      = p_{o} ( o_{t} \vert s_{t} , a_{t} )
  \end{align}\]

    &lt;ul&gt;
      &lt;li&gt;A belief is used to infer the state.&lt;/li&gt;
    &lt;/ul&gt;

\[\begin{equation}
      b'( s' ) = \eta p_{o} ( o' \vert s' , a ) \sum_{ s \in \mathcal{S} } p ( s' \vert s , a ) b ( s )
      \quad
      \text{where, } \eta = \frac{1}{ \Pr ( o \vert b , a ) }
  \end{equation}\]

\[\begin{equation}
      \Pr ( o \vert b , a ) = \sum_{ s' \in \mathcal{S} } p_{o} ( o' \vert s' , a ) \sum_{ s \in \mathcal{S} } p ( s' \vert s , a ) b ( s )
  \end{equation}\]
  &lt;/li&gt;
  &lt;li&gt;Decentralized POMDP (Dec-POMDP): A tuple of \(( \mathcal{S}, \{ \mathcal{A}_{i} \}, \mathcal{P}, \{ \mathcal{O}_{i} \}, \mathcal{P}_{o}, \mathcal{R}, \gamma )\)&lt;/li&gt;
  &lt;li&gt;Partially observable stochastic game (POSG): A tuple of \(( \mathcal{S}, \{ \mathcal{A}_{i} \}, \mathcal{P}, \{ \mathcal{O}_{i} \}, \mathcal{P}_{o}, \{ \mathcal{R}_{i} \}, \gamma )\)&lt;/li&gt;
  &lt;li&gt;Non-stationary MDP (NSMDP): A tuple of \(\mathcal{M}^{(i)} = ( \mathcal{S}^{(i)}, \mathcal{A}^{(i)}, \mathcal{P}^{(i)}, \mathcal{R}^{(i)}, \gamma^{(i)} )\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Lecture 1 and 2 of &lt;a href=&quot;https://www.davidsilver.uk/teaching/&quot;&gt;David Silverâ€™s lecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Chapter 3 in &lt;a href=&quot;https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other helpful resources are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse-fa15/docs/mdp-cheatsheet.pdf&quot;&gt;MDP Cheatsheet Reference from John Schulman&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>D. K. Ryu</name></author><category term="Mathematics&amp;#x003a; Markov Model" /><summary type="html">A Markov decision process is a type of Markov models used in decision making problems, which describes the world with a controllable system. It is used to model board games, robot locomotions, and more recently language models. This post aims to provide a tutorial on a Markov decision process and its variants, including a partially observable Markov decision process.</summary></entry></feed>