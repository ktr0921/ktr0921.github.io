<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Value-based Reinforcement Learning - Kelâ€™Logg</title>
<meta name="description" content="Value-based reinforcement learning is one of two fundamental classes of reinforcement learning algorithms. It implicitly constructs the policy based on estimated value functions, so its objective is to correctly estimate value functions. This post aims to provide a tutorial on value-based reinforcement learning algorithms.">


  <meta name="author" content="D. K. Ryu">
  
  <meta property="article:author" content="D. K. Ryu">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Kel'Logg">
<meta property="og:title" content="Value-based Reinforcement Learning">
<meta property="og:url" content="http://localhost:4000/posts/machine_learning/decision_1_value_based_reinforcement_learning/">


  <meta property="og:description" content="Value-based reinforcement learning is one of two fundamental classes of reinforcement learning algorithms. It implicitly constructs the policy based on estimated value functions, so its objective is to correctly estimate value functions. This post aims to provide a tutorial on value-based reinforcement learning algorithms.">







  <meta property="article:published_time" content="2022-09-15T00:00:00+09:00">



  <meta property="article:modified_time" content="2023-04-15T00:00:00+09:00">




<link rel="canonical" href="http://localhost:4000/posts/machine_learning/decision_1_value_based_reinforcement_learning/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Kel'Logg Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Kel'Logg
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/mathematics/">Mathematics</a>
            </li><li class="masthead__menu-item">
              <a href="/neuroscience/">Neuroscience</a>
            </li><li class="masthead__menu-item">
              <a href="/machine_learning/">Machine Learning</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/bio-photo.jpg" alt="D. K. Ryu" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">D. K. Ryu</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Ph.D. candidate in reinforcement learning and natural language processing</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="www.linkedin.com/in/dkryu" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
          
            <li><a href="https://github.com/ktr0921" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="mailto:dongwon.ryu@monash.edu" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Value-based Reinforcement Learning">
    <meta itemprop="description" content="Value-based reinforcement learning is one of two fundamental classes of reinforcement learning algorithms. It implicitly constructs the policy based on estimated value functions, so its objective is to correctly estimate value functions. This post aims to provide a tutorial on value-based reinforcement learning algorithms.">
    <meta itemprop="datePublished" content="2022-09-15T00:00:00+09:00">
    <meta itemprop="dateModified" content="2023-04-15T00:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Value-based Reinforcement Learning
</h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          28 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Table of Contents</h4></header>
              <ul class="toc__menu"><li><a href="#policy-evaluation">Policy Evaluation</a><ul><li><a href="#comparison">Comparison</a></li><li><a href="#bias-and-variance">Bias and Variance</a></li><li><a href="#n-step-bootstrapping">\(n\)-Step Bootstrapping</a></li></ul></li><li><a href="#dynamic-programming">Dynamic Programming</a><ul><li><a href="#policy-iteration">Policy Iteration</a></li><li><a href="#example">Example</a></li><li><a href="#value-iteration">Value Iteration</a></li></ul></li><li><a href="#temporal-difference-algorithm">Temporal Difference Algorithm</a><ul><li><a href="#sarsa">SARSA</a></li><li><a href="#q-learning">Q Learning</a></li></ul></li><li><a href="#summary">Summary</a><ul><li><a href="#reference">Reference</a></li></ul></li></ul>

            </nav>
          </aside>
        
        
<div class="notice--primary">
  
<p><strong>Prerequisites</strong></p>

<ul>
  <li><a href="/posts/mathematics/markov_model_1_markov_chain">Markov Chain</a></li>
  <li><a href="/posts/mathematics/markov_model_2_markov_decision_process">Markov Decision Process</a></li>
</ul>

</div>

<blockquote>
    Machine intelligence is the last invention that humanity will ever need to make.
    <br />
    <cite>Nick Bostrom</cite>
</blockquote>

<p>If the universe is constructed with patterns and humans can interpret this, then so may machines. If machines become intelligent over humans, <em>humans need not apply</em>.</p>

<p><strong>Machine learning</strong> is the study of algorithms that involve training a computer to learn patterns and relationships in data without being explicitly programmed. It can be divided into three types:</p>
<ul>
  <li><strong>Supervised Learning</strong>: The algorithm, or model, is trained on labeled data, where the goal is to map each input to the correct output.</li>
  <li><strong>Unsupervised Learning</strong>: The algorithm, or model, is trained on unlabeled data, where the goal is to identify patterns or structure in the data without being told what to look for.</li>
  <li><strong>Reinforcement Learning</strong>: The algorithm, or agent, is trained on the interactive worlds, where the goal is to learn a policy that maximizes rewards and minimize penalties.</li>
</ul>

<p>The problems in ML can be largely divided by four types:</p>
<ul>
  <li><strong>Computer Vision</strong>: The problems require machines to interpret and understand visual data, i.e. images and videos. They includes image classification, object localization, video analysis, image captioning and visual question answering.</li>
  <li><strong>Natural Language Processing</strong>: The problems require machines to interpret and understand human language, i.e. words and documents. They includes text classification, translation, summarization, speech analysis, question answering and dialogue system.</li>
  <li><strong>Graph Analysis</strong>: The problems require machines to interpret and understand graph data, i.e. nodes and edges in graphs. They includes graph classification, link prediction, knowledge graph, social network analysis, recommendation system, combinatorial optimization and path finding.</li>
  <li><strong>Decision Making</strong>: The problems require machines to interact with the world and choose an appropriate action based on data and objectives. They include game playing, autonomous vehicles and robotics.</li>
</ul>

<p><strong>Reinforcement learning</strong> is a subfield of machine learning, popular in decision making problems. The algorithms are generally grouped by:</p>

<table class="table-centering">
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 100px"> </td>
        <td style="text-align: center; vertical-align: middle; width: 400px" colspan="2"> <b>Value-based</b> </td>
        <td style="text-align: center; vertical-align: middle; width: 400px" rowspan="2"> <b>Policy-based</b> </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 100px"> </td>
        <td style="text-align: center; vertical-align: middle; width: 200px"> <b>Model-based</b> </td>
        <td style="text-align: center; vertical-align: middle; width: 200px"> <b>Model-free</b> </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 100px">
            <b>Classic</b>
        </td>
        <td style="text-align: center; vertical-align: middle; width: 200px">
            <a href="/posts/machine_learning/decision_1_value_based_reinforcement_learning/#policy-iteration">Policy Iteration</a>,
            <br />
            <a href="/posts/machine_learning/decision_1_value_based_reinforcement_learning/#value-iteration">Value Iteration</a>
        </td>
        <td style="text-align: center; vertical-align: middle; width: 200px">
            <a href="/posts/machine_learning/decision_1_value_based_reinforcement_learning/#policy-evaluation">Monte Carlo</a>,
            <a href="/posts/machine_learning/decision_1_value_based_reinforcement_learning/#sarsa">SARSA</a>,
            <br />
            <a href="/posts/machine_learning/decision_1_value_based_reinforcement_learning/#q-learning">Q Learning</a>
        </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            <a href="/posts/machine_learning/decision_2_policy_based_reinforcement_learning/#policy-gradient">
                Policy Gradient
            </a>,
            <a href="/posts/machine_learning/decision_2_policy_based_reinforcement_learning/#deterministic-policy-gradient">
                Deterministic Policy Gradient
            </a>
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 100px">
            <b>Deep</b>
        </td>
        <td style="text-align: center; vertical-align: middle; width: 400px" colspan="2">
            Deep Q Network, Double Deep Q Network,
            <br />
            Duel Deep Q Network, Rainbow
        </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            Vanilla Policy Gradient, Natural Policy Gradient,
            <br />
            Trust Region Policy Optimization,
            <br />
            Proximal Policy Optimization
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 100px">
            <b>Classic</b>
        </td>
        <td style="text-align: center; vertical-align: middle; width: 1000px" colspan="4">
            <a href="/posts/machine_learning/decision_2_policy_based_reinforcement_learning/#actor-critic">Actor Critic</a>
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 100px">
            <b>Deep</b>
        </td>
        <td style="text-align: center; vertical-align: middle; width: 1000px" colspan="4">
            Deep Deterministic Policy Gradient, Twin Delayed Deep Deterministic Policy Gradient,
            <br />
            Soft Q Learning, Soft Actor Critic
        </td>
    </tr>
</table>

<p><strong>Value-based reinforcement learning (RL)</strong> is one of two fundamental classes of RL algorithms. It implicitly constructs the policy based on estimated value functions, so its objective is to correctly estimate value functions. The procedure is divided into two steps.</p>
<ul>
  <li><strong>Policy evaluation</strong>: A process of evaluating the policy by obtaining the Bellman equation.</li>
  <li><strong>Policy improvement</strong>: A process of obtaining the improved policy based on the Bellman equation.</li>
</ul>

<p>This post aims to provide a tutorial on value-based RL algorithms.</p>

<h1 id="policy-evaluation">Policy Evaluation</h1>

<p>The policy evaluation is a process of evaluating the policy by obtaining the Bellman equation. There are mainly three methods in the policy evaluation.</p>

<table class="table-centering">
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 300px"> Dynamic Programming </td>
        <td style="text-align: center; vertical-align: middle; width: 300px"> Temporal Difference </td>
        <td style="text-align: center; vertical-align: middle; width: 300px"> Monte Carlo </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 300px">
            <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/1_1_dp.png" style="width: 300px" />
        </td>
        <td style="text-align: center; vertical-align: middle; width: 300px">
            <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/1_1_td.png" style="width: 300px" />
        </td>
        <td style="text-align: center; vertical-align: middle; width: 300px">
            <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/1_1_mc.png" style="width: 300px" />
        </td>
    </tr>
    <figcaption class="figure-caption text-center">
        Methods for policy evaluation from
        <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf">
            David Silver's lecture
        </a>
    </figcaption>
</table>

<p>Where the update for the value functions is:</p>

\[\begin{align}
    \text{DP: } &amp; v_{\pi}^{(i+1)} ( s ) \leftarrow \sum_{a \in A} \pi (a \vert s)
    \left(
    \sum_{s' \in S} p (s' \vert s , a) \left( r' + \gamma \cdot v_{\pi}^{(i)} (s') \right)
    \right)
    \\
    \text{TD: } &amp; v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} )
    + \alpha \left( \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right) - v_{\pi}^{(i)} ( s_{t} ) \right)
    \\
    \text{MC: } &amp; v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} )
    + \alpha \left( \left( \sum_{k = 0}^{\infty} \gamma^{k} r_{t + k + 1} \right) - v_{\pi}^{(i)} ( s_{t} ) \right)
\end{align}\]

<p>Where \(s \equiv s_{t}\) and \(s' \equiv s_{t+1}\). The notation \(s\) and \(s'\) signify the state and the neighbouring state while \(s_{t}\) and \(s_{t+1}\) signify the current state and the next state for a given trajectory, \(\tau = ( S_{0} = s_{0} , A_{0} = a_{0} , \cdots  A_{T-1} = a_{T-1} , S_{T} = s_{T} )\).</p>

<p><strong>Dynamic programming (DP)</strong> is both a mathematical optimization method and a computer programming method. It is a general idea of expressing a complex problem into simpler sub-problems, but in this post, I will focus on reinforcement learning (RL), in particular solving a Markov decision process (MDP). Note that this method is developed by Richard Bellman, which the Bellman equation is named after. DP simply iterates through all the states and actions to recursively update the value functions with full access to the environment dynamics of the MDP.</p>

<p>In practice, DP is not preferred as most of the environments are either 1) unknown MDPs, i.e. how the opponent will play the game Go is unknown, or 2) known but intractably large MDPs, i.e. the game Go has \(\vert \mathcal{S} \vert \approx 10^{170}\) and \(\vert \mathcal{A} \vert \approx 10^{360}\). To handle this, we often use sampling methods for the policy evaluation, which are divided into <strong>temporal difference (TD)</strong> or <strong>Monte-Carlo (MC)</strong>. They do not require to access the environment dynamics, but instead, for a given \(\tau\), the states and actions are sampled for value function updates.</p>

<h2 id="comparison">Comparison</h2>

<p>Policy evaluation methods can be compared with respect to 1) sampling and 2) bootstrapping.</p>

<figure class="align-center" style="width: 450px">
    <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/1_2_dp_td_mc.png" />
    <figcaption class="figure-caption text-center">
        Comparison between methods from
        <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf">
            David Silver's lecture
        </a>
    </figcaption>
</figure>

<ul>
  <li>Full backups account for every possible action to compute the Bellman equations while sample backups account for only sampled actions.</li>
  <li>Deep backups account for the entire trajectory to compute the Bellman equations while shallow backups account for only a single time step ahead.</li>
</ul>

<p>While the term <em>full backups</em> and <em>sample backups</em> are used to describe the update method of the value functions, algorithms that utilize these techniques are referred differently:</p>

<ul>
  <li><strong>Model-based</strong>: The optimal policy is obtained using the environment dynamics, such as \(p ( s' \vert s , a )\).
    <ul>
      <li>DP is known as model-based while the heuristic search can be model-based or model-free.</li>
      <li>Model-based algorithms include those models that do not access the environment dynamics, but learn and predict the environment dynamics. Thus, generally, any model-free algorithm with additional world predicting algorithms that help the update is considered model-based.</li>
    </ul>
  </li>
  <li><strong>Model-free</strong>: The optimal policy is obtained using sampled trajectories \(\tau\).
    <ul>
      <li>Both TD and MC are considered model-free since they do not access nor predict environment dynamics.</li>
      <li>Majority of algorithms used in RL are model-free, but as mentioned, if additional world predicting algorithms are employed to help the update, they are classified as model-based.</li>
    </ul>
  </li>
</ul>

<p>The standard RL usually refers to model-free RL. TD and MC can further be generalized as:</p>

\[\begin{equation}
    v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} ) + \alpha \delta
    \quad
    \delta = \hat{v}_{\pi}^{(i)} ( s_{t} ) - v_{\pi}^{(i)} ( s_{t} )
    \\
    \hat{v}_{\pi}^{(i)} ( s_{t} ) =
    \begin{cases}
        r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) &amp; \text{for TD} \\
        R_{t} = \sum_{k = 0}^{\infty} \gamma^{k} r_{t+k+1} &amp; \text{for MC} \\
    \end{cases}
\end{equation}\]

<p>Where:</p>

<ul>
  <li>\(\hat{v}_{\pi}^{(i)}\) is the estimated target value and \(v_{\pi}^{(i)}\) is the value.</li>
  <li>\(\delta\) is the difference between \(\hat{v}_{\pi}^{(i)}\) and \(v_{\pi}^{(i)}\), where \(\delta \rightarrow 0\) as \(i \rightarrow \infty\).</li>
  <li>Higher \(\alpha\) results in faster convergence, but may induce oscillations during learning.</li>
  <li>Lower \(\alpha\) results in slow convergence that may potentially result in falling into the local optima.</li>
  <li>The difference between TD and MC is how \(\hat{v}_{\pi}^{(i)}\) is constructed.</li>
</ul>

<p>The idea of TD and MC is sampling techniques in the policy evaluation, where the update is done using trajectories that the agent travelled with the initial state distribution, \(p_{0}\), instead of accessing the state transition probabilities. While the policy evaluation differs, the policy improvement in TD and MC follows the greedy policy, the same as DP.</p>

<h2 id="bias-and-variance">Bias and Variance</h2>

<p>Model-free algorithms suffer from bias and variance problems since they use a sampling technique to update the value functions.</p>

<ol>
  <li>
    <p>MC has no bias: Since \(v_{\pi} ( s_{t} ) = \mathbb{E}_{ \tau \sim \pi } \left[ R_{t} \vert s_{t} \right]\):</p>

\[\begin{equation}
     \mathbb{E}_{ \tau \sim \pi } \left[ \hat{v}_{\pi}^{(i)} ( s_{t} ) \right]
     = \mathbb{E}_{ \tau \sim \pi } \left[ R_{t} \vert s_{t} \right] \equiv v_{\pi} ( s_{t} )
 \end{equation}\]
  </li>
  <li>
    <p>TD has bias: Since \(\hat{v}_{\pi}^{(i)} ( s_{t} ) = r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} )\), \(\hat{v}_{\pi}^{(i)} ( s_{t} )\) follows \(v_{\pi}^{(i)} ( s_{t+1} )\), where \(v_{\pi}^{(i)} ( s_{t+1} )\) is an incorrect estimate. Thus, the initialization of the value functions causes bias in TD.</p>

\[\require{color}
 \begin{align}
     v_{\pi}^{(i+1)} ( s_{t} )
     &amp; \leftarrow v_{\pi}^{(i)} ( s_{t} ) + \alpha \left( \hat{v}_{\pi}^{(i)} ( s_{t} ) - v_{\pi}^{(i)} ( s_{t} ) \right)
     \quad
     \hat{v}_{\pi}^{(i)} ( s_{t} ) = r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} )
     \\
     &amp; = \textcolor{red}{ v_{\pi}^{(i)} ( s_{t} ) + \alpha \left( \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right) - v_{\pi}^{(i)} ( s_{t} ) \right) }
     \\
     &amp; = \textcolor{blue}{ ( 1 - \alpha ) \cdot v_{\pi}^{(i)} ( s_{t} )
     + \alpha \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right) }
     \\
     &amp; = ( 1 - \alpha ) \cdot \left(
     \textcolor{red}{ v_{\pi}^{(i-1)} ( s_{t} ) + \alpha \left( \left( r_{t+1} + \gamma v_{\pi}^{(i-1)} ( s_{t+1} ) \right) - v_{\pi}^{(i-1)} ( s_{t} ) \right) }
     \right)
     + \alpha \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right)
     \\
     &amp; = ( 1 - \alpha ) \cdot \left(
     \textcolor{blue}{ ( 1 - \alpha ) \cdot v_{\pi}^{(i-1)} ( s_{t} )
     + \alpha \left( r_{t+1} + \gamma v_{\pi}^{(i-1)} ( s_{t+1} ) \right) }
     \right)
     + \alpha \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right)
     \\
     &amp; = ( 1 - \alpha )^{2} \cdot v_{\pi}^{(i-1)} ( s_{t} )
     + \alpha ( 1 - \alpha )^{1} \left( r_{t+1} + \gamma v_{\pi}^{(i-1)} ( s_{t+1} ) \right)
     + \alpha ( 1 - \alpha )^{0} \left( r_{t+1} + \gamma v_{\pi}^{(i-0)} ( s_{t+1} ) \right)
     \\
     &amp; \; \vdots
     \\
     &amp; = ( 1 - \alpha )^{i} \cdot v_{\pi}^{(0)} ( s_{t} )
     + \sum_{j = 0}^{i} \alpha ( 1 - \alpha )^{j} \left( r_{t+1} + \gamma v_{\pi}^{(i-j)} ( s_{t+1} ) \right)
 \end{align}\]

    <ul>
      <li>If \(i \rightarrow \infty\), then \(( 1 - \alpha )^{i} \cdot v_{\pi}^{(0)} ( s_{t} ) \rightarrow 0\). However, the second term still includes the value functions of the early time steps, i.e. \(\alpha ( 1 - \alpha )^{i-1} \left( r_{t+1} + \gamma v_{\pi}^{(1)} ( s_{t+1} ) \right)\). Thus, the bias decreases over time, but will not be removed. This is based on
 <a href="https://stats.stackexchange.com/questions/454856/why-is-temporal-difference-learning-biased-in-reinforcement-learning">Why is temporal difference learning biased in reinforcement learning?</a>.</li>
    </ul>
  </li>
  <li>
    <p>MC has high variance: \(R_{t}\) at small \(t\) change drastically if \(a_{t}\) at small \(t\) change. Intuitively, in complex environments, there are infinitely many trajectories, but most of them are useless, so \(G_{0}\) for most trajectories will be zero or very small while there are some trajectories that produce relatively high \(G_{0}\).</p>

\[\require{color}
 \begin{align}
     \text{Var} \left[ R_{t} \right]
     &amp; = \text{Var} \left[ \sum_{k = 0}^{\infty} \gamma^{k} r_{t+k+1} \right]
     \\
     &amp; = \sum_{k = 0}^{\infty} (\gamma^{k})^{2} \text{Var} \left[ r_{t+k+1} \right]
     + \sum_{i \neq j} \gamma^{i} \cdot \gamma^{j} \cdot \text{Cov} \left[ r_{t+i+1}, r_{t+j+1} \right]
     \quad
     \text{Cov} \left[ r_{t+i+1}, r_{t+j+1} \right] = 0
     \\
     &amp; = \sum_{k = 0}^{\infty} (\gamma^{k})^{2} \text{Var} \left[ r_{t+k+1} \right]
     + \sum_{i \neq j} \gamma^{i} \cdot \gamma^{j} \cdot 0
     \\
     &amp; = \sum_{k = 0}^{\infty} (\gamma^{k})^{2} \text{Var} \left[ r_{t+k+1} \right]
     \\
     &amp; &lt; \sum_{k = 0}^{\infty} \gamma^{k} \text{Var} \left[ r_{t+k+1} \right]
     \quad
     \text{where, } \gamma \in (0, 1)
 \end{align}\]

    <ul>
      <li>Note that \(\text{Cov} \left[ r_{t+i+1}, r_{t+j+1} \right] = 0\) is not really true since rewards in different time steps are correlated, but this is to provide a simple intuitive derivation. This is based on
 <a href="https://ai.stackexchange.com/questions/17810/how-does-monte-carlo-have-high-variance">How does Monte Carlo have high variance?</a>.</li>
    </ul>
  </li>
  <li>
    <p>TD has low variance: This term â€˜lowâ€™ is relative to MC. When computing \(\hat{v}_{\pi}^{(i)} ( s_{t} ) = r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} )\), we do not account variance for rewards in the later time steps, and thus, the variance is \(\text{Var} \left[ r_{t+1} \right]\), which is smaller than \(\sum_{k = 0}^{\infty} \gamma^{k} \text{Var} \left[ r_{t+k+1} \right]\).</p>
  </li>
</ol>

<p>Thus, TD is known to have <code class="language-plaintext highlighter-rouge">high bias and low variance</code>, while MC is known to have <code class="language-plaintext highlighter-rouge">low bias and high variance</code>.</p>

<h2 id="n-step-bootstrapping">\(n\)-Step Bootstrapping</h2>

<p>In RL, the <strong>bootstrapping</strong> is a process of computing \(\hat{v}_{\pi}^{(i)} ( s_{t} )\) based on \(v_{\pi}^{(i)} ( s_{t+1} )\). Instead of thoroughly relying on either TD or MC, we can use the \(n\)<strong>-step bootstrapping</strong> to balance bias and variance.</p>

<figure class="align-center" style="width: 400px">
    <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/1_3_td_mc.png" />
    <figcaption class="figure-caption text-center">
        Comparison between temporal difference and Monte Carlo methods from
        <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf">
            David Silver's lecture
        </a>
    </figcaption>
</figure>

<p>Where the update for the value functions is done according to:</p>

\[\require{color}
\begin{equation}
    \hat{v}_{\pi}^{(i)} ( s_{t} ) = R_{t}^{( \textcolor{red}{n} )} = \sum_{\textcolor{blue}{k} = 0}^{\textcolor{red}{n} - 1} \gamma^{\textcolor{blue}{k}} r_{t + \textcolor{blue}{k} + 1} + \gamma^{ \textcolor{red}{n} } v_{\pi}^{(i)} ( s_{t + \textcolor{red}{n}} )
\end{equation}\]

<p>The \(n\)-step bootstrapping with \(n = 1\) is the standard TD approach and \(n = \infty\) is equivalent to MC. Due to how it can balance bias and variance, the \(n\)-step bootstrapping with reasonable \(n\) is more preferred than MC.</p>

<h1 id="dynamic-programming">Dynamic Programming</h1>

<p>Since DP can provide nice intuition, I will go through some common algorithms in DP. There are two classes of value-based RL algorithms.</p>
<ul>
  <li><strong>Policy iteration</strong>: The policy evaluation with the Bellman expectation equation and the policy improvement.</li>
  <li><strong>Value iteration</strong>: The policy evaluation with the Bellman optimality equation, which encapsulates the policy improvement with the greedy policy.</li>
</ul>

<h2 id="policy-iteration">Policy Iteration</h2>

<p>The <strong>policy iteration</strong> is to find the optimal policy with the Bellman expectation equation. There exist two policy iteration algorithms based on the type of value functions used in the policy evaluation.</p>
<ul>
  <li><strong>Generalized policy iteration</strong>, or <strong>V-policy iteration</strong> or iterative policy evaluation: The state value is used as a value function.</li>
</ul>

\[\begin{equation}
    v_{\pi}^{(i+1)} ( s )
    \leftarrow \sum_{a \in A} \pi (a \vert s)
    \left( \sum_{s' \in S} p (s' \vert s , a) \left( r' + \gamma v_{\pi}^{(i)} (s') \right) \right)
\end{equation}\]

<ul>
  <li><strong>Q-policy iteration</strong>: The Q value is used as a value function.</li>
</ul>

\[\begin{equation}
    q_{\pi}^{(i+1)} ( s , a )
    \leftarrow \sum_{s' \in S} p (s' \vert s , a)
    \left( r' + \gamma \sum_{a' \in A} \pi (a' \vert s') q_{\pi}^{(i)} (s', a') \right)
\end{equation}\]

<p>While the policy evaluation differs, the policy improvement usually uses the greedy policy.</p>

\[\begin{equation}
    \pi_{\text{greedy}} ( a \vert s )
    = \begin{cases}
      1 &amp; \arg \max_{a} q_{\pi} ( s , a ) \\
      0 &amp; \text{otherwise}
    \end{cases}
\end{equation}\]

<p>Where \(\pi_{\text{greedy}}\) is the most common example of the deterministic policy. Note that there are other ways to construct the policy in the policy improvement, i.e. the softmax policy, \(\pi_{\text{softmax}} ( a \vert s ) = \frac{ e^{ q_{\pi} ( s , a ) } }{ \sum_{a' \in \mathcal{A}} e^{ q_{\pi} ( s , a' ) } }\), for the stochastic policy.</p>

<p>The procedure of the generalized policy iteration follows:</p>

<ol>
  <li>Policy evaluation: Obtain \(v_{\pi}^{(k)} ( s )\) for a given \(\pi^{(i)} ( a \vert s )\), where \(k\) is any reasonable number. Then, obtain \(q_{\pi}^{(i)} (s, a) = \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v_{\pi}^{(k)} (s') \right)\) for the policy improvement.</li>
  <li>Policy improvement: Set \(\pi^{(i+1)} ( a \vert s )\) as the greedy policy for a given \(q_{\pi}^{(i)} (s, a)\).</li>
  <li>Policy iteration: Repeat the policy evaluation and the policy improvement to obtain \(\pi^{(\infty)} ( a \vert s ) \equiv \pi^{\ast} ( a \vert s )\).</li>
</ol>

<figure class="align-center" style="width: 600px">
    <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_1_pi.png" />
    <figcaption class="figure-caption text-center">
        Generalized policy iteration diagram from
        <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf">
            David Silver's lecture
        </a>
    </figcaption>
</figure>

<p>Thus, this process is:</p>

\[\begin{equation}
    \pi^{(0)} ( a \vert s )
    \rightarrow v_{\pi}^{(0 \rightarrow k)} ( s )
    \rightarrow q_{\pi}^{(1)} (s, a)
    \rightarrow \pi^{(1)} ( a \vert s ) \rightarrow \cdots
    \rightarrow \pi^{(\infty)} ( a \vert s )
\end{equation}\]

<p>The generalized policy iteration requires computing the Q value additionally to obtain the policy.</p>

<h2 id="example">Example</h2>

<p>Consider a 4-by-4 deterministic gridworld problem with action set, \(\mathcal{A} = \{ N, S, E, W \}\).</p>

<figure class="align-center" style="width: 450px">
    <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_2_gw.png" />
    <figcaption class="figure-caption text-center">
        Gridworld from
        <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf">
            David Silver's lecture
        </a>
    </figcaption>
</figure>

<p>The grey regions are the terminal states and a reward is held in every state transition, \(r_{t+1} = r ( s_{t} , a_{t} , s_{t+1} ) = -1\). Let the initial policy be a uniform distribution over action, \(\pi^{(0)} ( a \vert s ) = \mathcal{U} ( \mathcal{A} )\), then, the value functions are computed by applying the Bellman expectation equation repetitively.</p>

<table class="table-centering">
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 150px"> $$ \pi^{(0)} ( a \vert s ) $$ </td>
        <td style="text-align: center; vertical-align: middle; width: 150px"> $$ v_{\pi}^{(0)} ( s ) $$ </td>
        <td style="text-align: center; vertical-align: middle; width: 150px"> $$ v_{\pi}^{(1)} ( s ) $$ </td>
        <td style="text-align: center; vertical-align: middle; width: 150px"> $$ v_{\pi}^{(2)} ( s ) $$ </td>
        <td style="text-align: center; vertical-align: middle; width: 150px"> $$ v_{\pi}^{(\infty)} ( s ) $$ </td>
        <td style="text-align: center; vertical-align: middle; width: 150px"> $$ \pi^{(1)} ( a \vert s ) $$ </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 150px">
            <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_pi0.png" style="width: 150px" />
        </td>
        <td style="text-align: center; vertical-align: middle; width: 150px">
            <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_v0.png" style="width: 150px" />
        </td>
        <td style="text-align: center; vertical-align: middle; width: 150px">
            <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_v1.png" style="width: 150px" />
        </td>
        <td style="text-align: center; vertical-align: middle; width: 150px">
            <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_v2.png" style="width: 150px" />
        </td>
        <td style="text-align: center; vertical-align: middle; width: 150px">
            <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_v.png" style="width: 150px" />
        </td>
        <td style="text-align: center; vertical-align: middle; width: 150px">
            <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/2_3_gw_pi1.png" style="width: 150px" />
        </td>
    </tr>
    <figcaption class="figure-caption text-center">
        Value and policy in gridworld with dynamic programming from
        <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/DP.pdf">
            David Silver's lecture
        </a>
    </figcaption>
</table>

<ol>
  <li>Policy evaluation: For a given \(\pi^{(0)} ( a \vert s )\), iterate value functions until it converges, \(v_{\pi}^{(0)} ( s ) \rightarrow v_{\pi}^{(\infty)} ( s )\),
    <ul>
      <li>The grey regions have \(v_{\pi}^{(\infty)} ( s ) = 0\) since it is the terminal state, no action is available.</li>
    </ul>
  </li>
  <li>Policy improvement: Update the policy with actions from low to high value functions, \(\pi^{(0)} ( a \vert s ) \rightarrow \pi^{(1)} ( a \vert s )\). For instance, define \(\pi ( a \vert s ) = \begin{bmatrix} p_{N} &amp; p_{S} &amp; p_{E} &amp; p_{W} \end{bmatrix}^{\intercal}\):
 \(\begin{equation}
     \pi^{(0)} ( a \vert s = 1 )
     = \begin{bmatrix} \frac{1}{4} &amp; \frac{1}{4} &amp; \frac{1}{4} &amp; \frac{1}{4} \end{bmatrix}^{\intercal}
     \quad \rightarrow \quad
     \pi^{(1)} ( a \vert s = 1 )
     = \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}^{\intercal}
     \\
     \pi^{(0)} ( a \vert s = 2 )
     = \begin{bmatrix} \frac{1}{4} &amp; \frac{1}{4} &amp; \frac{1}{4} &amp; \frac{1}{4} \end{bmatrix}^{\intercal}
     \quad \rightarrow \quad
     \pi^{(1)} ( a \vert s = 2 )
     = \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 1 \end{bmatrix}^{\intercal}
     \\
     \pi^{(0)} ( a \vert s = 3 )
     = \begin{bmatrix} \frac{1}{4} &amp; \frac{1}{4} &amp; \frac{1}{4} &amp; \frac{1}{4} \end{bmatrix}^{\intercal}
     \quad \rightarrow \quad
     \pi^{(1)} ( a \vert s = 3 )
     = \begin{bmatrix} 0 &amp; \frac{1}{2} &amp; 0 &amp; \frac{1}{2} \end{bmatrix}^{\intercal}
 \end{equation}\)</li>
</ol>

<p>In this example, \(k \rightarrow \infty\). In theory, under \(\pi^{(0)} ( a \vert s ) = \mathcal{U} ( \mathcal{A} )\), a single policy improvement with \(v_{\pi}^{(\infty)} ( s )\) is required for the optimal policy, \(\pi^{(1)} ( a \vert s ) \equiv \pi^{(\ast)} ( a \vert s )\). However, in practice, <em>iterating through the policy evaluation and the policy improvement is more efficient</em>. Thus, the generalized policy iteration usually switches between the policy evaluation and the policy improvement with a reasonable \(k\).</p>

<h2 id="value-iteration">Value Iteration</h2>

<p>The policy iteration algorithms iterate both value function and policy. The <strong>value iteration</strong> algorithms iterate only the value functions under the idea that the greedy policy, a.k.a. the Bellman optimality equation. Similar to the policy iteration, there exist two value iteration algorithms.</p>

<ul>
  <li><strong>V-value iteration</strong>: The state value is used as a value function.</li>
</ul>

\[\begin{equation}
    v_{\pi}^{(i+1)} ( s )
    \leftarrow \max_{a}
    \left( \sum_{s' \in S} p (s' \vert s , a) \left( r' + \gamma v_{\pi}^{(i)} (s') \right) \right)
\end{equation}\]

<ul>
  <li><strong>Q-value iteration</strong>: The Q value is used as a value function.</li>
</ul>

\[\begin{equation}
    q_{\pi}^{(i+1)} ( s , a )
    \leftarrow \sum_{s' \in S} p (s' \vert s , a)
    \left( r' + \gamma \max_{a'} q_{\pi}^{(i)} (s', a') \right)
\end{equation}\]

<p>The only difference between the policy iteration is that the value functions are computed following the greedy policy.</p>

<p>The value iteration can be seen as performing the policy improvement for every single step of the policy evaluation, so both policy iteration and value iteration yield the same optimal policy. Due to this, the value iteration is usually considered more efficient.</p>

<p>The process of the value iteration becomes much simpler than the policy iteration.</p>

\[\begin{equation}
    \pi^{(0)} ( a \vert s )
    \rightarrow v_{\pi}^{(0 \rightarrow k)} ( s )
    \rightarrow q_{\pi}^{(1)} (s, a)
    \rightarrow \pi^{(\infty)} ( a \vert s )
\end{equation}\]

<h1 id="temporal-difference-algorithm">Temporal Difference Algorithm</h1>

<p>TD algorithms are the most popular method to solve MDP in the real world scenario.</p>

<ul>
  <li>Since they sample trajectories, they are robust to complex unknown environments.</li>
  <li>Due to their low variance and simplicity, they converge quickly.</li>
  <li>The bias and variance can be balanced with \(n\)-step bootstrapping.</li>
</ul>

<p>The <strong>temporal difference (TD) learning</strong> is a TD version of the generalized policy iteration that follows the basic form:</p>

\[\begin{equation}
    v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} ) + \alpha \left( \hat{v}_{\pi}^{(i)} ( s_{t} ) - v_{\pi}^{(i)} ( s_{t} ) \right)
    \\
    \hat{v}_{\pi}^{(i)} ( s_{t} ) = r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} )
\end{equation}\]

<p>It often requires more iterations than the generalized policy iteration as they sample the trajectories for value function update.</p>

<h2 id="sarsa">SARSA</h2>

<p>Similar to the Q-policy iteration, instead of using state value in the TD learning, we can directly evaluate the policy using the Q value and apply the policy improvement. This TD version of Q-policy iteration is referred to as the <strong>state-action-reward-state-action (SARSA)</strong>, originated from how it samples a data point \(( s, a, r, s', a' )\).</p>

\[\begin{equation}
    q_{\pi}^{(i+1)} ( s_{t} , a_{t} ) \leftarrow q_{\pi}^{(i)} ( s_{t} , a_{t} ) + \alpha \left( \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) - q_{\pi}^{(i)} ( s_{t} , a_{t} ) \right)
    \\
    \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) = r_{t+1} + \gamma q_{\pi}^{(i)} ( s_{t+1} , a_{t+1} )
\end{equation}\]

<p>Since the policy improvement follows the greedy policy, the agent only selects actions with the highest Q value. Due to this, both TD learning and SARSA suffer from poor exploration. To remedy this, we introduce:</p>

<ul>
  <li><strong>On-policy</strong>: How the agent explores is the same as what it learns, or \(a \sim \pi ( a \vert s )\) where \(\pi ( a \vert s ) = \arg \max_{a} q_{\pi} ( s , a )\).</li>
  <li><strong>Off-policy</strong>: How the agent explores is different from what it learns, or \(a \sim \beta ( a \vert s )\) and \(\pi ( a \vert s ) = \arg \max_{a} q_{\pi} ( s , a )\), where \(\beta ( a \vert s )\) is referred to as the <strong>behaviour policy</strong> and \(\pi ( a \vert s )\) is referred to as the <strong>target policy</strong> such that \(\beta ( a \vert s ) \neq \pi ( a \vert s )\).</li>
</ul>

<p>\(\beta\) is used to act on the environment and collect trajectories while \(\pi\) is the policy updated with the collected trajectories. In general, \(\beta\) uses \(\pi\) partially, either with probability or smoothing. The most common off-policy algorithm is \(\epsilon\)<strong>-greedy exploration</strong>:</p>

\[\begin{equation}
    \pi ( a \vert s )
    = \begin{cases}
      1 &amp; \arg \max_{a} q_{\pi} ( s , a ) \\
      0 &amp; \text{otherwise}
    \end{cases}
    \quad
    \beta ( a \vert s )
    =
    \begin{cases}
        1 - \epsilon + \frac{ \epsilon }{ \vert \mathcal{A} \vert } &amp; \arg \max_{a} q_{\pi} ( s , a ) \\
        \frac{ \epsilon }{ \vert \mathcal{A} \vert } &amp; \text{otherwise}
    \end{cases}
\end{equation}\]

<!-- - Note that in order for off-policy to work, an additional condition is that $$ \beta ( a \vert s ) $$ must not deviate much from $$ \pi ( a \vert s ) $$. If $$ \beta ( a \vert s ) $$ is very different from $$ \pi ( a \vert s ) $$, i.e. $$ \beta ( a \vert s ) = \arg \min_{a} q_{\pi} ( s , a ) $$, off-policy would not work. -->

<p>\(\beta\) selects any action with \(\frac{ \epsilon }{ \vert \mathcal{A} \vert }\), resulting in diverse action selection, and thus, better exploration.</p>

<p>However, the off-policy method introduces the <strong>distribution shift</strong>, where the distribution we are following, \(\beta\), is not the same as the distribution we are updating, \(\pi\). To diminish this effect, we use the <strong>importance sampling (IS)</strong>, or importance sampling correction.</p>

\[\require{color}
\begin{align}
    \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} )
    &amp; = \textcolor{blue}{\prod_{l=0}^{\infty}}
    \frac{ \pi ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    { \beta ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    \left( \textcolor{green}{\sum_{k=0}^{\infty}} \gamma^{\textcolor{green}{k}} r_{t+\textcolor{green}{k}+1}^{[\beta]} \right)
    = \textcolor{blue}{\prod_{l=0}^{\infty}}
    \frac{ \pi ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    { \beta ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    R_{t}^{[\beta]}
    = R_{t}^{[\pi]}
    \\
    &amp; \equiv \textcolor{blue}{\prod_{l=0}^{\textcolor{red}{n}-1}}
    \frac{ \pi ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    { \beta ( a_{t+\textcolor{blue}{l}} \vert s_{t+\textcolor{blue}{l}} ) }
    \left( \textcolor{green}{\sum_{k=0}^{\textcolor{red}{n}-1}} \gamma^{\textcolor{green}{k}} r_{t+\textcolor{green}{k}+1}^{[\beta]}
    + \gamma^{\textcolor{red}{n}} q_{\pi}^{(i)} ( s_{t+\textcolor{red}{n}} , a_{t+\textcolor{red}{n}} ) \right)
    \\
    &amp; \; \vdots
    \\
    &amp; \equiv
    \frac{ \pi ( a_{t} \vert s_{t} ) }{ \beta ( a_{t} \vert s_{t} ) }
    \frac{ \pi ( a_{t+1} \vert s_{t+1} ) }{ \beta ( a_{t+1} \vert s_{t+1} ) }
    \left( r_{t+1}^{[\beta]} + \gamma r_{t+2}^{[\beta]} + \gamma^{2} q_{\pi}^{(i)} ( s_{t+2} , a_{t+2} ) \right)
    \\
    &amp; \equiv
    \frac{ \pi ( a_{t} \vert s_{t} ) }{ \beta ( a_{t} \vert s_{t} ) }
    \left( r_{t+1}^{[\beta]} + \gamma q_{\pi}^{(i)} ( s_{t+1} , a_{t+1} ) \right)
\end{align}\]

<p>Where \(\prod_{l=0}^{\infty} \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }{ \beta ( a_{t+l} \vert s_{t+l} ) }\) is the IS and \(r^{[\beta]}\) signifies the reward acquired by \(a \sim \beta ( a \vert s )\). Applying the IS transforms \(r^{[\beta]} \rightarrow r^{[\pi]}\), so that they can be used to update \(\pi ( a \vert s )\). In other words, \(\tau\) sampled from \(\beta\) can be transformed into \(\tau\) sampled from \(\pi\) with the IS, and thus, updating \(q_{\pi}^{(i)}\) becomes valid. Therefore:</p>
<ul>
  <li>The SARSA follows \(\hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) \equiv \frac{ \pi ( a_{t} \vert s_{t} ) }{ \beta ( a_{t} \vert s_{t} ) } \left( r_{t+1}^{[\beta]} + \gamma q_{\beta}^{(i)} ( s_{t+1} , a_{t+1} ) \right)\).</li>
  <li>The \(n\)-step TD method follows \(\hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) \equiv \prod_{l=0}^{n-1} \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }{ \beta ( a_{t+l} \vert s_{t+l} ) } \left( \sum_{k=0}^{n-1} \gamma^{k} r_{t+k+1}^{[\beta]} + \gamma^{n} q_{\pi}^{(i)} ( s_{t+n} , a_{t+n} ) \right)\).</li>
  <li>The MC method follows \(\hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) \equiv \prod_{l=0}^{\infty} \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }{ \beta ( a_{t+l} \vert s_{t+l} ) } R_{t}^{[\beta]}\).</li>
</ul>

<p>The idea of IS can be used for re-using previously acquired experiences from old policies (experience replay) or learning from experiences collected by different agents (offline learning).</p>

<h2 id="q-learning">Q Learning</h2>

<p>The <strong>Q learning (QL)</strong> is a TD version of the Q-value iteration. It is the most popular value-based RL algorithm as it naturally uses the off-policy mechanism to update the policy, but without the IS. The QL updates the Q value with respect to the maximum Q value at the next time step. In other words, the greedy policy is used as the target policy.</p>

\[\begin{equation}
    q_{\pi}^{(i+1)} ( s_{t} , a_{t} ) \leftarrow q_{\pi}^{(i)} ( s_{t} , a_{t} ) + \alpha \left( \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} ) - q_{\pi}^{(i)} ( s_{t} , a_{t} ) \right)
    \\
    \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} )
    = r_{t+1}^{(\beta)} + \gamma \max_{a'} q^{(i)} ( s_{t+1} , a' )
    \equiv r_{t+1}^{(\beta)} + \gamma q_{\pi}^{(i)} ( s_{t+1} , a_{t+1} )
\end{equation}\]

<ul>
  <li>\(\max_{a'} q^{(i)} ( s_{t+1} , a' )\) means that \(a'\) has been sampled in a way that obtains the maximum Q value at \(s_{t+1}\), which is <em>equivalent to using the greedy target policy</em>, so \(\max_{a'} q^{(i)} ( s_{t+1} , a' ) \equiv q_{\pi}^{(i)} ( s_{t+1} , a_{t+1} )\).</li>
  <li>This also means that the QL requires to compute \(q^{(i)} ( s_{t+1} , a' )\) for every \(a' \in \mathcal{A}\) during the update while the TD learning and SARSA do not.</li>
  <li>The IS can be ignored in the QL because:
    <ul>
      <li>\(\max_{a'} q^{(i)} ( s_{t+1} , a' )\) is equivalent to \(\gamma q_{\pi}^{(i)} ( s_{t+1} , a_{t+1} )\) with the greedy policy.</li>
      <li>\(r_{t+1}^{(\beta)}\) is used to update the Q value of the action that acquired the reward.</li>
      <li>Some discussions on stack exchange are <a href="https://stats.stackexchange.com/questions/335396/why-dont-we-use-importance-sampling-for-one-step-q-learning">Why donâ€™t we use importance sampling for one step Q-learning?</a> and <a href="https://ai.stackexchange.com/questions/21859/why-we-dont-use-importance-sampling-in-tabular-q-learning">Why we donâ€™t use importance sampling in tabular Q-Learning?</a>.</li>
    </ul>
  </li>
  <li>The QL follows \(1\)-step bootstrapping, so in theory, if the \(n\)-step is used, the QL also needs the IS. <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Sutton and Barto</a> introduced the \(n\)-step tree backup algorithm for the \(n\)-step QL, but it seems like small \(n\) without the IS still works in practice, i.e. 3-step in <a href="https://arxiv.org/abs/1710.02298?context=cs">RAINBOW</a>.</li>
</ul>

<p>The QL pseudocode is:</p>

<figure class="align-center" style="width: 600px">
    <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/3_1_ql_pseudocode.png" />
    <figcaption class="figure-caption text-center">
        QL pseudocode from
        <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">
            Reinforcement Learning: An Introduction
        </a>
    </figcaption>
</figure>

<p>The QL obtains the optimal value directly, so it is more efficient than the SARSA. However, this introduces additional maximization bias. Since \(\max\) is convex, under the Jensenâ€™s inequality:</p>

\[\begin{align}
    \mathbb{E}_{ \tau \sim \pi } \left[ \max_{a'} q^{(i)} ( s_{t+1} , a' ) \right]
    \geq
    \max_{a'} \mathbb{E}_{ \tau \sim \pi } \left[ q^{(i)} ( s_{t+1} , a' ) \right]
\end{align}\]

<p>Thus, using the bias definition:</p>

\[\begin{align}
    b ( \max_{a'} q^{(i)} ( s_{t+1} , a' ) )
    &amp; = \mathbb{E}_{ \tau \sim \pi } \left[ \max_{a'} q^{(i)} ( s_{t+1} , a' ) \right] - \max_{a'} q^{(i)} ( s_{t+1} , a' )
    \\
    &amp; = \mathbb{E}_{ \tau \sim \pi } \left[ \max_{a'} q^{(i)} ( s_{t+1} , a' ) \right] - \max_{a'} \mathbb{E}_{ \tau \sim \pi } \left[ q^{(i)} ( s_{t+1} , a' ) \right]
    \\
    &amp; \geq 0
\end{align}\]

<p>This can be easily viewed in the cliff walking problem:</p>

<figure class="align-center" style="width: 800px">
    <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/3_2_cliff_walking.png" />
    <figcaption class="figure-caption text-center">
        Cliff walking problem with SARSA and QL from
        <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf">
            David Silver's lecture
        </a>
    </figcaption>
</figure>

<p>Where <code class="language-plaintext highlighter-rouge">The Cliff</code> is the terminal state with the reward of \(-100\), otherwise the reward of \(-1\). The QL always takes the shortest but relatively risky path while the SARSA takes the safe path.</p>

<h1 id="summary">Summary</h1>
<p>In this post, the value-based RL algorithms are covered. Their methods are divided into:</p>
<ul>
  <li>Policy evaluation: A process of evaluating the policy by obtaining the Bellman equation.
    <ul>
      <li>Dynamic Programming: Full &amp; shallow backups, known as model-based. It is not preferred in the real world scenario since it requires to access the environment dynamics.</li>
    </ul>

\[\begin{equation}
      v_{\pi}^{(i+1)} ( s ) \leftarrow \sum_{a \in A} \pi (a \vert s)
      \left(
      \sum_{s' \in S} p (s' \vert s , a) \left( r' + \gamma \cdot v_{\pi}^{(i)} (s') \right)
      \right)
  \end{equation}\]

    <ul>
      <li>Temporal Difference: Sample &amp; shallow backups, known as model-free. It is generally preferred due to its flexibility and simplicity. It is known to have high bias and low variance, but bias and variance can be balanced using n-step bootstrapping.</li>
    </ul>

\[\begin{equation}
      v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} )
      + \alpha \left( \left( r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) \right) - v_{\pi}^{(i)} ( s_{t} ) \right)
  \end{equation}\]

    <ul>
      <li>Monte Carlo: Sample &amp; deep backups, known as model-free. It is generally less preferred than the temporal difference due to its inflexibility and complexity. It is known to have low bias and high variance.</li>
    </ul>

\[\begin{equation}
      v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} )
      + \alpha \left( \left( \sum_{k = 0}^{\infty} \gamma^{k} r_{t + k + 1} \right) - v_{\pi}^{(i)} ( s_{t} ) \right)
  \end{equation}\]
  </li>
  <li>Policy improvement: A process of obtaining the improved policy from the Bellman equation.
    <ul>
      <li>Greedy policy: A type of deterministic policy.</li>
    </ul>

\[\begin{equation}
      \pi_{\text{greedy}} ( a \vert s )
      = \begin{cases}
        1 &amp; \arg \max_{a} q_{\pi} ( s , a ) \\
        0 &amp; \text{otherwise}
      \end{cases}
  \end{equation}\]

    <ul>
      <li>Softmax policy: A type of stochastic policy.</li>
    </ul>

\[\begin{equation}
      \pi_{\text{softmax}} ( a \vert s )
      = \frac{ e^{ q_{\pi} ( s , a ) } }
      { \sum_{a' \in \mathcal{A}} e^{ q_{\pi} ( s , a' ) } }
  \end{equation}\]
  </li>
</ul>

<p>Based on the policy evaluation, the algorithms are divided into:</p>
<ul>
  <li>Model-based: The optimal policy is obtained using the environment dynamics, such as \(p ( s' \vert s , a )\). DP is known as model-based.</li>
  <li>
    <p>Model-free: The optimal policy is obtained using sampled trajectories \(\tau\). TD and MC are known as model-free.</p>

    <ul>
      <li>The update of model-free algorithms can further be generalized as:</li>
    </ul>

\[\begin{equation}
      v_{\pi}^{(i+1)} ( s_{t} ) \leftarrow v_{\pi}^{(i)} ( s_{t} ) + \alpha \delta
      \quad
      \delta = \hat{v}_{\pi}^{(i)} ( s_{t} ) - v_{\pi}^{(i)} ( s_{t} )
      \\
      \hat{v}_{\pi}^{(i)} ( s_{t} ) =
      \begin{cases}
          r_{t+1} + \gamma v_{\pi}^{(i)} ( s_{t+1} ) &amp; \text{for TD} \\
          R_{t} = \sum_{k = 0}^{\infty} \gamma^{k} r_{t+k+1} &amp; \text{for MC} \\
      \end{cases}
  \end{equation}\]

    <ul>
      <li>Model-free algorithms are known to suffer from bias and variance problems.
        <ul>
          <li>TD is known to have high bias and low variance</li>
          <li>MC is known to have low bias and high variance.</li>
        </ul>
      </li>
      <li>To balance bias and variance, the \(n\)-step bootstrapping is often employed.</li>
    </ul>

\[\require{color}
  \begin{equation}
      \hat{v}_{\pi}^{(i)} ( s_{t} ) = R_{t}^{( n )} = \sum_{k = 0}^{n - 1} \gamma^{k} r_{t + k + 1} + \gamma^{ n } v_{\pi}^{(i)} ( s_{t + n} )
  \end{equation}\]
  </li>
</ul>

<p>The value-based RL algorithms are generally divided into:</p>
<ul>
  <li>Policy Iteration: The policy evaluation with the Bellman expectation equation and the policy improvement.</li>
  <li>Value Iteration: The policy evaluation with the Bellman optimality equation, which encapsulates the policy improvement with the greedy policy.</li>
</ul>

<p>DP and TD algorithms are:</p>

<figure class="align-center" style="width: 500px">
    <img src="/assets/machine_learning/decision_1_value_based_reinforcement_learning/4_1_dp_td.png" />
    <figcaption class="figure-caption text-center">
        A comparison between DP and TD from
        <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/control.pdf">
            David Silver's lecture
        </a>
    </figcaption>
</figure>

<p>Where the first and second algorithms are of the policy iteration and the last algorithm is of the value iteration.</p>

<p>In TD, there are two ways to sample \(\tau\).</p>
<ul>
  <li>On-policy: How the agent explores is the same as what it learns, or \(a \sim \pi ( a \vert s )\) where \(\pi ( a \vert s ) = \arg \max_{a} q_{\pi} ( s , a )\).</li>
  <li>Off-policy: How the agent explores is different from what it learns, or \(a \sim \beta ( a \vert s )\) and \(\pi ( a \vert s ) = \arg \max_{a} q_{\pi} ( s , a )\), where \(\beta ( a \vert s )\) is referred to as the behaviour policy and \(\pi ( a \vert s )\) is referred to as the target policy such that \(\beta ( a \vert s ) \neq \pi ( a \vert s )\).</li>
</ul>

<p>\(\beta\) is used to act on the environment and collect trajectories while \(\pi\) is the policy updated with the collected trajectories. In general, \(\beta\) uses \(\pi\) partially, either with probability or smoothing.</p>

<p>The off-policy method introduces the distribution shift, where the distribution we are following, \(\beta\), is not the same as the distribution we are updating, \(\pi\). To diminish this effect, we use the IS.</p>

\[\require{color}
\begin{align}
    \hat{q}_{\pi}^{(i)} ( s_{t} , a_{t} )
    &amp; = \prod_{l=0}^{\infty}
    \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }
    { \beta ( a_{t+l} \vert s_{t+l} ) }
    \left( \sum_{k=0}^{\infty} \gamma^{k} r_{t+k+1}^{[\beta]} \right)
    = \prod_{l=0}^{\infty}
    \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }
    { \beta ( a_{t+l} \vert s_{t+l} ) }
    R_{t}^{[\beta]}
    = R_{t}^{[\pi]}
    \\
    &amp; \equiv \prod_{l=0}^{n-1}
    \frac{ \pi ( a_{t+l} \vert s_{t+l} ) }
    { \beta ( a_{t+l} \vert s_{t+l} ) }
    \left( \sum_{k=0}^{n-1} \gamma^{k} r_{t+k+1}^{[\beta]}
    + \gamma^{n} q_{\pi}^{(i)} ( s_{t+n} , a_{t+n} ) \right)
\end{align}\]

<p>The QL is the most popular value-based RL algorithm. In summary, the QL:</p>
<ol>
  <li>uses sampling, so it is robust to complex unknown environments. (Pros over DP)</li>
  <li>has low variance due to bootstrapping, so it converges fast. (Pros over MC)</li>
  <li>is off-policy, so it has better exploration than on-policy. (Pros over on-policy)</li>
  <li>avoids importance sampling correction since it uses the Bellman optimality equation. (Pros over TD learning and SARSA)</li>
  <li>has initialization bias, so the initialization influences the convergence. (Cons over MC)</li>
  <li>has overestimation bias, so it takes a risky path. (Cons over SARSA)</li>
</ol>

<h2 id="reference">Reference</h2>

<ul>
  <li>Lecture 3 to 5 of <a href="https://www.davidsilver.uk/teaching/">David Silverâ€™s lecture</a></li>
  <li>Chapter 4 to 7 in <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Reinforcement Learning: An Introduction</a></li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#machine-learning-x003a-decision" class="page__taxonomy-item" rel="tag">Machine Learning&#x003a; Decision</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2023-04-15">April 15, 2023</time></p>


      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=Value-based+Reinforcement+Learning%20http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fmachine_learning%2Fdecision_1_value_based_reinforcement_learning%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fmachine_learning%2Fdecision_1_value_based_reinforcement_learning%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fmachine_learning%2Fdecision_1_value_based_reinforcement_learning%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="#" class="pagination--pager disabled">Previous</a>
    
    
      <a href="/posts/machine_learning/decision_2_policy_based_reinforcement_learning/" class="pagination--pager" title="Policy-based Reinforcement Learning
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/neuroscience/spinal_cord_brain_and_nervous_system/" rel="permalink">Spinal Cord, Brain and Nervous System
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          21 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">In the field of neuroscience, there are specific biological components that play unique roles. The brain serves as the primary organ for processing informati...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/neuroscience/neuron_and_neural_tissue/" rel="permalink">Neuron and Neural Tissue
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">In the field of neuroscience, there are specific biological components that play unique roles. A neuron acts as an electrical device that sends a signal by i...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/mathematics/markov_model_2_markov_decision_process/" rel="permalink">Markov Decision Process
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          33 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">A Markov decision process is a type of Markov models used in decision making problems, which describes the world with a controllable system. It is used to mo...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/mathematics/markov_model_1_markov_chain/" rel="permalink">Markov Chain
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          35 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">A Markov chain is the simplest form of a Markov model, which describes the world with an autonomous system. It is used to model simulations, speech recogniti...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="https://twitter.com/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://instagram.com/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> Instagram</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Kel'Logg. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/posts/machine_learning/decision_1_value_based_reinforcement_learning/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/posts/machine_learning/decision_1_value_based_reinforcement_learning"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://kellogg-1.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  




<!-- mathjax -->


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: ["tex2jax.js"],
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
       processEscapes: true
     },
     "HTML-CSS": { availableFonts: ["TeX"] }
   });
</script>



  </body>
</html>
