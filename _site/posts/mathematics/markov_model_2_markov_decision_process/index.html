<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Markov Decision Process - Kelâ€™Logg</title>
<meta name="description" content="A Markov decision process is a type of Markov models used in decision making problems, which describes the world with a controllable system. It is used to model board games, robot locomotions, and more recently language models. This post aims to provide a tutorial on a Markov decision process and its variants, including a partially observable Markov decision process.">


  <meta name="author" content="D. K. Ryu">
  
  <meta property="article:author" content="D. K. Ryu">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Kel'Logg">
<meta property="og:title" content="Markov Decision Process">
<meta property="og:url" content="http://localhost:4000/posts/mathematics/markov_model_2_markov_decision_process/">


  <meta property="og:description" content="A Markov decision process is a type of Markov models used in decision making problems, which describes the world with a controllable system. It is used to model board games, robot locomotions, and more recently language models. This post aims to provide a tutorial on a Markov decision process and its variants, including a partially observable Markov decision process.">







  <meta property="article:published_time" content="2022-09-15T00:00:00+09:00">



  <meta property="article:modified_time" content="2023-04-15T00:00:00+09:00">




<link rel="canonical" href="http://localhost:4000/posts/mathematics/markov_model_2_markov_decision_process/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": null,
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Kel'Logg Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Kel'Logg
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/posts/">Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Categories</a>
            </li><li class="masthead__menu-item">
              <a href="/mathematics/">Mathematics</a>
            </li><li class="masthead__menu-item">
              <a href="/neuroscience/">Neuroscience</a>
            </li><li class="masthead__menu-item">
              <a href="/machine_learning/">Machine Learning</a>
            </li><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/assets/images/bio-photo.jpg" alt="D. K. Ryu" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">D. K. Ryu</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>Military science and technology (MS&amp;T) research soldier in the Republic of Korea (ROK) armed forces<br />Former Ph.D. candidate in reinforcement learning and natural language processing</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      

      
        
          
            <li><a href="mailto:dongwon.ryu@monash.edu" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://github.com/ktr0921" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="www.linkedin.com/in/dkryu" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Markov Decision Process">
    <meta itemprop="description" content="A Markov decision process is a type of Markov models used in decision making problems, which describes the world with a controllable system. It is used to model board games, robot locomotions, and more recently language models. This post aims to provide a tutorial on a Markov decision process and its variants, including a partially observable Markov decision process.">
    <meta itemprop="datePublished" content="2022-09-15T00:00:00+09:00">
    <meta itemprop="dateModified" content="2023-04-15T00:00:00+09:00">

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Markov Decision Process
</h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          32 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Table of Contents</h4></header>
              <ul class="toc__menu"><li><a href="#decision-making">Decision Making</a><ul><li><a href="#markov-decision-process">Markov Decision Process</a></li><li><a href="#ergodicity">Ergodicity</a></li><li><a href="#value-function">Value Function</a></li><li><a href="#bellman-equation">Bellman Equation</a></li></ul></li><li><a href="#extensions-to-markov-decision-process">Extensions to Markov Decision Process</a><ul><li><a href="#partial-observability">Partial Observability</a></li><li><a href="#other-environments">Other Environments</a></li></ul></li><li><a href="#environments">Environments</a><ul><li><a href="#fully-observable-environments">Fully Observable Environments</a></li><li><a href="#partially-observable-environments">Partially Observable Environments</a></li></ul></li><li><a href="#summary">Summary</a><ul><li><a href="#reference">Reference</a></li></ul></li></ul>

            </nav>
          </aside>
        
        
<div class="notice--primary">
  
<p><strong>Prerequisites</strong></p>

<ul>
  <li><a href="posts/mathematics/markov_model_1_markov_chain">Markov Chain</a></li>
</ul>

</div>

<blockquote>
    All is number.
    <br />
    <cite>Pythagoras</cite>
</blockquote>

<p>Humans have invented and developed languages to symbolize entities, constructing <em>Matrix</em> that captures the essence of reality. On the other hand, any form of entities can be modelled as <em>number</em>, from natural to complex numbers and from points to objects, which existed long before human civilization. Thereby, the universe itself is inherently number, or in short, <em>all is number</em>.</p>

<p><strong>Mathematics</strong> is the study of numbers, so if an entity exists, there exists mathematics to describe it, even if we cannot perceive. This is why we (arguably) refer mathematics as <em>discovery</em> instead of <em>invention</em> and often considered the most fundamental and precise way of describing reality. Mathematics is a vast, diverse and abstract field such that it requires books of worth. Here, a few branches of mathematics are focused on.</p>

<p>A <strong>Markov model</strong> is a stochastic process that describes pseudo-randomly changing systems under the <strong>Markov property</strong>, or the Markov assumption. The Markov property stipulates the future state depends only on the current state, not the history of states. There are four common Markov models:</p>

<table class="table-centering">
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 200px"> </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            <b>Fully Observable</b>
        </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            <b>Partially Observable</b>
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 200px">
            <b>Autonomous System</b>
        </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            <a href="/posts/mathematics/markov_model_1_markov_chain/#markov-chain">Markov Chain</a>
        </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            <a href="/posts/mathematics/markov_model_1_markov_chain/#hidden-markov-model">Hidden Markov Model</a>
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 200px"> <b>Controllable System</b> </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            <a href="/posts/mathematics/markov_model_2_markov_decision_process/#markov-decision-process">Markov Decision Process</a>
        </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            <a href="/posts/mathematics/markov_model_2_markov_decision_process/#partial-observabiliby">Partially Observable Markov Decision Process</a>
        </td>
    </tr>
</table>

<p>A <strong>Markov decision process</strong> is a type of Markov models used in decision making problems, which describes the world with a controllable system. It is used to model board games, robot locomotions, and more recently language models. This post aims to provide a tutorial on a Markov decision process and its variants, including a partially observable Markov decision process.</p>

<h1 id="decision-making">Decision Making</h1>

<p>Let the agent navigate the world and execute an action. To model decision making problems, we need to have the following three concepts.</p>
<ul>
  <li>A <strong>state</strong>, \(s \in \mathcal{S}\), is a computational model of the world that the agent observes, i.e. <code class="language-plaintext highlighter-rouge">hungry</code> state, <code class="language-plaintext highlighter-rouge">an enemy is in front of me</code> state, or an instance of a board game.</li>
  <li>An <strong>action</strong>, \(a \in \mathcal{A}\), is an incremental move that the agent takes to change the state, i.e. <code class="language-plaintext highlighter-rouge">eat</code> action for <code class="language-plaintext highlighter-rouge">hungry</code> state, or <code class="language-plaintext highlighter-rouge">attack</code> action for <code class="language-plaintext highlighter-rouge">an enemy is in front of me</code> state.</li>
  <li>A <strong>reward</strong>, \(r \in \mathcal{R}\), is a quantity that drives the agent to produce an appropriate action for a given state. It can be positive to encourage or negative to suppress a particular action, i.e. positive rewards on reaching <code class="language-plaintext highlighter-rouge">full</code> state by executing <code class="language-plaintext highlighter-rouge">eat</code> action from <code class="language-plaintext highlighter-rouge">hungry</code> state while negative on reaching <code class="language-plaintext highlighter-rouge">hungrier</code> state by executing <code class="language-plaintext highlighter-rouge">run</code> action from <code class="language-plaintext highlighter-rouge">hungry</code> state.</li>
</ul>

<p>However, the model that accounts for these three concepts is intractable. Here, I will go through a Markov decision process in a countably finite discrete-time setting. It employs the Markov property to simplify the world into a tractable model. Note that an agent is a system that executes an action, so the term <em>agent</em> is often used in decision making problems while the term <em>system</em> is for autonomous MCs.</p>

<h2 id="markov-decision-process">Markov Decision Process</h2>
<p>A <strong>Markov Decision Process (MDP)</strong> is a Markov chain (MC) with <strong>actions for controllability</strong> and <strong>rewards for motivation</strong>. It is a tuple of \(( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma )\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action set, \(\mathcal{P}\) is the state transition probability, \(\mathcal{R}\) is the reward function, and \(\gamma\) is the discount factor.</p>

<p>In an MDP, the agent ought to 1) execute actions, \(a \in \mathcal{A}\); 2) to navigate to the states, \(s \in \mathcal{S}\); 3) that hold positive rewards, \(r \in \mathcal{R}\). Due to the presence of \(\mathcal{A}\), an MDP has two main differences compared to an MC.</p>
<ol>
  <li>There exists a probability distribution over actions for a given history of states and actions, referred to as a <strong>policy</strong>, \(\pi ( A_{t} = a_{t} \vert S_{0:t} = s_{0:t} ) \in \Pi\).</li>
  <li>A transition probability is often referred to as a <strong>state transition probability</strong> and is a function of a history of states and actions, \(p ( S_{t+1} = s_{t+1} \vert S_{0:t} = s_{0:t} , A_{0:t} = a_{0:t} ) \in \mathcal{P}\).</li>
</ol>

<p>Thus, more precisely, in an MDP, the agent ought to 1) execute actions, \(a \in \mathcal{A}\); 2) following the policy, \(\pi \in \Pi\); 3) to navigate to the states, \(s \in \mathcal{S}\); 4) through state transition probabilities, \(p \in \mathcal{P}\); 5) that hold positive rewards, \(r \in \mathcal{R}\).</p>

<p>Under the Markov property, \(\Pi\), \(\mathcal{P}\) and \(\mathcal{R}\) can be expressed as:</p>

\[\begin{equation}
    \Pr ( A_{t} = a_{t} \vert S_{0:t} = s_{0:t} )
    \equiv \pi ( A_{t} = a_{t} \vert S_{t} = s_{t} )
    = \pi ( a_{t} \vert s_{t} , a_{t-1} )
    \\
    \Pr ( S_{t+1} = s_{t+1} \vert S_{0:t} = s_{0:t} , A_{0:t} = a_{0:t} )
    \equiv p ( S_{t+1} = s_{t+1} \vert S_{t} = s_{t} , A_{t} = a_{t} )
    = p ( s_{t+1} \vert s_{t} , a_{t} )
    \\
    r ( S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} )
    \equiv r ( S_{t} = s_{t} , A_{t-1} = a_{t-1} , S_{t-1} = s_{t-1} )
    = r ( s_{t} , a_{t-1} , s_{t-1} )
\end{equation}\]

<p>Note that a reward is expressed as \(\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}\), but this is a design choice, where some define it as \(\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\) or \(\mathcal{R}: \mathcal{S} \rightarrow \mathbb{R}\).</p>

<p>A collection of states and actions that the agent travelled for a given maximum time, \(T\), is referred to as a <strong>trajectory</strong>, \(\tau = ( S_{0} = s_{0} , A_{0} = a_{0} , \cdots  A_{T-1} = a_{T-1} , S_{T} = s_{T} )\).</p>
<ul>
  <li>If \(T \rightarrow \infty\), an MDP is referred to as an <strong>average-reward continuous MDP</strong>, where the task may continue forever. The environment usually provides a reward at each time step.</li>
  <li>If \(T &lt; \infty\), an MDP is referred to as a <strong>start-state episodic MDP</strong>, where the task has a clear ending. The environment usually provides the sum of rewards at each episode.</li>
</ul>

<p>Here, I will focus on an episodic MDP. An episode may finish if a player died in gameplay or a time step reaches the maximum time in robot locomotion.</p>

<p>The objective of an MDP is to design an agent that produces the optimal policy, \(\pi^{\ast} \in \Pi\), that receives as high total cumulative rewards as possible in the shortest travel distance. Consider the following MDP.</p>

<table style="margin-left: auto; margin-right: auto;">
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 750px" colspan="2">
            Markov decision process from
            <a href="https://commons.wikimedia.org/wiki/File:Markov_Decision_Process.svg">
                Wikimedia Commons
            </a>
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 400px" rowspan="2">
            <img src="/assets/mathematics/markov_model_2_markov_decision_process/1_mdp.png" />
        </td>
        <td style="text-align: center; vertical-align: middle; width: 350px">
            $$
            \begin{equation}
                \mathcal{S} = \{ s_{0} , s_{1} , s_{2} \}
                \qquad
                \mathcal{A} = \{ a_{0} , a_{1} \}
                \\
                r ( s_{0} , a_{1} , s_{2} ) = -1
                \qquad
                r ( s_{0} , a_{0} , s_{1} ) = +5
            \end{equation}
            $$
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 350px">
            $$
            \begin{align}
                \mathbf{P} ( a_{0} )
                &amp; = \begin{bmatrix}
                    p ( s_{0} \vert s_{0} , a_{0} ) &amp; p ( s_{1} \vert s_{0} , a_{0} ) &amp; p ( s_{2} \vert s_{0} , a_{0} ) \\
                    p ( s_{0} \vert s_{1} , a_{0} ) &amp; p ( s_{1} \vert s_{1} , a_{0} ) &amp; p ( s_{2} \vert s_{1} , a_{0} ) \\
                    p ( s_{0} \vert s_{2} , a_{0} ) &amp; p ( s_{1} \vert s_{2} , a_{0} ) &amp; p ( s_{2} \vert s_{2} , a_{0} ) \\
                \end{bmatrix}
                \\
                &amp; = \begin{bmatrix}
                    0.5 &amp; 0.0 &amp; 0.5 \\
                    0.7 &amp; 0.1 &amp; 0.2 \\
                    0.4 &amp; 0.0 &amp; 0.6 \\
                \end{bmatrix}
                \\
                \mathbf{P} ( a_{1} )
                &amp; = \begin{bmatrix}
                    p ( s_{0} \vert s_{0} , a_{1} ) &amp; p ( s_{1} \vert s_{0} , a_{1} ) &amp; p ( s_{2} \vert s_{0} , a_{1} ) \\
                    p ( s_{0} \vert s_{1} , a_{1} ) &amp; p ( s_{1} \vert s_{1} , a_{1} ) &amp; p ( s_{2} \vert s_{1} , a_{1} ) \\
                    p ( s_{0} \vert s_{2} , a_{1} ) &amp; p ( s_{1} \vert s_{2} , a_{1} ) &amp; p ( s_{2} \vert s_{2} , a_{1} ) \\
                \end{bmatrix}
                \\
                &amp; = \begin{bmatrix}
                    0.0 &amp; 0.0 &amp; 1.0 \\
                    0.0 &amp; 0.95 &amp; 0.05 \\
                    0.3 &amp; 0.3 &amp; 0.4 \\
                \end{bmatrix}
            \end{align}
            $$
        </td>
    </tr>
</table>

<p>Since the objective is to acquire high rewards, the agent should move towards positive rewards and avoid negative rewards. Therefore, the optimal policy, \(\pi^{\ast} \in \Pi\) is:</p>

\[\begin{equation}
    \pi^{\ast} ( a \vert s )
    = \begin{bmatrix}
        \pi ( a_{0} \vert s_{0} ) &amp; \pi ( a_{1} \vert s_{0} ) \\
        \pi ( a_{0} \vert s_{1} ) &amp; \pi ( a_{1} \vert s_{1} ) \\
        \pi ( a_{0} \vert s_{2} ) &amp; \pi ( a_{1} \vert s_{2} ) \\
    \end{bmatrix}
    = \begin{bmatrix}
        0.0 &amp; 1.0 \\
        1.0 &amp; 0.0 \\
        0.0 &amp; 1.0 \\
    \end{bmatrix}
\end{equation}\]

<p>A policy can be:</p>
<ul>
  <li>Deterministic: The choice of action is non-probabilistic, so the next action is produced for a given state, or \(a = \pi ( a \vert s )\).</li>
  <li>Stochastic: The choice of action is probabilistic, so the next action is sampled from policy distribution, or \(a \sim \pi ( a \vert s ) \in \mathbb{R}^{m}\), where \(\sum_{a \in \mathcal{A}} \pi ( a \vert s ) = 1\).</li>
</ul>

<p>Any type of policy can be used for any type of MDPs, i.e. a deterministic policy in a stochastic MDP or a stochastic MDP in a deterministic MDP.</p>

<p>Both have pros and cons. In the optimal world, where you can fully observe how the environment dynamics work, which is the above example, the optimal policy is deterministic, but in the real world, the stochastic policy is usually preferred because:</p>
<ul>
  <li>Deterministic policy lacks diversity (exploration-exploitation dilemma).</li>
  <li>The agent does not fully observe states in many practical scenarios (partial observability).</li>
</ul>

<p>A good example is <a href="https://web.stanford.edu/class/cs234/CS234Win2019/slides/lnotes8.pdf">an aliased gridworld from Stanford CS234 lecture</a>.</p>

<h2 id="ergodicity">Ergodicity</h2>

<p>The <strong>ergodicity</strong> is a good property for an MDP to hold, but this is more complicated than an MC due to the presence of the actions and the policy. Consider that the agent is in <code class="language-plaintext highlighter-rouge">an enemy is in front of me</code> state with two available actions, <code class="language-plaintext highlighter-rouge">attack</code> and <code class="language-plaintext highlighter-rouge">wait</code>. When the agent executes <code class="language-plaintext highlighter-rouge">attack</code>, the agent may or may not defeat the enemy.</p>
<ol>
  <li>With \(p (\) <code class="language-plaintext highlighter-rouge">an enemy is defeated</code> \(\vert\) <code class="language-plaintext highlighter-rouge">an enemy is in front of me</code>, <code class="language-plaintext highlighter-rouge">attack</code> \()\), the agent reaches <code class="language-plaintext highlighter-rouge">an enemy is defeated</code> state.</li>
  <li>With \(p (\) <code class="language-plaintext highlighter-rouge">an enemy dodged attack</code> \(\vert\) <code class="language-plaintext highlighter-rouge">an enemy is in front of me</code>, <code class="language-plaintext highlighter-rouge">attack</code> \()\), the agent reaches <code class="language-plaintext highlighter-rouge">an enemy dodged attack</code> state.</li>
</ol>

<p>The agent might require to execute <code class="language-plaintext highlighter-rouge">attack</code> until it reaches <code class="language-plaintext highlighter-rouge">an enemy is defeated</code> state. However, if the agent executes <code class="language-plaintext highlighter-rouge">wait</code>, it will reach <code class="language-plaintext highlighter-rouge">an enemy killed you</code> state, or \(p (\) <code class="language-plaintext highlighter-rouge">an enemy killed you</code> \(\vert\) <code class="language-plaintext highlighter-rouge">an enemy is in front of me</code>, <code class="language-plaintext highlighter-rouge">wait</code> \() = 1\). If the agent dies, it can never return to <code class="language-plaintext highlighter-rouge">an enemy is in front of me</code> state again.</p>
<ul>
  <li>If there is even a small chance of selecting <code class="language-plaintext highlighter-rouge">wait</code>, \(\pi (\) <code class="language-plaintext highlighter-rouge">wait</code> \(\vert\) <code class="language-plaintext highlighter-rouge">an enemy is in front of me</code> \() &gt; 0\), an MDP has absorbing states or chains.</li>
  <li>If there is no chance of selecting <code class="language-plaintext highlighter-rouge">wait</code>, \(\pi (\) <code class="language-plaintext highlighter-rouge">wait</code> \(\vert\) <code class="language-plaintext highlighter-rouge">an enemy is in front of me</code> \() = 0\), an MDP has only positive recurrent states.</li>
</ul>

<p>However, unlike an MC, having absorbing states or chains does not mean that the MDP is absorbing. More formally, define a <strong>state stationary distribution</strong>, stationary distribution or steady-state distribution, \(d ( s )\):</p>

\[\begin{align}
    d ( s )
    = &amp; \lim_{t \rightarrow \infty} \Pr (S_{t} = s \vert s_{0} , \pi)
    \\
    \equiv &amp; \sum_{k=0}^{\infty} \Pr ( S_{0} = s_{0} \rightarrow S_{k} = s \vert k , \pi)
    = \sum_{k=0}^{\infty} \Pr (s_{0} \rightarrow s \vert k , \pi)
\end{align}\]

<p>Similar to an MC, it is the probability distribution over states that you will end up if you travel an MDP infinitely under \(\pi\).</p>
<ul>
  <li>The first term signifies that for a given \(S_{0} = s_{0}\), the probability that the agent travels to \(s\) when \(t \rightarrow \infty\) following \(\pi\).</li>
  <li>The second term signifies the summation of the probability that the agent travels from \(S_{0} = s_{0}\) to \(s\) at \(k\) number of steps following \(\pi\) across \(k = 0\) to \(\infty\). This is often referred to as a <strong>state visitation probability</strong>.</li>
</ul>

\[\begin{align}
    \Pr (s \rightarrow s \vert 0 , \pi )
    = &amp; 1
    \\
    \Pr (s \rightarrow s' \vert 0 , \pi )
    = &amp; 0
    \\
    \Pr (s \rightarrow s' \vert 1 , \pi )
    = &amp; \sum_{a \in \mathcal{A}} \pi ( a \vert s ) p ( s' \vert s , a )
    \\
    \Pr (s \rightarrow s'' \vert k , \pi )
    = &amp; \sum_{s' \in \mathcal{S}} \Pr (s \rightarrow s' \vert n, \pi ) \cdot \Pr (s' \rightarrow s'' \vert k - n , \pi )
\end{align}\]

<p>Furthermore, \(d\) can be represented recursively.</p>

\[\begin{equation}
    d ( s' )
    = \sum_{s \in \mathcal{S}} d ( s )
    \sum_{a \in \mathcal{A}} \pi ( a \vert s ) p ( s' \vert s , a )
\end{equation}\]

<p>Before getting into the ergodicity, there is one additional class of an MDP. An <strong>unichain MDP</strong> is an MDP that guarantees a unique stationary state distribution for every policy, but its visitation is not guaranteed for every state. This means that the unichain MDP has a unique stationary state distribution that contains \(d (s) = 0\) for some \(s\), therefore, not ergodic.</p>

<p>There seems to be no formal definition on the ergodicity in an MDP, but three different definitions I found from <a href="https://ai.stackexchange.com/questions/27196/what-is-ergodicity-in-a-markov-decision-process-mdp">What is ergodicity in a Markov Decision Process (MDP)?</a> are:</p>
<ul>
  <li>There exists a policy \(\pi\) for a unique stationary distribution, \(d ( s )\), such that \(d ( s ) &gt; 0\) from <a href="https://dl.acm.org/doi/10.5555/3042573.3042759">Moldovan</a>.</li>
  <li>For every policy \(\pi\), a unique stationary distribution exists, \(d ( s )\), such that \(d ( s ) &gt; 0\) from <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887">Puterman</a>.</li>
  <li>For every policy \(\pi\), a unique stationary distribution exists, \(d ( s )\), such that \(d ( s ) \geq 0\) from <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Sutton</a>.</li>
</ul>

<p>Since the policy is usually learned, any of the definitions has a risk of falling into absorbing states. Most reinforcement learning takes the third definition, but it is not truly ergodic since a limiting distribution may reach zero for some \(\pi\).</p>
<ul>
  <li>In many reinforcement learning problems, an ergodicity assumption breaks since closed irreducible sets are present in an MDP, i.e. the permanent damages to robots or the progress of the game stage. There is one interesting perspective on how the ergodicity deals with the safety of the agent, <a href="https://dl.acm.org/doi/10.5555/3042573.3042759">Safe Exploration in Markov Decision Processes by Moldovan</a>.</li>
  <li>The research towards ergodicity in an MDP appears to be more related to operations research, so if you are interested, take a look at operations research.</li>
</ul>

<h2 id="value-function">Value Function</h2>

<p>The optimal policy is to guide the agent towards states with <em>the highest total cumulative rewards</em>.</p>

\[\begin{equation}
    \pi^{\ast} ( a \vert s ) = \arg \max_{\pi} \mathbb{E} [ \sum_{t = 0}^{\infty} r ( s_{t} ) ]
\end{equation}\]

<p>However, the agent must pursue the highest total cumulative rewards in <em>the shortest travel distance</em>. \(\sum_{t = 0}^{\infty} r ( s_{t} )\) weights the significance of the rewards in short distance and long distance equal. Then, the agent may potentially:</p>
<ul>
  <li>Fall into an infinite loop, where it repetitively visits the same states with rewards.</li>
  <li>Aim for only large but distant rewards, where it ignores many small rewards in a short distance.</li>
</ul>

<p>To comprehend the length of the travel distance into the optimal policy, we include a discount factor inside the optimization, often referred to as a <strong>return</strong> that is the total discounted cumulative rewards that the agent received throughout the trajectory, \(R_{t} = \sum_{k = 0}^{\infty} \gamma^{k} r ( s_{t + k + 1} )\).</p>

\[\begin{equation}
    \pi^{\ast} ( a \vert s )
    = \arg \max_{\pi} \mathbb{E} [ R_{t} ]
    = \arg \max_{\pi} \mathbb{E} [ \sum_{k = 0}^{\infty} \gamma^{k} r ( s_{t + k + 1} ) ]
\end{equation}\]

<p>Appropriately selecting \(\gamma \in [0,1]\) diminishes the effects of the long-distance rewards when computing the return.</p>
<ul>
  <li>If \(\gamma = 0\), the agent only cares about rewards in a single step, or <em>short-sighted</em>, which may result in the agent not learning anything if the rewards are sparse.</li>
  <li>If \(\gamma = 1\), the agent cares about rewards of the infinite horizon, or <em>long-sighted</em>, which may fall into an infinite loop or care about high but far-to-reach rewards.</li>
</ul>

<p>You can view short-sighted and long-sighted from the <em>Stanford marshmallow experiment</em>. A discount factor has a mathematical property that is bounded by \(\sum_{k = 0}^{\infty} \gamma^{k} = \frac{ 1 }{ 1 - \gamma }\). There are other ways to balance short- and long-sighted and they have different mathematic properties.</p>

<p>Additionally, \(d\) also includes a discount factor in its expression.</p>

\[\begin{align}
    d ( s )
    = &amp; \lim_{t \rightarrow \infty} \gamma^{t} \Pr (S_{t} = s \vert s_{0} , \pi)
\end{align}\]

<p>To make the mathematical expression of the optimization more flexible for reinforcement learning algorithms, we use the expectation of a return, or a <strong>value function</strong>.</p>

\[\begin{equation}
    v ( s )
    = \mathbb{E} [ R_{t} \vert S = s ]
    \qquad
    q ( s , a )
    = \mathbb{E} [ R_{t} \vert S = s , A = a ]
\end{equation}\]

<p>Where \(v\) is a state value function or <strong>state value</strong>, and \(q\) is a state-action value function or <strong>Q value</strong>. \(v\) quantifies how good a particular state is based on the expected return from the state following the policy while \(q\) deals with a state and action pair.</p>

<p>Thus, the optimal policy is expressed as:</p>

\[\begin{align}
    \pi^{\ast} ( a \vert s )
    = &amp; \arg \max_{\pi} \mathbb{E} [ R_{t} ]
    \\
    \equiv &amp; \arg \max_{\pi} \mathbb{E} [ v ( s ) ]
    = \arg \max_{\pi} \sum_{s \in \mathcal{S}} d ( s ) v ( s )
    \\
    \equiv &amp; \arg \max_{\pi} \mathbb{E} [ q ( s , a ) ]
    = \arg \max_{\pi} \sum_{s \in \mathcal{S}} d ( s ) \sum_{a \in \mathcal{A}} \pi ( a \vert s ) q ( s , a )
\end{align}\]

<p>An average-reward continuous MDP formalizes the value functions differently. It represents the <em>expected average reward without a discount factor</em>.</p>

\[\begin{equation}
    v ( s ) = \sum_{t = 1}^{\infty} \mathbb{E} \left[ r_{t} - R_{t} \vert S = s \right]
    \qquad
    q ( s , a ) = \sum_{t = 1}^{\infty} \mathbb{E} \left[ r_{t} - R_{t} \vert S = s , A = a \right]
\end{equation}\]

<h2 id="bellman-equation">Bellman Equation</h2>

<p>In an MDP, the <strong>Bellman equation</strong> is a recursion of value functions. The fundamental idea is more abstract, where it deals with a necessary condition for optimality in dynamic programming, but here, I will just cover the simplest application of the Bellman equation in an MDP.</p>

<p>The <strong>Bellman expectation equation</strong> is a type of Bellman equations that decomposes the value function recursively into a reward and discounted future value function. Consider a white circle as a state and a black dot as an action:</p>

<table style="margin-left: auto; margin-right: auto;">
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 300px">
            State value
        </td>
        <td style="text-align: center; vertical-align: middle; width: 300px">
            Q value
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 300px">
            <img src="/assets/mathematics/markov_model_2_markov_decision_process/2_1_be_1.png" width="200px" />
        </td>
        <td style="text-align: center; vertical-align: middle; width: 300px">
            <img src="/assets/mathematics/markov_model_2_markov_decision_process/2_1_be_2.png" width="200px" />
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 300px">
            $$
            \begin{align}
                v (s) = \sum_{a \in A} \pi (a \vert s) q (s, a)
            \end{align}
            $$
        </td>
        <td style="text-align: center; vertical-align: middle; width: 300px">
            $$
            \begin{align}
                q (s, a) = \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v (s') \right)
            \end{align}
            $$
        </td>
    </tr>
    <figcaption class="figure-caption text-center">
        Bellman expectation equation from
        <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf">
            David Silver's lecture
        </a>
    </figcaption>
</table>

<p>Two value functions can be combined to become recursive.</p>

<table style="margin-left: auto; margin-right: auto;">
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            State value
        </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            Q value
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            <img src="/assets/mathematics/markov_model_2_markov_decision_process/2_2_be_1.png" width="250px" />
        </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            <img src="/assets/mathematics/markov_model_2_markov_decision_process/2_2_be_2.png" width="250px" />
        </td>
    </tr>
    <tr>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            $$
            \begin{align}
                v (s)
                = \sum_{a \in A} \pi (a \vert s)
                \left(
                \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v (s') \right)
                \right)
            \end{align}
            $$
        </td>
        <td style="text-align: center; vertical-align: middle; width: 400px">
            $$
            \begin{align}
                q (s, a)
                = \sum_{s' \in S} p (s' \vert s , a)
                \left(
                r ( s , a , s' ) + \gamma \sum_{a' \in A} \pi (a' \vert s') q (s', a')
                \right)
            \end{align}
            $$
        </td>
    </tr>
    <figcaption class="figure-caption text-center">
        Bellman expectation equation 2 from
        <a href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MDP.pdf">
            David Silver's lecture
        </a>
    </figcaption>
</table>

<p>The <strong>Bellman optimality equation</strong> is another Bellman equation, which is basically the Bellman expectation equation with the greedy policy.</p>

\[\begin{equation}
    \pi^{\ast} ( a \vert s )
    = \begin{cases}
      1 &amp; \arg \max_{a} q ( s , a ) \\
      0 &amp; \text{otherwise}
    \end{cases}
\end{equation}\]

<p>The greedy policy is a type of deterministic policies, driven by only the maximum \(q ( s , a )\). Then, the two value functions can be formulated as:</p>

\[\begin{equation}
    v^{\ast} (s)
    = \sum_{a \in A} \pi^{\ast} (a \vert s) q^{\ast} (s, a)
    = \max_{a} q^{\ast} (s, a)
    \\
    q^{\ast} (s, a)
    = \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v^{\ast} (s') \right)
\end{equation}\]

<p>Furthermore, recursively:</p>

\[\begin{equation}
    v^{\ast} (s)
    = \max_a
    \left(
    \sum_{s' \in S} p (s' \vert s , a) ( r ( s , a , s' ) + \gamma v^{\ast} (s'))
    \right)
    \\
    q^{\ast} (s, a)
    = \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma \max_{a'} q^{\ast} (s', a') \right)
\end{equation}\]

<p>Often notations vary:</p>
<ul>
  <li>\(d^{\pi}\): The state stationary distribution following the policy, for instance, \(d^{\pi} ( s ) \neq d^{\pi'} ( s )\).</li>
  <li>\(v_{\pi}\): The state value following the policy, for instance, \(v_{\pi} ( s ) \neq v_{\pi'} ( s )\).</li>
  <li>\(R ( s_{t} , a_{t} )\): The return at \(s_{t}\) and \(a_{t}\), for instance, \(R_{t} = R ( s_{t} , a_{t} )\).</li>
  <li>\(\mathbb{E}_{ \tau \sim \pi }\): The expectation such that the trajectories are sampled following the policy, for instance, \(v_{\pi} (s) = \mathbb{E}_{ \tau \sim \pi } [ R_{t} \vert S = s ]\).</li>
  <li>\(\mathbb{E}_{ s \sim d^{\pi} , a \sim \pi }\): The expectation such that the state is sampled from the state stationary distribution and the action is from the policy, for instance, \(\mathbb{E}_{ \tau \sim \pi } [ R_{t} \vert S = s ] = \mathbb{E}_{ s \sim d^{\pi} , a \sim \pi } [ R_{t} \vert S = s ]\).</li>
  <li>The mathematical expressions of the Bellman expectation and optimality equations might be different from other articles or lectures. This is because a reward function takes different inputs, i.e. \(r ( s , a )\) or \(r ( s )\), but there is no practical difference.</li>
  <li>The exact same notations are applied to the Q value.</li>
</ul>

<h1 id="extensions-to-markov-decision-process">Extensions to Markov Decision Process</h1>

<p>An MDP is a fully observable fixed environment with only a single player. There are a number of extended MDPs commonly used in reinforcement learning.</p>

<h2 id="partial-observability">Partial Observability</h2>

<p>A <strong>partially observable Markov decision process (POMDP)</strong> is an MDP with partial observability. Instead of observing states directly, the agent receives an observation, infers a state and executes an action. A POMDP can be seen as an HMM version of an MDP, a tuple of \(( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{O}, \mathcal{P}_{o}, \mathcal{R}, \gamma )\), where \(\mathcal{O}\) is the observation space and \(\mathcal{P}_{o}\) is the conditional observation probability. \(\mathcal{P}_{o}\) also carries the Markov property.</p>

\[\begin{align}
    \Pr ( O_{t} = o_{t} \vert S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} )
    \equiv &amp; p_{o} ( O_{t} = o_{t} \vert S_{t} = s_{t} , A_{t-1} = a_{t-1} )
    \\
    = &amp; p_{o} ( o_{t} \vert s_{t} , a_{t} )
\end{align}\]

<p>The conditional observation probability can be written as \(\mathcal{P}_{o} : \mathcal{S} \rightarrow \mathcal{O}\) or \(\mathcal{P}_{o} : \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{O}\), but others share the same with MDP.</p>

<p>In the classic POMDP, a <strong>belief</strong>, \(b (s)\), is used to infer the state, which is a probability distribution over states. Then, the state can be inferred from:</p>

\[\begin{equation}
    b'( s' ) = \eta \cdot p_{o} ( o' \vert s' , a ) \sum_{ s \in \mathcal{S} } p ( s' \vert s , a ) b ( s )
\end{equation}\]

<p>Where \(\eta = \frac{1}{ \Pr ( o \vert b , a ) }\) is a normalizing constant with:</p>

\[\begin{equation}
    \Pr ( o \vert b , a ) = \sum_{ s' \in \mathcal{S} } p_{o} ( o' \vert s' , a ) \sum_{ s \in \mathcal{S} } p ( s' \vert s , a ) b ( s )
\end{equation}\]

<ul>
  <li>\(\sum_{ s \in \mathcal{S} } p ( s' \vert s , a ) b ( s )\): For an action that we executed, we multiply the state probability in the belief to the state transition probability for the next state, then we sum all across every state. This will give us the probability that we will reach at \(s'\) for a given \(a\), or \(\Pr ( s' \vert a )\).</li>
  <li>\(p_{o} ( o' \vert s' , a ) \Pr ( s' \vert a )\): For an acquired observation \(o'\), we can multiply the conditional observation probability of \(o'\) for \(s'\) and \(a\) pair to \(\Pr ( s' \vert a )\) to get the unnormalized probability that we are in \(s'\) for a given \(o'\) and \(a\). This is the unnormalized belief.</li>
  <li>\(b( s )\): A probability distribution over states that determines where the agent would be at the current time step.</li>
</ul>

<p>However, in deep reinforcement learning, instead, we often use a neural network as a function that maps from the observation space to the state space, \(f : \mathcal{O} \rightarrow \mathcal{S}\).</p>

<h2 id="other-environments">Other Environments</h2>

<p>A more general setting to a POMDP is a <strong>decentralized POMDP (Dec-POMDP)</strong>, where multiple agents are involved in decision making. It considers uncertainty in outcomes, sensors and communications. A Dec-POMDP is a 7-tuple \(( \mathcal{S}, \{ \mathcal{A}_{i} \}, \mathcal{P}, \{ \mathcal{O}_{i} \}, \mathcal{P}_{o}, \mathcal{R}, \gamma )\), where there are \(i\) number of agents, so \(i\) number of acquirable observations and executable actions. Hence, a POMDP is a special case of a Dec-POMDP with a single agent.</p>

<p>Further generalization on Dec-POMDP is a <strong>partially observable stochastic game (POSG)</strong>. The agents in a POSG hold different rewards, \(( \mathcal{S}, \{ \mathcal{A}_{i} \}, \mathcal{P}, \{ \mathcal{O}_{i} \}, \mathcal{P}_{o}, \{ \mathcal{R}_{i} \}, \gamma )\), so they either compete or cooperate in pursuing their assigned rewards. The terminology is from a game theory, where a <strong>stochastic game</strong>, or <strong>Markov game</strong>, refers to a repeated game with probabilistic transitions played by one or more players while partially observable is equivalent to adding observations and emission probabilities to the stochastic game.</p>

<p>An MDP can also hold non-stationarity, referred to as a <strong>non-stationary MDP (NSMDP)</strong>. It is simply a varying MDP over time or epoch, formally \(\mathcal{M}^{(i)} = ( \mathcal{S}^{(i)}, \mathcal{A}^{(i)}, \mathcal{P}^{(i)}, \mathcal{R}^{(i)}, \gamma^{(i)} )\), where there is \(i \in \mathbb{R}\). A special case of an NSMDP is used for multi-task reinforcement learning, meta reinforcement learning, and continual reinforcement learning, i.e. \(\mathcal{M}^{(i)} =  ( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}^{(i)}, \gamma )\) if the environment does not change. Note that this can also be applied to a POMDP, \(\mathcal{M}^{(i)} = ( \mathcal{S}^{(i)}, \mathcal{A}^{(i)}, \mathcal{P}^{(i)}, \mathcal{O}^{(i)}, \mathcal{P}_{o}^{(i)}, \mathcal{R}^{(i)}, \gamma^{(i)} )\).</p>

<p>If you are interested in further generalizations in an MDP, please read <a href="http://rbr.cs.umass.edu/camato/decpomdp/overview.html">this blog</a>.</p>

<!-- Papers that formally define this notation are,
- <a href="https://proceedings.neurips.cc/paper/2019/file/859b00aec8885efc83d1541b52a1220d-Paper.pdf">Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based Reinforcement Learning</a>
- <a href="http://proceedings.mlr.press/v119/chandak20a/chandak20a.pdf">Optimizing for the Future in Non-Stationary MDPs</a> -->

<h1 id="environments">Environments</h1>

<p>Environments can be modelled with few properties.</p>

<ul>
  <li>Environment can be:
    <ul>
      <li>Stochastic: Multiple states can be reached via an action for a given state, or there exist two or more states that have \(p ( s' \vert s , a ) \in (0, 1)\).</li>
      <li>Deterministic: Only a single state can be reached via an action for a given state, or there exists a single state that has \(p ( s' \vert s , a ) = 1\).</li>
    </ul>
  </li>
  <li>A state, action and time can be:
    <ul>
      <li>Discrete: State space, action space and time are countable, or there exists \(\mathcal{S} = \{ s_{i} \}_{i = 0}^{N}\), \(\mathcal{A} = \{ a_{i} \}_{i = 0}^{M}\), and \(\mathcal{T} = \{ t_{i} \}_{i = 0}^{K}\), where \(\sum_{s \in \mathcal{S}} \sum_{a \in \mathcal{A}} p (s' \vert s , a) = 1\) and \(\sum_{a \in \mathcal{A}} \pi (s \vert a) = 1\).</li>
      <li>Continuous: State space, action space and time are measurable, or there exists \(\mathcal{S} = \mathbb{R}^{n}\), \(\mathcal{A} = \mathbb{R}^{m}\), and \(\mathcal{T} = \mathbb{R}\), where \(\int_{S} \int_{A} p (s' \vert s , a) \text{ d}a \text{ d}s = 1\) and \(\int_{A} \pi (s \vert a) \text{ d}a = 1\).</li>
    </ul>
  </li>
</ul>

<p>Note that discrete state space is interpreted as there exists \(N\) number of states while in continuous, an infinitely many (uncountable) number of states exist in \(n\) space.</p>

<h2 id="fully-observable-environments">Fully Observable Environments</h2>

<p>There are two typical fully observable environments.</p>

<ul>
  <li><strong>Gridworld</strong>: Fully Observable Discrete Deterministic MDP
    <ul>
      <li>The agent ought to <code class="language-plaintext highlighter-rouge">navigate (action)</code> for <code class="language-plaintext highlighter-rouge">a given location of the player (state)</code> to <code class="language-plaintext highlighter-rouge">solve the game (reward)</code>.</li>
    </ul>
  </li>
</ul>

<figure class="align-center" style="width: 300px">
    <img src="/assets/mathematics/markov_model_2_markov_decision_process/3_1_gridworld.png" />
    <figcaption class="figure-caption text-center">
        Gridworld from
        <a href="https://www.mathworks.com/help/reinforcement-learning/ug/create-custom-grid-world-environments.html">
            MathWorks
        </a>
    </figcaption>
</figure>

<ul>
  <li><strong>Game Go</strong> in the two-player board game: Fully Observable Discrete Stochastic MDP
    <ul>
      <li>The agent ought to <code class="language-plaintext highlighter-rouge">place a stone (action)</code> for <code class="language-plaintext highlighter-rouge">a given location of all the stones (state)</code> to <code class="language-plaintext highlighter-rouge">win the game (reward)</code>.</li>
      <li>Many multi-player board games, including game Go and Chess, can be seen as either deterministic with multi-agent or stochastic with single-agent settings. For instance, the opponents can be seen as environment dynamics that induce stochasticity in the environment or other agents acting on the deterministic environment.</li>
      <li>Many multi-player card games are often considered as discrete stochastic MDP with partial observability, a.k.a. a POMDP, or with no observation until the game finishes, a.k.a. an episodic MDP with a return at the end of the game.</li>
    </ul>
  </li>
</ul>

<figure class="align-center" style="width: 150px">
    <img src="/assets/mathematics/markov_model_2_markov_decision_process/3_1_game_go.png" />
    <figcaption class="figure-caption text-center">
        Game Go from
        <a href="https://commons.wikimedia.org/wiki/File:Gokof.png">
            Wikimedia Commons
        </a>
    </figcaption>
</figure>

<h2 id="partially-observable-environments">Partially Observable Environments</h2>
<p>If the agent takes the state as an input, the environment becomes fully observable.</p>

<ul>
  <li><strong>Zork I</strong> in the text-based game: Discrete Stochastic POMDP
    <ul>
      <li>The agent ought to <code class="language-plaintext highlighter-rouge">execute a textual command (action)</code> for <code class="language-plaintext highlighter-rouge">a given textual description of the world (observation)</code> generated from <code class="language-plaintext highlighter-rouge">the world graph (state)</code> to <code class="language-plaintext highlighter-rouge">solve puzzles in the game (reward)</code>.</li>
      <li>Many text-based games are either deterministic or mild stochastic as they are designed to solve a set of specific tasks with limited causal relationships between entities. It is practically impossible to create infinitely many tasks with infinitely many causal relationships between entities.</li>
    </ul>
  </li>
</ul>

<figure class="align-center" style="width: 500px">
    <img src="/assets/mathematics/markov_model_2_markov_decision_process/3_2_zork1.png" />
    <figcaption class="figure-caption text-center">
        Zork I from
        <a href="https://www.oldgames.sk/en/gallery.php?image=7594">
            Old Games
        </a>
    </figcaption>
</figure>

<ul>
  <li><strong>Space Invader</strong> in the arcade learning environment: Continuous state and Discrete action Stochastic POMDP
    <ul>
      <li>The agent ought to <code class="language-plaintext highlighter-rouge">navigate or attack (action)</code> for <code class="language-plaintext highlighter-rouge">a given image of the world (observation)</code> generated from <code class="language-plaintext highlighter-rouge">the location of the player, enemies and items (state)</code> to <code class="language-plaintext highlighter-rouge">defeat the enemies (reward)</code>.</li>
      <li>Similar to the game Go, every enemy can be seen as another fixed agent, but the environment is still stochastic since where the enemies and items appear is probabilistic.</li>
    </ul>
  </li>
</ul>

<figure class="align-center" style="width: 300px">
    <img src="/assets/mathematics/markov_model_2_markov_decision_process/3_2_space_invader.png" />
    <figcaption class="figure-caption text-center">
        Space invader from
        <a href="https://en.wikipedia.org/wiki/File:SpaceInvaders-Gameplay.gif">
            Wikipedia
        </a>
    </figcaption>
</figure>

<ul>
  <li><strong>Robot Locomotion</strong> in the Open AI Gym: Continuous Deterministic POMDP.
    <ul>
      <li>The agent ought to <code class="language-plaintext highlighter-rouge">move joints (action)</code> for <code class="language-plaintext highlighter-rouge">a given image of the world (observation)</code> generated from <code class="language-plaintext highlighter-rouge">the coordinates of the joints (state)</code> to <code class="language-plaintext highlighter-rouge">locomote as intended (reward)</code>.</li>
      <li>If realistic factors are accounted, i.e. winds and terrains, it becomes stochastic.</li>
      <li>Similar to the space invader, it can be a multi-agent setting if two or more robots are used to interact.</li>
    </ul>
  </li>
</ul>

<figure class="align-center" style="width: 300px">
    <img src="/assets/mathematics/markov_model_2_markov_decision_process/3_2_robot_locomotion.gif" />
    <figcaption class="figure-caption text-center">
        Robot locomotion from
        <a href="https://commons.wikimedia.org/wiki/File:F4-motion.gif">
            Wikimedia Commons
        </a>
    </figcaption>
</figure>

<p>A time is discretized so that the agent can make a decision per time step. Note that by changing some factors, the environment can hold any property, i.e. gridworld with multi-agent setting or robot locomotion with a discrete action space \(\mathcal{A} = \{ 5 \text{m/s}, 10 \text{m/s} \}\). However, regardless of these properties, the policy and return are applied, so in practice:</p>
<ul>
  <li>All the environments are formalized as a stochastic setting.</li>
  <li>In discrete MDP, the state is embedded using a high dimensional vector.</li>
  <li>In POMDP, the agent infers a high dimensional state, i.e. embedding or vector space, from observation using an encoder.</li>
</ul>

<h1 id="summary">Summary</h1>

<p>In this post, Markov models with a controllable system in a discrete-time setting are covered.</p>

<p>An MDP is a tuple of \(( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma )\), where a controllable system, or an agent, ought to 1) execute actions, \(a \in \mathcal{A}\); 2) following the policy, \(\pi \in \Pi\); 3) to navigate to the states, \(s \in \mathcal{S}\); 4) through state transition probabilities, \(p \in \mathcal{P}\); 5) that hold positive rewards, \(r \in \mathcal{R}\).</p>

<p>A policy, state transition probability and reward in an MDP follow the Markov property.</p>

\[\begin{equation}
    \Pr ( A_{t} = a_{t} \vert S_{0:t} = s_{0:t} )
    \equiv \pi ( A_{t} = a_{t} \vert S_{t} = s_{t} )
    = \pi ( a_{t} \vert s_{t} , a_{t-1} )
    \\
    \Pr ( S_{t+1} = s_{t+1} \vert S_{0:t} = s_{0:t} , A_{0:t} = a_{0:t} )
    \equiv p ( S_{t+1} = s_{t+1} \vert S_{t} = s_{t} , A_{t} = a_{t} )
    = p ( s_{t+1} \vert s_{t} , a_{t} )
    \\
    r ( S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} )
    \equiv r ( S_{t} = s_{t} , A_{t-1} = a_{t-1} , S_{t-1} = s_{t-1} )
    = r ( s_{t} , a_{t-1} , s_{t-1} )
\end{equation}\]

<p>A collection of states and actions that the agent travelled for a given maximum time, \(T\), is referred to as a <strong>trajectory</strong>, \(\{ S_{0} = s_{0} , A_{1} = a_{1} , \cdots  A_{T-1} = a_{T-1} , S_{T} = s_{T} \}\).</p>
<ul>
  <li>If \(T \rightarrow \infty\), an MDP is referred to as a <strong>continuous average-reward MDP</strong>, where the task may continue forever.</li>
  <li>If \(T &lt; \infty\), an MDP is referred to as an <strong>episodic start-state MDP</strong>, where the task has a clear ending.</li>
</ul>

<p>An MDP is all about the policy. A policy can be:</p>
<ul>
  <li>Deterministic: The choice of action is non-probabilistic, so the next action is produced for a given state, or \(a = \pi ( a \vert s )\). Ideal for the optimal world, where environment dynamics are fully observed.</li>
  <li>Stochastic: The choice of action is probabilistic, so the next action is sampled from policy distribution, or \(a \sim \pi ( a \vert s ) \in \mathbb{R}^{m}\), where \(\sum_{a \in \mathcal{A}} \pi ( a \vert s ) = 1\). Ideal for the real world, where environment dynamics are not fully observed.</li>
</ul>

<p>An MDP also has the concept of ergodicity, but the definition varies by literature.</p>
<ul>
  <li>There exists a policy \(\pi\) for a unique stationary distribution, \(d ( s )\), such that \(d ( s ) &gt; 0\) from <a href="https://dl.acm.org/doi/10.5555/3042573.3042759">Moldovan</a>.</li>
  <li>For every policy \(\pi\), a unique stationary distribution exists, \(d ( s )\), such that \(d ( s ) &gt; 0\) from <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887">Puterman</a>.</li>
  <li>For every policy \(\pi\), a unique stationary distribution exists, \(d ( s )\), such that \(d ( s ) \geq 0\) from <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Sutton</a>.</li>
  <li>Where:
    <ul>
      <li>A state stationary distribution, stationary distribution or steady-state distribution, \(d ( s )\):</li>
    </ul>

\[\begin{align}
      d ( s )
      = &amp; \lim_{t \rightarrow \infty} \Pr (S_{t} = s \vert s_{0} , \pi)
      \equiv \sum_{k=0}^{\infty} \Pr (s_{0} \rightarrow s \vert k , \pi)
      \\
      = &amp; \sum_{s' \in \mathcal{S}} d ( s' )
      \sum_{a' \in \mathcal{A}} \pi ( a' \vert s' ) p ( s \vert s' , a' )
  \end{align}\]

    <ul>
      <li>A state visitation probability, \(\Pr (s_{0} \rightarrow s \vert k , \pi )\):</li>
    </ul>

\[\begin{align}
      \Pr (s \rightarrow s' \vert 1 , \pi )
      = \sum_{a \in \mathcal{A}} \pi ( a \vert s ) p ( s' \vert s , a )
      \qquad
      \Pr (s \rightarrow s'' \vert k , \pi )
      = \sum_{s' \in \mathcal{S}} \Pr (s \rightarrow s' \vert n, \pi ) \cdot \Pr (s' \rightarrow s'' \vert k - n , \pi )
  \end{align}\]
  </li>
</ul>

<p>A policy is all about the rewards.</p>
<ol>
  <li>
    <p>The optimal policy is the policy that acquires the highest total cumulative rewards.</p>

\[\begin{equation}
     \pi^{\ast} ( a \vert s ) = \arg \max_{\pi} \mathbb{E} [ \sum_{t = 0}^{\infty} r ( s_{t} ) ]
 \end{equation}\]
  </li>
  <li>
    <p>The optimal policy is the policy that acquires the highest total cumulative rewards in the shortest travel distance. A return is to comprehend both the total cumulative rewards and the travel length into the optimal policy.</p>

\[\begin{equation}
     \pi^{\ast} ( a \vert s )
     = \arg \max_{\pi} \mathbb{E} [ R_{t} ]
     = \arg \max_{\pi} \mathbb{E} [ \sum_{k = 0}^{\infty} \gamma^{k} r ( s_{t + k + 1} ) ]
 \end{equation}\]

    <ul>
      <li>If \(\gamma = 0\), the agent only cares about rewards in a single step, or <em>short-sighted</em>, which may result in the agent not learning anything if the rewards are sparse.</li>
      <li>If \(\gamma = 1\), the agent cares about rewards of the infinite horizon, or <em>long-sighted</em>, which may fall into an infinite loop or care about high but far-to-reach rewards.</li>
    </ul>
  </li>
  <li>
    <p>To make the mathematical expression of the optimization more flexible for reinforcement learning algorithms, we use the expectation of a return, or a value function. There are two types of value functions, a state value, \(v ( s ) = \mathbb{E} [ R_{t} \vert S = s ]\), and Q value, \(q ( s , a ) = \mathbb{E} [ R_{t} \vert S = s , A = a ]\). \(v\) quantifies how good a particular state is based on the expected return from the state following the policy while \(q\) deals with a state and action pair.</p>

\[\begin{align}
     \pi^{\ast} ( a \vert s )
     \equiv &amp; \arg \max_{\pi} \mathbb{E} [ v ( s ) ]
     = \arg \max_{\pi} \sum_{s \in \mathcal{S}} d ( s ) v ( s )
     \\
     \equiv &amp; \arg \max_{\pi} \mathbb{E} [ q ( s , a ) ]
     = \arg \max_{\pi} \sum_{s \in \mathcal{S}} d ( s ) \sum_{a \in \mathcal{A}} \pi ( a \vert s ) q ( s , a )
 \end{align}\]

    <ul>
      <li>The Bellman expectation equation decomposes the value function recursively into a reward and discounted future value function.</li>
    </ul>

\[\begin{align}
     v (s)
     = &amp; \sum_{a \in A} \pi (a \vert s) q (s, a)
     = \sum_{a \in A} \pi (a \vert s)
     \left(
     \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v (s') \right)
     \right)
     \\
     q (s, a)
     = &amp; \sum_{s' \in S} p (s' \vert s , a)
     \left(
     r ( s , a , s' ) + \gamma v (s')
     \right)
     = \sum_{s' \in S} p (s' \vert s , a)
     \left(
     r ( s , a , s' ) + \gamma \sum_{a' \in A} \pi (a' \vert s') q (s', a')
     \right)
 \end{align}\]

    <ul>
      <li>The Bellman optimality equation is the Bellman expectation equation with the greedy policy.</li>
    </ul>

\[\begin{align}
     v^{\ast} (s)
     = &amp; \sum_{a \in A} \pi^{\ast} (a \vert s) q^{\ast} (s, a)
     = \max_{a} q^{\ast} (s, a)
     = \max_a
     \left(
     \sum_{s' \in S} p (s' \vert s , a) ( r ( s , a , s' ) + \gamma v^{\ast} (s'))
     \right)
     \\
     q^{\ast} (s, a)
     = &amp; \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma v^{\ast} (s') \right)
     = \sum_{s' \in S} p (s' \vert s , a) \left( r ( s , a , s' ) + \gamma \max_{a'} q^{\ast} (s', a') \right)
 \end{align}\]
  </li>
</ol>

<p>Some MDP variants are:</p>
<ul>
  <li>
    <p>Partially observable Markov decision process (POMDP): A tuple of \(( \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{O}, \mathcal{P}_{o}, \mathcal{R}, \gamma )\)</p>

\[\begin{align}
      \Pr ( O_{t} = o_{t} \vert S_{0:t} = s_{0:t} , A_{0:t-1} = a_{0:t-1} , O_{0:t-1} = o_{0:t-1} )
      \equiv p_{o} ( O_{t} = o_{t} \vert S_{t} = s_{t} , A_{t-1} = a_{t-1} )
      = p_{o} ( o_{t} \vert s_{t} , a_{t} )
  \end{align}\]

    <ul>
      <li>A belief is used to infer the state.</li>
    </ul>

\[\begin{equation}
      b'( s' ) = \eta p_{o} ( o' \vert s' , a ) \sum_{ s \in \mathcal{S} } p ( s' \vert s , a ) b ( s )
      \quad
      \text{where, } \eta = \frac{1}{ \Pr ( o \vert b , a ) }
  \end{equation}\]

\[\begin{equation}
      \Pr ( o \vert b , a ) = \sum_{ s' \in \mathcal{S} } p_{o} ( o' \vert s' , a ) \sum_{ s \in \mathcal{S} } p ( s' \vert s , a ) b ( s )
  \end{equation}\]
  </li>
  <li>Decentralized POMDP (Dec-POMDP): A tuple of \(( \mathcal{S}, \{ \mathcal{A}_{i} \}, \mathcal{P}, \{ \mathcal{O}_{i} \}, \mathcal{P}_{o}, \mathcal{R}, \gamma )\)</li>
  <li>Partially observable stochastic game (POSG): A tuple of \(( \mathcal{S}, \{ \mathcal{A}_{i} \}, \mathcal{P}, \{ \mathcal{O}_{i} \}, \mathcal{P}_{o}, \{ \mathcal{R}_{i} \}, \gamma )\)</li>
  <li>Non-stationary MDP (NSMDP): A tuple of \(\mathcal{M}^{(i)} = ( \mathcal{S}^{(i)}, \mathcal{A}^{(i)}, \mathcal{P}^{(i)}, \mathcal{R}^{(i)}, \gamma^{(i)} )\)</li>
</ul>

<h2 id="reference">Reference</h2>

<ul>
  <li>Lecture 1 and 2 of <a href="https://www.davidsilver.uk/teaching/">David Silverâ€™s lecture</a></li>
  <li>Chapter 3 in <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Reinforcement Learning: An Introduction</a></li>
</ul>

<p>Other helpful resources are:</p>
<ul>
  <li><a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa15/docs/mdp-cheatsheet.pdf">MDP Cheatsheet Reference from John Schulman</a></li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#mathematics-x003a-markov-model" class="page__taxonomy-item" rel="tag">Mathematics&#x003a; Markov Model</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2023-04-15">April 15, 2023</time></p>


      </footer>

      <section class="page__share">
  

  <a href="https://twitter.com/intent/tweet?text=Markov+Decision+Process%20http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fmathematics%2Fmarkov_model_2_markov_decision_process%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fmathematics%2Fmarkov_model_2_markov_decision_process%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2Fmathematics%2Fmarkov_model_2_markov_decision_process%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/posts/mathematics/markov_model_1_markov_chain/" class="pagination--pager" title="Markov Chain
">Previous</a>
    
    
      <a href="/posts/neuroscience/neuron_and_neural_tissue/" class="pagination--pager" title="Neuron and Neural Tissue
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You May Also Enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/neuroscience/spinal_cord_brain_and_nervous_system/" rel="permalink">Spinal Cord, Brain and Nervous System
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          21 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">In the field of neuroscience, there are specific biological components that play unique roles. The brain serves as the primary organ for processing informati...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/neuroscience/neuron_and_neural_tissue/" rel="permalink">Neuron and Neural Tissue
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          24 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">In the field of neuroscience, there are specific biological components that play unique roles. A neuron acts as an electrical device that sends a signal by i...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/mathematics/markov_model_1_markov_chain/" rel="permalink">Markov Chain
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          35 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">A Markov chain is the simplest form of a Markov model, which describes the world with an autonomous system. It is used to model simulations, speech recogniti...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/posts/machine_learning/decision_2_policy_based_reinforcement_learning/" rel="permalink">Policy-based Reinforcement Learning
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          39 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">Policy-based reinforcement learning is one of two fundamental classes of reinforcement learning algorithms. It explicitly constructs the policy, directly map...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    

    
      
        
          <li><a href="mailto:dongwon.ryu@monash.edu" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
        
      
        
          <li><a href="https://github.com/ktr0921" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="www.linkedin.com/in/dkryu" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Kel'Logg. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/posts/mathematics/markov_model_2_markov_decision_process/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/posts/mathematics/markov_model_2_markov_decision_process"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://kellogg-1.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  




<!-- mathjax -->


<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
   MathJax.Hub.Config({
     extensions: ["tex2jax.js"],
     jax: ["input/TeX", "output/HTML-CSS"],
     tex2jax: {
       inlineMath: [ ['$','$'], ["\\(","\\)"] ],
       displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
       processEscapes: true
     },
     "HTML-CSS": { availableFonts: ["TeX"] }
   });
</script>



  </body>
</html>
